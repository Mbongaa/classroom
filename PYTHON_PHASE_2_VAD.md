# Python Phase 2: VAD Segmentation & File Saving

**Goal**: Use Silero VAD to segment teacher audio and save .wav files

**Duration**: 2-3 hours

**Prerequisites**: Phase 1 completed (agent connects and detects teacher audio)

**Location**: ALL code runs in Python agent (not Next.js)

---

## ğŸ“‹ Prerequisites Checklist

- [x] Phase 1 completed (agent detects teacher audio tracks)
- [x] Agent logs show "Teacher audio track detected"
- [x] Dependencies installed (includes livekit-plugins-silero)
- [ ] Teacher can speak and you hear audio in Next.js app

---

## ğŸ¯ Phase 2 Deliverables

1. âœ… Silero VAD model loaded
2. âœ… Audio stream processed with VAD
3. âœ… Speech segments extracted
4. âœ… .wav files saved to disk with timestamps
5. âœ… Silence periods ignored (no files created)

---

## ğŸ§  Step 1: Load Silero VAD in Prewarm

**File**: `agents/voice-segmenter/agent.py`

**Update the `prewarm()` function**:

```python
from livekit.plugins import silero

def prewarm(proc: JobProcess):
    """Prewarm function - runs once when worker starts"""
    logger.info('ğŸ”¥ Prewarming agent...')

    # Validate configuration
    errors = config.validate()
    if errors:
        logger.error(f'âŒ Configuration errors: {errors}')
        raise ValueError(f'Configuration invalid: {errors}')

    config.print_config()
    logger.info('âœ… Configuration validated')

    # âœ… NEW: Load Silero VAD model
    logger.info('ğŸ§  Loading Silero VAD model...')
    try:
        proc.userdata['vad'] = silero.VAD.load()
        logger.info('âœ… Silero VAD model loaded successfully')
    except Exception as e:
        logger.error(f'âŒ Failed to load Silero VAD: {e}')
        raise

    logger.info('âœ… Agent prewarmed successfully')
```

---

## ğŸ¤ Step 2: Create Audio Processor Module

**File**: `agents/voice-segmenter/audio_processor.py`

```python
import asyncio
import logging
import io
import wave
from datetime import datetime

from livekit import rtc
from livekit.plugins import silero
from livekit.agents import vad

logger = logging.getLogger('voice-segmenter.audio_processor')


class AudioProcessor:
    """Process audio with VAD segmentation (in-memory only)"""

    def __init__(self, vad):
        self.vad = vad
        self.active_languages = set()
        self.segment_count = 0
        self.total_segments_processed = 0

        logger.info('âœ… Audio processor initialized (in-memory mode)')

    async def process_track(
        self,
        track: rtc.Track,
        participant: rtc.RemoteParticipant,
        room_name: str
    ):
        """Process audio track with VAD segmentation"""
        logger.info(f'ğŸ§ Starting audio processing for: {participant.identity}')
        logger.info(f'ğŸ“‚ Processing room: {room_name}')

        try:
            # Create audio stream from track
            audio_stream = rtc.AudioStream(track)
            logger.info('âœ… Audio stream created')

            # Apply VAD to stream
            logger.info('ğŸ”Š Starting VAD segmentation...')
            vad_stream = self.vad.stream()

            # Create two concurrent tasks:
            # 1. Push audio frames to VAD
            # 2. Process VAD events
            async def push_audio_frames():
                """Push audio frames from track to VAD"""
                try:
                    async for event in audio_stream:
                        vad_stream.push_frame(event.frame)
                except Exception as e:
                    logger.error(f'Error pushing audio frames: {e}')
                finally:
                    await vad_stream.aclose()

            async def process_vad_events():
                """Process VAD events and extract speech segments"""
                try:
                    async for vad_event in vad_stream:
                        if vad_event.type == vad.VADEventType.START_OF_SPEECH:
                            logger.debug('ğŸ¤ Speech started')

                        elif vad_event.type == vad.VADEventType.END_OF_SPEECH:
                            logger.info('ğŸ¤ Speech ended')

                            # Get speech frames from VAD event
                            if vad_event.frames and len(vad_event.frames) > 0:
                                self.segment_count += 1
                                self.total_segments_processed += 1

                                # Get sample rate from first frame
                                sample_rate = vad_event.frames[0].sample_rate if hasattr(vad_event.frames[0], 'sample_rate') else 16000

                                # Calculate duration
                                total_samples = sum(len(frame.data) // 2 for frame in vad_event.frames)
                                duration = total_samples / sample_rate

                                # Convert to WAV bytes (in-memory)
                                wav_bytes = self.convert_to_wav_bytes(
                                    vad_event.frames,
                                    sample_rate=sample_rate
                                )

                                if wav_bytes:
                                    size_kb = len(wav_bytes) / 1024

                                    logger.info(f'âœ… Speech segment detected', extra={
                                        'segment': self.segment_count,
                                        'duration': f'{duration:.1f}s',
                                        'size': f'{size_kb:.1f}KB',
                                        'sample_rate': sample_rate,
                                        'languages': list(self.active_languages)
                                    })

                                    # TODO Phase 3: Send wav_bytes to Gemini translator
                                    logger.info(f'ğŸ’¾ WAV bytes prepared (in-memory, ready for translation)')

                except Exception as e:
                    logger.error(f'Error processing VAD events: {e}')

            # Run both tasks concurrently
            await asyncio.gather(
                push_audio_frames(),
                process_vad_events()
            )

        except Exception as e:
            logger.error(f'âŒ Audio processing error: {e}')

    def convert_to_wav_bytes(self, frames, sample_rate=16000):
        """
        Convert audio frames to WAV format bytes (in-memory, no file I/O)

        Args:
            frames: List of audio frames from VAD
            sample_rate: Sample rate from audio frames

        Returns:
            bytes: WAV formatted audio data ready for Gemini API
        """
        try:
            # Create in-memory buffer (no disk I/O!)
            buffer = io.BytesIO()

            # Write WAV format to buffer
            with wave.open(buffer, 'wb') as wav_file:
                wav_file.setnchannels(1)  # Mono
                wav_file.setsampwidth(2)  # 16-bit
                wav_file.setframerate(sample_rate)

                # Write all frames to buffer
                for frame in frames:
                    wav_file.writeframes(frame.data.tobytes())

            # Get bytes from buffer
            buffer.seek(0)
            wav_bytes = buffer.read()

            logger.debug(f'âœ… Converted {len(frames)} frames to {len(wav_bytes)} bytes')

            return wav_bytes

        except Exception as e:
            logger.error(f'âŒ Failed to convert frames to WAV bytes: {e}')
            return None

    def add_language(self, language_code: str):
        """Add language to active translation list"""
        if language_code not in self.active_languages:
            self.active_languages.add(language_code)
            logger.info(f'â• Language added: {language_code}')

    def remove_language(self, language_code: str):
        """Remove language from active translation list"""
        if language_code in self.active_languages:
            self.active_languages.remove(language_code)
            logger.info(f'â– Language removed: {language_code}')
```

---

## ğŸ”„ Step 3: Update Main Agent to Use Audio Processor

**File**: `agents/voice-segmenter/agent.py`

**Add imports at top**:

```python
from livekit.plugins import silero
from audio_processor import AudioProcessor
```

**Update `prewarm()` to load VAD and create processor**:

```python
def prewarm(proc: JobProcess):
    """Prewarm function - runs once when worker starts"""
    logger.info('ğŸ”¥ Prewarming agent...')

    # ... existing validation code ...

    # Load Silero VAD model
    logger.info('ğŸ§  Loading Silero VAD model...')
    try:
        vad = silero.VAD.load()
        proc.userdata['vad'] = vad
        logger.info('âœ… Silero VAD model loaded successfully')
    except Exception as e:
        logger.error(f'âŒ Failed to load Silero VAD: {e}')
        raise

    # Create audio processor (in-memory mode)
    proc.userdata['audio_processor'] = AudioProcessor(vad)
    logger.info('âœ… Audio processor initialized (in-memory mode)')

    logger.info('âœ… Agent prewarmed successfully')
```

**Update `entrypoint()` to integrate audio processor**:

```python
async def entrypoint(ctx: JobContext):
    # ... existing connection code ...

    # Get audio processor
    audio_processor = ctx.proc.userdata['audio_processor']

    @ctx.room.on('track_subscribed')
    def on_track_subscribed(
        track: rtc.Track,
        publication: rtc.TrackPublication,
        participant: rtc.RemoteParticipant
    ):
        logger.info(f'ğŸµ Track subscribed: {track.kind} from {participant.identity}')

        if track.kind != rtc.TrackKind.KIND_AUDIO:
            logger.debug(f'â­ï¸ Skipping non-audio track')
            return

        if not is_teacher(participant):
            logger.debug(f'â­ï¸ Skipping non-teacher audio')
            return

        logger.info(f'ğŸ¤ Teacher audio track detected: {participant.identity}')

        # Process teacher audio with VAD
        asyncio.create_task(
            audio_processor.process_track(track, participant, ctx.room.name)
        )

        logger.info(f'âœ… Audio processing started for: {participant.identity}')
```

---

## âœ… Step 4: Verification Tests

### Test 1: VAD Model Loading

**Start agent**:
```bash
cd /mnt/c/Users/HP/Desktop/meet/agents/voice-segmenter
python agent.py dev
```

**Expected logs**:
```
[INFO] ğŸ§  Loading Silero VAD model...
[INFO] âœ… Silero VAD model loaded successfully
[INFO] âœ… Audio processor initialized
[INFO] âœ… Agent prewarmed successfully
```

If errors, check:
- Internet connection (model downloads on first run)
- Disk space (~50MB needed)

---

### Test 2: Speech Segmentation

**Setup**:
1. **Terminal 1**: Run Next.js (`pnpm dev`)
2. **Terminal 2**: Run agent (`python agent.py dev`)
3. **Browser**: Join as teacher with mic enabled

**Actions**:
1. Navigate to: `http://localhost:3000/t/vad-test?classroom=true&role=teacher`
2. Enable microphone
3. Join room
4. **Speak clearly**: "Hello, this is segment one." (pause 2 seconds)
5. **Speak again**: "This is segment two." (pause 2 seconds)
6. **Speak again**: "And this is segment three."

**Expected agent logs**:
```
[INFO] ğŸ¤ Teacher audio track detected: Test Teacher__xxxx
[INFO] âœ… Audio processing started
[INFO] ğŸ§ Starting audio processing for: Test Teacher__xxxx
[INFO] ğŸ“‚ Saving segments to: /path/to/segments/vad-test
[INFO] âœ… Audio stream created
[INFO] ğŸ”Š Starting VAD segmentation...
[DEBUG] ğŸ¤ Speech started
[INFO] ğŸ¤ Speech ended, saving segment...
[INFO] ğŸ’¾ Segment saved: segment_001_20251008_140523.wav (size: 45.2KB, duration: 2.8s)
[DEBUG] ğŸ¤ Speech started
[INFO] ğŸ¤ Speech ended, saving segment...
[INFO] ğŸ’¾ Segment saved: segment_002_20251008_140545.wav (size: 38.1KB, duration: 2.4s)
```

**Check output directory**:
```bash
ls -lh agents/voice-segmenter/segments/vad-test/
```

**Expected files**:
```
segment_001_20251008_140523.wav
segment_002_20251008_140545.wav
segment_003_20251008_140612.wav
```

**Verify audio files**:
```bash
# Play one file (Windows)
start agents\voice-segmenter\segments\vad-test\segment_001_*.wav

# Or check file properties
file agents/voice-segmenter/segments/vad-test/segment_001_*.wav
```

Should show: `WAVE audio, 16000 Hz, mono`

---

## ğŸ› Troubleshooting

### Issue: No .wav files created

**Symptoms**:
- Agent logs show "Speech ended"
- But no files appear in segments/

**Solutions**:

1. **Check output directory**:
   ```python
   # Verify in audio_processor.py
   logger.info(f'Saving to: {filepath.absolute()}')
   ```

2. **Check permissions**:
   ```bash
   # Make sure directory is writable
   chmod 755 agents/voice-segmenter/segments/
   ```

3. **Check frames are captured**:
   ```python
   # Add to save_segment()
   logger.info(f'Frames to save: {len(frames)}')
   ```

---

### Issue: Files are 0 bytes or corrupted

**Solutions**:

1. **Check audio data**:
   ```python
   # In frames_to_audio()
   logger.info(f'Audio data shape: {audio_data.shape}')
   logger.info(f'Audio data dtype: {audio_data.dtype}')
   ```

2. **Verify WAV parameters**:
   ```python
   # Check sample rate matches LiveKit
   # Default: 16kHz, mono, 16-bit
   ```

---

### Issue: Speech not detected

**Symptoms**:
- Teacher speaks
- No "Speech ended" logs

**Solutions**:

1. **Check VAD sensitivity**:
   ```python
   # In silero.VAD.load(), add options:
   vad = silero.VAD.load(
       min_speech_duration_ms=250,
       min_silence_duration_ms=500
   )
   ```

2. **Add debug events**:
   ```python
   # Log all VAD events
   async for event in vad_stream:
       logger.debug(f'VAD event: {event.type}')
   ```

---

## âœ… Phase 2 Success Criteria

Before proceeding to Phase 3, verify ALL:

- [x] VAD model loads without errors
- [x] Agent processes teacher audio
- [x] .wav files appear in `segments/<room-name>/` directory
- [x] Files are valid audio (can play in media player)
- [x] Filenames include timestamps and segment numbers
- [x] File sizes are reasonable (20-100KB per segment)
- [x] Silence periods don't create files
- [x] Multiple segments saved for continuous speech

---

## ğŸ‰ Phase 2 Complete!

**What we built**:
- âœ… Silero VAD integration (in Python)
- âœ… Audio stream processing (in Python)
- âœ… Speech segment extraction (in Python)
- âœ… .wav file saving with timestamps (in Python)

**What's working**:
- âœ… Teacher audio segmented automatically
- âœ… Speech saved as playable .wav files
- âœ… Organized by room name
- âœ… Timestamped filenames

**What's NOT working yet** (expected):
- âŒ No translation (Phase 3)
- âŒ No captions for students (Phase 3)
- âŒ Just raw audio files

---

## ğŸ“Š Output Example

After a 5-minute classroom session:

```
agents/voice-segmenter/segments/
â””â”€â”€ classroom_abc123/
    â”œâ”€â”€ segment_001_20251008_140000.wav  # "Welcome everyone"
    â”œâ”€â”€ segment_002_20251008_140015.wav  # "Today we'll learn about..."
    â”œâ”€â”€ segment_003_20251008_140045.wav  # "The first concept is..."
    â”œâ”€â”€ segment_004_20251008_140112.wav  # "Let me explain further..."
    â””â”€â”€ ...
    â””â”€â”€ segment_025_20251008_144530.wav  # "See you next time"
```

**Total**: ~25-30 segments for 5 minutes of active speaking

---

## ğŸ“š Next Steps

**Ready for Phase 3?**

Once all Phase 2 success criteria are met, proceed to:

**`PYTHON_PHASE_3_TRANSLATION.md`** - Gemini translation integration

Phase 3 will add (ALL in Python):
- Gemini API integration (Python)
- Speech-to-text transcription (Python)
- Translation to multiple languages (Python)
- Save translation .txt files (Python)
- Publish to LiveKit for live captions (Python)

---

## ğŸ’¾ File Management Tips

**Clean up old segments**:
```bash
# Delete segments older than 7 days
find agents/voice-segmenter/segments/ -type f -mtime +7 -delete
```

**Check disk usage**:
```bash
du -sh agents/voice-segmenter/segments/
```

**Backup segments**:
```bash
# Copy to archive
cp -r agents/voice-segmenter/segments/ backup/segments_$(date +%Y%m%d)/
```
