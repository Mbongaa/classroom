Directory structure:
â””â”€â”€ mbongaa-bayaan-server/
    â”œâ”€â”€ README.md
    â”œâ”€â”€ broadcasting.py
    â”œâ”€â”€ config.py
    â”œâ”€â”€ CONNECTION_TROUBLESHOOTING.md
    â”œâ”€â”€ database.py
    â”œâ”€â”€ database_enhanced.py
    â”œâ”€â”€ database_integration.patch
    â”œâ”€â”€ DEPLOYMENT_INSTRUCTIONS.md
    â”œâ”€â”€ deployment_report_sentence_context_20250728_031206.md
    â”œâ”€â”€ Dockerfile
    â”œâ”€â”€ environment.template
    â”œâ”€â”€ FIX_AGENT_DISPATCH.md
    â”œâ”€â”€ GHOST_FIX_DEPLOYMENT.md
    â”œâ”€â”€ main.py
    â”œâ”€â”€ main_integration.md
    â”œâ”€â”€ main_production.py
    â”œâ”€â”€ merge_report_20250727_213400.md
    â”œâ”€â”€ prompt_builder.py
    â”œâ”€â”€ render.yaml
    â”œâ”€â”€ requirements.txt
    â”œâ”€â”€ resource_management.py
    â”œâ”€â”€ speechmatics_advanced.py
    â”œâ”€â”€ speechmatics_domain_patch.py
    â”œâ”€â”€ sync_dev_to_production.sh
    â”œâ”€â”€ sync_ghost_session_fix.sh
    â”œâ”€â”€ sync_report_20250728_212515.md
    â”œâ”€â”€ text_processing.py
    â”œâ”€â”€ translation_helpers.py
    â”œâ”€â”€ translator.py
    â”œâ”€â”€ update_from_server_dev.sh
    â”œâ”€â”€ update_prod.sh
    â”œâ”€â”€ update_server.sh
    â”œâ”€â”€ UPDATE_TO_126_GUIDE.md
    â”œâ”€â”€ VERSION_STRATEGY.md
    â”œâ”€â”€ webhook_handler.py
    â”œâ”€â”€ .env.example
    â”œâ”€â”€ backup_20250723_021521/
    â”‚   â”œâ”€â”€ broadcasting.py
    â”‚   â”œâ”€â”€ config.py
    â”‚   â”œâ”€â”€ database.py
    â”‚   â”œâ”€â”€ Dockerfile
    â”‚   â”œâ”€â”€ main.py
    â”‚   â”œâ”€â”€ main_production.py
    â”‚   â”œâ”€â”€ prompt_builder.py
    â”‚   â”œâ”€â”€ render.yaml
    â”‚   â”œâ”€â”€ requirements.txt
    â”‚   â”œâ”€â”€ resource_management.py
    â”‚   â”œâ”€â”€ text_processing.py
    â”‚   â”œâ”€â”€ translation_helpers.py
    â”‚   â”œâ”€â”€ translator.py
    â”‚   â””â”€â”€ webhook_handler.py
    â”œâ”€â”€ backup_20250723_173804/
    â”‚   â”œâ”€â”€ broadcasting.py
    â”‚   â”œâ”€â”€ main.py
    â”‚   â”œâ”€â”€ resource_management.py
    â”‚   â”œâ”€â”€ translation_helpers.py
    â”‚   â””â”€â”€ translator.py
    â”œâ”€â”€ backup_20250727_213400/
    â”‚   â”œâ”€â”€ broadcasting.py
    â”‚   â”œâ”€â”€ config.py
    â”‚   â”œâ”€â”€ database.py
    â”‚   â”œâ”€â”€ Dockerfile
    â”‚   â”œâ”€â”€ main.py
    â”‚   â”œâ”€â”€ main_production.py
    â”‚   â”œâ”€â”€ prompt_builder.py
    â”‚   â”œâ”€â”€ render.yaml
    â”‚   â”œâ”€â”€ requirements.txt
    â”‚   â”œâ”€â”€ resource_management.py
    â”‚   â”œâ”€â”€ text_processing.py
    â”‚   â”œâ”€â”€ translation_helpers.py
    â”‚   â”œâ”€â”€ translator.py
    â”‚   â”œâ”€â”€ update_from_server_dev.sh
    â”‚   â”œâ”€â”€ update_prod.sh
    â”‚   â”œâ”€â”€ update_server.sh
    â”‚   â””â”€â”€ webhook_handler.py
    â”œâ”€â”€ backup_20250728_212515/
    â”‚   â”œâ”€â”€ broadcasting.py
    â”‚   â”œâ”€â”€ config.py
    â”‚   â”œâ”€â”€ database.py
    â”‚   â”œâ”€â”€ database_enhanced.py
    â”‚   â”œâ”€â”€ Dockerfile
    â”‚   â”œâ”€â”€ main.py
    â”‚   â”œâ”€â”€ main_production.py
    â”‚   â”œâ”€â”€ prompt_builder.py
    â”‚   â”œâ”€â”€ render.yaml
    â”‚   â”œâ”€â”€ requirements.txt
    â”‚   â”œâ”€â”€ resource_management.py
    â”‚   â”œâ”€â”€ sync_dev_to_production.sh
    â”‚   â”œâ”€â”€ sync_ghost_session_fix.sh
    â”‚   â”œâ”€â”€ text_processing.py
    â”‚   â”œâ”€â”€ translation_helpers.py
    â”‚   â”œâ”€â”€ translator.py
    â”‚   â”œâ”€â”€ update_from_server_dev.sh
    â”‚   â”œâ”€â”€ update_prod.sh
    â”‚   â”œâ”€â”€ update_server.sh
    â”‚   â””â”€â”€ webhook_handler.py
    â”œâ”€â”€ backup_ghost_fix_20250728_034924/
    â”‚   â”œâ”€â”€ database.py
    â”‚   â”œâ”€â”€ main.py
    â”‚   â””â”€â”€ requirements.txt
    â”œâ”€â”€ backup_sentence_context_20250728_031206/
    â”‚   â”œâ”€â”€ broadcasting.py
    â”‚   â””â”€â”€ database.py
    â””â”€â”€ .claude/
        â””â”€â”€ settings.local.json

================================================
FILE: README.md
================================================
# Bayaan LiveKit Agent - Production Deployment

This directory contains the production-ready version of the Bayaan LiveKit Agent, optimized for deployment on Render as a background worker.

## ðŸš€ Quick Deploy to Render

### 1. Repository Setup

1. Push this directory to your GitHub repository: `https://github.com/Mbongaa/Bayaan-server.git`
2. Connect the repository to Render

### 2. Environment Variables

Set the following environment variables in your Render dashboard:

**Required:**
- `LIVEKIT_URL` - Your LiveKit server URL (e.g., `wss://your-livekit-server.com`)
- `LIVEKIT_API_KEY` - LiveKit API key
- `LIVEKIT_API_SECRET` - LiveKit API secret
- `OPENAI_API_KEY` - OpenAI API key for translation
- `SUPABASE_URL` - Supabase project URL
- `SUPABASE_SERVICE_ROLE_KEY` - Supabase service role key
- `SUPABASE_ANON_KEY` - Supabase anonymous key

**Optional:**
- `SPEECHMATICS_API_KEY` - Speechmatics API key (for enhanced STT)
- `LOG_LEVEL` - Logging level (default: INFO)
- `AGENT_NAME` - Agent name (default: bayaan-transcriber)
- `MAX_WORKERS` - Maximum worker processes (default: 2)

### 3. Deploy

1. Create a new Web Service on Render
2. Connect to your GitHub repository
3. Set the following:
   - **Environment**: Docker
   - **Dockerfile Path**: `./Dockerfile`
   - **Build Command**: (leave empty)
   - **Start Command**: `python main_production.py start`

## ðŸ“ File Structure

```
bayaan-server-production/
â”œâ”€â”€ main_production.py         # Production-ready main entry point
â”œâ”€â”€ Dockerfile                 # Production Docker configuration
â”œâ”€â”€ render.yaml               # Render deployment configuration
â”œâ”€â”€ requirements.txt          # Python dependencies with versions
â”œâ”€â”€ environment.template      # Environment variables template
â”œâ”€â”€ README.md                 # This file
â””â”€â”€ [original server files]   # All original server functionality
```

## ðŸ”§ Production Features

### Health Checks
- Built-in health check endpoint: `python main_production.py health`
- Validates environment variables and agent status
- Integrated with Render's health monitoring

### Graceful Shutdown
- Handles SIGTERM and SIGINT signals
- Waits for current jobs to complete (max 30 seconds)
- Properly closes database connections

### Logging
- Structured logging with configurable levels
- JSON output for production monitoring
- Timestamp and context information

### Error Handling
- Robust error handling and recovery
- Automatic restart on failure
- Detailed error reporting

## ðŸŽ¯ Testing Your Deployment

### 1. Verify Environment
```bash
# Check if all required environment variables are set
python main_production.py health
```

### 2. Local Testing
```bash
# Install dependencies
pip install -r requirements.txt

# Set environment variables (use environment.template)
export LIVEKIT_URL=wss://your-livekit-server.com
export LIVEKIT_API_KEY=your-api-key
# ... other variables

# Run the agent
python main_production.py start
```

### 3. Production Verification
- Check Render logs for successful startup
- Verify agent registration in LiveKit server
- Test with a room connection from your frontend

## ðŸ“Š Monitoring

### Render Dashboard
- CPU and memory usage
- Application logs
- Health check status
- Auto-scaling metrics

### LiveKit Server
- Agent registration status
- Room assignments
- Connection health

## ðŸ”„ Scaling

The service is configured for auto-scaling:
- **Minimum instances**: 1
- **Maximum instances**: 3
- **Scaling triggers**: CPU > 80% or Memory > 80%

## ðŸ› ï¸ Troubleshooting

### Common Issues

1. **Agent won't start**
   - Check all environment variables are set
   - Verify LiveKit server connectivity
   - Check logs for specific error messages

2. **Database connection errors**
   - Verify Supabase credentials
   - Check network connectivity
   - Ensure database is accessible

3. **Translation failures**
   - Check OpenAI API key and quota
   - Verify Speechmatics configuration (if used)
   - Check language configuration

### Debug Commands

```bash
# Health check
python main_production.py health

# Verbose logging
LOG_LEVEL=DEBUG python main_production.py start

# Check configuration
python -c "from config import get_config; print(get_config())"
```

## ðŸ“ž Support

For issues specific to this deployment:
1. Check Render logs first
2. Verify all environment variables
3. Test connectivity to external services
4. Contact support with relevant log excerpts

## ðŸ”’ Security

- Non-root user in Docker container
- Environment variables for sensitive data
- Network isolation in Render
- Regular security updates

## ðŸ“ˆ Performance

- Optimized for background worker usage
- Efficient resource utilization
- Automatic scaling based on load
- Connection pooling for database

---

**Ready for production? Deploy to Render and start testing with your first subject!** 


================================================
FILE: broadcasting.py
================================================
"""
Broadcasting module for LiveKit AI Translation Server.
Handles real-time broadcasting of transcriptions and translations to displays.
"""
import asyncio
import logging
import hashlib
import uuid
from typing import Optional, Dict, Any
from datetime import datetime

from config import get_config
from database import broadcast_to_channel, store_transcript_in_database

logger = logging.getLogger("transcriber.broadcasting")
config = get_config()


class BroadcastError(Exception):
    """Custom exception for broadcasting-related errors."""
    pass


async def broadcast_to_displays(
    message_type: str, 
    language: str, 
    text: str, 
    tenant_context: Optional[Dict[str, Any]] = None,
    sentence_context: Optional[Dict[str, Any]] = None
) -> bool:
    """
    Send transcription/translation to frontend via Supabase Broadcast and store in database.
    
    This function handles both real-time broadcasting and database storage of
    transcriptions and translations. It uses Supabase's broadcast feature for
    real-time updates and stores the data for persistence.
    
    Args:
        message_type: Type of message ("transcription" or "translation")
        language: Language code (e.g., "ar", "nl")
        text: The text content to broadcast
        tenant_context: Optional context containing room_id, mosque_id, etc.
        sentence_context: Optional context for sentence tracking (sentence_id, is_complete, etc.)
        
    Returns:
        bool: True if broadcast was successful, False otherwise
    """
    if not text or not text.strip():
        logger.debug("Empty text provided, skipping broadcast")
        return False
    
    success = False
    
    # Phase 1: Immediate broadcast via Supabase for real-time display
    if tenant_context and tenant_context.get("room_id") and tenant_context.get("mosque_id"):
        try:
            channel_name = f"live-transcription-{tenant_context['room_id']}-{tenant_context['mosque_id']}"
            
            # Generate unique message ID based on timestamp and content hash
            timestamp = datetime.utcnow().isoformat() + "Z"
            text_hash = hashlib.md5(text.encode('utf-8')).hexdigest()[:8]
            msg_id = f"{timestamp}_{text_hash}"
            
            # Build payload with optional sentence context
            data_payload = {
                "text": text,
                "language": language,
                "timestamp": timestamp,
                "msg_id": msg_id
            }
            
            # Add sentence context if provided
            if sentence_context:
                data_payload.update({
                    "sentence_id": sentence_context.get("sentence_id"),
                    "is_complete": sentence_context.get("is_complete", False),
                    "is_fragment": sentence_context.get("is_fragment", True)
                })
            
            payload = {
                "type": message_type,
                "room_id": tenant_context["room_id"],
                "mosque_id": tenant_context["mosque_id"],
                "data": data_payload
            }
            
            # Use the broadcast_to_channel function from database module
            success = await broadcast_to_channel(channel_name, message_type, payload)
            
            if success:
                logger.info(
                    f"ðŸ“¡ LIVE: Sent {message_type} ({language}) via Supabase broadcast: "
                    f"{text[:50]}{'...' if len(text) > 50 else ''}"
                )
            else:
                logger.warning(f"âš ï¸ Failed to broadcast {message_type} to Supabase")
                
        except Exception as e:
            logger.error(f"âŒ Broadcast error: {e}")
            success = False
    else:
        logger.warning("âš ï¸ Missing tenant context for Supabase broadcast")
    
    # Phase 2: Direct database storage (no batching)
    if tenant_context and tenant_context.get("room_id") and tenant_context.get("mosque_id"):
        try:
            # Store directly in database using existing function
            # Use create_task to avoid blocking the broadcast with proper error handling
            task = asyncio.create_task(
                _store_with_error_handling(message_type, language, text, tenant_context, sentence_context)
            )
            task.add_done_callback(lambda t: None if not t.exception() else logger.error(f"Storage task failed: {t.exception()}"))
            logger.debug(
                f"ðŸ’¾ DIRECT: Storing {message_type} directly to database "
                f"for room {tenant_context['room_id']}"
            )
        except Exception as e:
            logger.error(f"âŒ Failed to initiate database storage: {e}")
    else:
        logger.warning("âš ï¸ Missing tenant context for database storage")
    
    return success


async def _store_with_error_handling(
    message_type: str, 
    language: str, 
    text: str, 
    tenant_context: Dict[str, Any],
    sentence_context: Optional[Dict[str, Any]] = None
) -> None:
    """
    Store transcript with proper error handling.
    
    This is a wrapper around store_transcript_in_database that ensures
    errors don't propagate and crash the application.
    
    Args:
        message_type: Type of message ("transcription" or "translation")
        language: Language code
        text: The text content to store
        tenant_context: Context containing room_id, mosque_id, etc.
        sentence_context: Optional context containing sentence_id, is_complete, is_fragment
    """
    try:
        success = await store_transcript_in_database(
            message_type, language, text, tenant_context, sentence_context
        )
        if not success:
            logger.warning(
                f"âš ï¸ Failed to store {message_type} in database for "
                f"room {tenant_context.get('room_id')}"
            )
    except Exception as e:
        logger.error(
            f"âŒ Database storage error for {message_type}: {e}\n"
            f"Room: {tenant_context.get('room_id')}, "
            f"Language: {language}"
        )


async def broadcast_batch(
    messages: list[tuple[str, str, str, Dict[str, Any]]]
) -> Dict[str, int]:
    """
    Broadcast multiple messages in batch for efficiency.
    
    Args:
        messages: List of tuples (message_type, language, text, tenant_context)
        
    Returns:
        Dictionary with counts of successful and failed broadcasts
    """
    results = {"success": 0, "failed": 0}
    
    # Process all broadcasts concurrently
    tasks = []
    for message_type, language, text, tenant_context in messages:
        task = broadcast_to_displays(message_type, language, text, tenant_context)
        tasks.append(task)
    
    # Wait for all broadcasts to complete
    broadcast_results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # Count results
    for result in broadcast_results:
        if isinstance(result, Exception):
            results["failed"] += 1
            logger.error(f"Batch broadcast error: {result}")
        elif result:
            results["success"] += 1
        else:
            results["failed"] += 1
    
    logger.info(
        f"ðŸ“Š Batch broadcast complete: "
        f"{results['success']} successful, {results['failed']} failed"
    )
    
    return results


def create_broadcast_payload(
    message_type: str,
    language: str,
    text: str,
    room_id: int,
    mosque_id: int,
    additional_data: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """
    Create a standardized broadcast payload.
    
    Args:
        message_type: Type of message
        language: Language code
        text: The text content
        room_id: Room ID
        mosque_id: Mosque ID
        additional_data: Optional additional data to include
        
    Returns:
        Formatted payload dictionary
    """
    # Generate unique message ID
    timestamp = datetime.utcnow().isoformat() + "Z"
    text_hash = hashlib.md5(text.encode('utf-8')).hexdigest()[:8]
    msg_id = f"{timestamp}_{text_hash}"
    
    payload = {
        "type": message_type,
        "room_id": room_id,
        "mosque_id": mosque_id,
        "data": {
            "text": text,
            "language": language,
            "timestamp": timestamp,
            "msg_id": msg_id
        }
    }
    
    if additional_data:
        payload["data"].update(additional_data)
    
    return payload


def get_channel_name(room_id: int, mosque_id: int) -> str:
    """
    Generate the channel name for a room.
    
    Args:
        room_id: Room ID
        mosque_id: Mosque ID
        
    Returns:
        Channel name string
    """
    return f"live-transcription-{room_id}-{mosque_id}"


================================================
FILE: config.py
================================================
"""
Configuration management for LiveKit AI Translation Server.
Centralizes all configuration values and environment variables.
"""
import os
from dataclasses import dataclass
from typing import Optional, Dict
# Try to load dotenv if available
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    # dotenv not available, will use system environment variables
    pass


@dataclass
class SupabaseConfig:
    """Supabase database configuration."""
    url: str
    service_role_key: str
    anon_key: Optional[str] = None
    
    # Timeouts
    http_timeout: float = 5.0  # General HTTP request timeout
    broadcast_timeout: float = 2.0  # Broadcast API timeout
    
    @classmethod
    def from_env(cls) -> 'SupabaseConfig':
        """Load configuration from environment variables."""
        url = os.getenv('SUPABASE_URL')
        service_key = os.getenv('SUPABASE_SERVICE_ROLE_KEY')
        anon_key = os.getenv('SUPABASE_ANON_KEY')
        
        if not url:
            raise ValueError("SUPABASE_URL environment variable is required")
        if not service_key:
            raise ValueError("SUPABASE_SERVICE_ROLE_KEY environment variable is required")
            
        return cls(
            url=url,
            service_role_key=service_key,
            anon_key=anon_key
        )


@dataclass
class TranslationConfig:
    """Translation-related configuration."""
    # Language settings
    default_source_language: str = "ar"  # Arabic
    default_target_language: str = "nl"  # Dutch
    
    # Context window settings
    use_context: bool = True
    max_context_pairs: int = 12  # Increased from 6 for better accuracy in sermons/lectures
    
    # Timing settings
    translation_delay: float = 10.0  # Delay before translating incomplete sentences
    
    # Supported languages
    supported_languages: Dict[str, Dict[str, str]] = None
    
    def __post_init__(self):
        if self.supported_languages is None:
            self.supported_languages = {
                "ar": {"name": "Arabic", "flag": "ðŸ‡¸ðŸ‡¦"},
                "en": {"name": "English", "flag": "ðŸ‡¬ðŸ‡§"},
                "es": {"name": "Spanish", "flag": "ðŸ‡ªðŸ‡¸"},
                "fr": {"name": "French", "flag": "ðŸ‡«ðŸ‡·"},
                "de": {"name": "German", "flag": "ðŸ‡©ðŸ‡ª"},
                "ja": {"name": "Japanese", "flag": "ðŸ‡¯ðŸ‡µ"},
                "nl": {"name": "Dutch", "flag": "ðŸ‡³ðŸ‡±"},
                "tr": {"name": "Turkish", "flag": "ðŸ‡¹ðŸ‡·"},
                "ba": {"name": "Bashkir", "flag": "ðŸ‡·ðŸ‡º"},
                "eu": {"name": "Basque", "flag": "ðŸ‡ªðŸ‡¸"},
                "be": {"name": "Belarusian", "flag": "ðŸ‡§ðŸ‡¾"},
                "bn": {"name": "Bengali", "flag": "ðŸ‡§ðŸ‡©"},
                "bg": {"name": "Bulgarian", "flag": "ðŸ‡§ðŸ‡¬"},
                "yue": {"name": "Cantonese", "flag": "ðŸ‡­ðŸ‡°"},
                "ca": {"name": "Catalan", "flag": "ðŸ‡ªðŸ‡¸"},
                "hr": {"name": "Croatian", "flag": "ðŸ‡­ðŸ‡·"},
                "cs": {"name": "Czech", "flag": "ðŸ‡¨ðŸ‡¿"},
                "da": {"name": "Danish", "flag": "ðŸ‡©ðŸ‡°"},
                "eo": {"name": "Esperanto", "flag": "ðŸŒ"},
                "et": {"name": "Estonian", "flag": "ðŸ‡ªðŸ‡ª"},
                "fi": {"name": "Finnish", "flag": "ðŸ‡«ðŸ‡®"},
                "gl": {"name": "Galician", "flag": "ðŸ‡ªðŸ‡¸"},
                "el": {"name": "Greek", "flag": "ðŸ‡¬ðŸ‡·"},
                "he": {"name": "Hebrew", "flag": "ðŸ‡®ðŸ‡±"},
                "hi": {"name": "Hindi", "flag": "ðŸ‡®ðŸ‡³"},
                "hu": {"name": "Hungarian", "flag": "ðŸ‡­ðŸ‡º"},
                "id": {"name": "Indonesian", "flag": "ðŸ‡®ðŸ‡©"},
                "ia": {"name": "Interlingua", "flag": "ðŸŒ"},
                "ga": {"name": "Irish", "flag": "ðŸ‡®ðŸ‡ª"},
                "it": {"name": "Italian", "flag": "ðŸ‡®ðŸ‡¹"},
                "ko": {"name": "Korean", "flag": "ðŸ‡°ðŸ‡·"},
                "lv": {"name": "Latvian", "flag": "ðŸ‡±ðŸ‡»"},
                "lt": {"name": "Lithuanian", "flag": "ðŸ‡±ðŸ‡¹"},
                "ms": {"name": "Malay", "flag": "ðŸ‡²ðŸ‡¾"},
                "mt": {"name": "Maltese", "flag": "ðŸ‡²ðŸ‡¹"},
                "cmn": {"name": "Mandarin", "flag": "ðŸ‡¨ðŸ‡³"},
                "mr": {"name": "Marathi", "flag": "ðŸ‡®ðŸ‡³"},
                "mn": {"name": "Mongolian", "flag": "ðŸ‡²ðŸ‡³"},
                "no": {"name": "Norwegian", "flag": "ðŸ‡³ðŸ‡´"},
                "fa": {"name": "Persian", "flag": "ðŸ‡®ðŸ‡·"},
                "pl": {"name": "Polish", "flag": "ðŸ‡µðŸ‡±"},
                "pt": {"name": "Portuguese", "flag": "ðŸ‡µðŸ‡¹"},
                "ro": {"name": "Romanian", "flag": "ðŸ‡·ðŸ‡´"},
                "ru": {"name": "Russian", "flag": "ðŸ‡·ðŸ‡º"},
                "sk": {"name": "Slovakian", "flag": "ðŸ‡¸ðŸ‡°"},
                "sl": {"name": "Slovenian", "flag": "ðŸ‡¸ðŸ‡®"},
                "sw": {"name": "Swahili", "flag": "ðŸ‡°ðŸ‡ª"},
                "sv": {"name": "Swedish", "flag": "ðŸ‡¸ðŸ‡ª"},
                "tl": {"name": "Tagalog", "flag": "ðŸ‡µðŸ‡­"},
                "ta": {"name": "Tamil", "flag": "ðŸ‡®ðŸ‡³"},
                "th": {"name": "Thai", "flag": "ðŸ‡¹ðŸ‡­"},
                "uk": {"name": "Ukrainian", "flag": "ðŸ‡ºðŸ‡¦"},
                "ur": {"name": "Urdu", "flag": "ðŸ‡µðŸ‡°"},
                "ug": {"name": "Uyghur", "flag": "ðŸ‡¨ðŸ‡³"},
                "vi": {"name": "Vietnamese", "flag": "ðŸ‡»ðŸ‡³"},
                "cy": {"name": "Welsh", "flag": "ðŸ´"},
            }
    
    def get_target_language(self, room_config: Optional[Dict[str, any]] = None) -> str:
        """Get target language from room config or use default."""
        if room_config:
            # Check both possible field names (translation_language and translation__language)
            if 'translation_language' in room_config and room_config['translation_language']:
                return room_config['translation_language']
            elif 'translation__language' in room_config and room_config['translation__language']:
                return room_config['translation__language']
        return self.default_target_language
    
    def get_source_language(self, room_config: Optional[Dict[str, any]] = None) -> str:
        """Get source language from room config or use default."""
        if room_config and 'transcription_language' in room_config and room_config['transcription_language']:
            return room_config['transcription_language']
        return self.default_source_language
    
    def get_context_window_size(self, room_config: Optional[Dict[str, any]] = None) -> int:
        """Get context window size from room config or use default."""
        if room_config and 'context_window_size' in room_config and room_config['context_window_size']:
            # Ensure the value is within valid range (3-20)
            size = int(room_config['context_window_size'])
            return max(3, min(20, size))
        return self.max_context_pairs


@dataclass
class SpeechmaticsConfig:
    """Speechmatics STT configuration."""
    language: str = "ar"
    operating_point: str = "enhanced"
    enable_partials: bool = False  # Disabled to reduce API costs - frontend doesn't use partials
    max_delay: float = 3.5  # Increased from 2.0 for better context and accuracy
    punctuation_sensitivity: float = 0.5  # Default punctuation sensitivity
    diarization: str = "speaker"
    
    def with_room_settings(self, room_config: Optional[Dict[str, any]] = None) -> 'SpeechmaticsConfig':
        """Create a new config with room-specific overrides."""
        if not room_config:
            return self
            
        # Create a copy with room-specific overrides
        import copy
        new_config = copy.deepcopy(self)
        
        # Override with room settings if available
        if 'transcription_language' in room_config and room_config['transcription_language']:
            new_config.language = room_config['transcription_language']
        if 'max_delay' in room_config and room_config['max_delay'] is not None:
            new_config.max_delay = float(room_config['max_delay'])
        if 'punctuation_sensitivity' in room_config and room_config['punctuation_sensitivity'] is not None:
            new_config.punctuation_sensitivity = float(room_config['punctuation_sensitivity'])
            
        return new_config


@dataclass
class ApplicationConfig:
    """Main application configuration."""
    # Component configurations
    supabase: SupabaseConfig
    translation: TranslationConfig
    speechmatics: SpeechmaticsConfig
    
    # Logging
    log_level: str = "INFO"
    
    # Testing/Development
    default_mosque_id: int = 1
    test_mosque_id: int = 546012  # Hardcoded test mosque
    test_room_id: int = 192577    # Hardcoded test room
    
    @classmethod
    def load(cls) -> 'ApplicationConfig':
        """Load complete configuration from environment and defaults."""
        return cls(
            supabase=SupabaseConfig.from_env(),
            translation=TranslationConfig(),
            speechmatics=SpeechmaticsConfig()
        )
    
    def validate(self) -> None:
        """Validate configuration at startup."""
        # Print configuration status
        print("ðŸ”§ Configuration loaded:")
        print(f"   SUPABASE_URL: {self.supabase.url[:50]}...")
        print(f"   SERVICE_KEY: {'âœ… SET' if self.supabase.service_role_key else 'âŒ NOT SET'}")
        print(f"   Default Languages: {self.translation.default_source_language} â†’ {self.translation.default_target_language}")
        print(f"   Context Window: {'âœ… ENABLED' if self.translation.use_context else 'âŒ DISABLED'} ({self.translation.max_context_pairs} pairs)")
        print(f"   STT Defaults: delay={self.speechmatics.max_delay}s, punctuation={self.speechmatics.punctuation_sensitivity}, partials={'âœ…' if self.speechmatics.enable_partials else 'âŒ'}")


# Global configuration instance
_config: Optional[ApplicationConfig] = None


def get_config() -> ApplicationConfig:
    """Get or create the global configuration instance."""
    global _config
    if _config is None:
        _config = ApplicationConfig.load()
        _config.validate()
    return _config


def reset_config() -> None:
    """Reset configuration (mainly for testing)."""
    global _config
    _config = None


================================================
FILE: CONNECTION_TROUBLESHOOTING.md
================================================
# Backend Connection Troubleshooting Guide

## Current Status âœ…
The backend server is **running correctly** and registered with LiveKit:
- Worker ID: `AW_mKa3Fyz7iQQE`
- LiveKit URL: `wss://jamaa-app-4bix2j1v.livekit.cloud`
- Region: Germany
- All plugins loaded successfully (Speechmatics, OpenAI, Silero)

## The Issue
The server is waiting for room connections but none are being created. This is a **client-side issue**, not a server problem.

## What Should Happen
1. Frontend creates a LiveKit room via Supabase edge function
2. LiveKit sends a webhook to request an agent
3. Agent accepts the request and joins the room
4. Translation begins

## Check These Things

### 1. Frontend Room Creation
Verify the frontend is calling the Supabase edge function to create LiveKit rooms:
- Check browser console for errors when clicking "Go Live"
- Look for network requests to `/create-livekit-room`

### 2. LiveKit Webhook Configuration
In your LiveKit Cloud dashboard:
- Go to Settings â†’ Webhooks
- Ensure webhook URL points to your agent's deployment
- The URL should be: `https://your-render-service.onrender.com/webhook` (if using webhook)
- OR ensure "Agents" are enabled for automatic dispatch

### 3. Agent Request Pattern
The agent is configured to accept ALL room requests:
```python
async def request_fnc(req: JobRequest):
    await req.accept(
        name="agent",
        identity="agent",
    )
```

### 4. Test Room Creation
Try creating a test room directly:
1. Use LiveKit playground or CLI
2. Create a room with any name
3. Check if agent receives the request

### 5. Check Render Logs
Look for these log messages:
- âœ… `"registered worker"` - Agent connected to LiveKit
- â³ Waiting for: `"ðŸŽ¯ Received job request for room"` - Room request received
- â³ Waiting for: `"âœ… Accepted job request"` - Agent accepted room

## Quick Test
Use LiveKit CLI to create a test room:
```bash
livekit-cli create-room --api-key YOUR_KEY --api-secret YOUR_SECRET --url wss://jamaa-app-4bix2j1v.livekit.cloud test-room
```

Then check Render logs to see if the agent receives the request.

## Frontend Fix Checklist
- [ ] Verify VITE_LIVEKIT_URL is set correctly in frontend
- [ ] Check Supabase edge function `create-livekit-room` is deployed
- [ ] Ensure LiveKit API keys match between frontend and backend
- [ ] Verify room creation API call succeeds (check Network tab)
- [ ] Check browser console for WebSocket connection errors

## Summary
**The backend is working correctly!** The issue is that no rooms are being created for it to join. Focus on:
1. Frontend room creation process
2. LiveKit webhook/agent configuration
3. API key consistency between services


================================================
FILE: database.py
================================================
"""
Database operations for LiveKit AI Translation Server.
Handles all Supabase database interactions with connection pooling and async support.
FIXED: Thread-safe connection pool that works with LiveKit's multi-process architecture.
"""
import asyncio
import logging
import uuid
from typing import Optional, Dict, Any
from datetime import datetime
import aiohttp
from contextlib import asynccontextmanager
import threading

from config import get_config

logger = logging.getLogger("transcriber.database")
config = get_config()


class ThreadSafeDatabasePool:
    """Thread-safe database connection pool that creates separate pools per thread/process."""
    
    def __init__(self, max_connections: int = 10):
        self.max_connections = max_connections
        self._local = threading.local()
        self._lock = threading.Lock()
        
    async def get_session(self) -> aiohttp.ClientSession:
        """Get or create a session for the current thread."""
        # Check if current thread has a session
        if not hasattr(self._local, 'session') or self._local.session is None or self._local.session.closed:
            # Create new session for this thread
            connector = aiohttp.TCPConnector(
                limit=self.max_connections,
                limit_per_host=self.max_connections,
                force_close=True  # Force close to avoid connection issues
            )
            self._local.session = aiohttp.ClientSession(
                connector=connector,
                trust_env=True  # Trust environment proxy settings
            )
            logger.debug(f"Created new connection pool for thread {threading.current_thread().ident}")
        
        return self._local.session
    
    async def close(self):
        """Close the session for current thread."""
        if hasattr(self._local, 'session') and self._local.session and not self._local.session.closed:
            await self._local.session.close()
            self._local.session = None
            logger.debug(f"Closed connection pool for thread {threading.current_thread().ident}")


# Use thread-safe pool
_pool = ThreadSafeDatabasePool()


@asynccontextmanager
async def get_db_headers():
    """Get headers for Supabase API requests."""
    if not config.supabase.service_role_key:
        raise ValueError("SUPABASE_SERVICE_ROLE_KEY not configured")
    
    yield {
        'apikey': config.supabase.service_role_key,
        'Authorization': f'Bearer {config.supabase.service_role_key}',
        'Content-Type': 'application/json'
    }


async def ensure_active_session(room_id: int, mosque_id: int) -> Optional[str]:
    """
    Ensure there's an active session for the room and return session_id.
    
    This function:
    1. Checks for existing active sessions
    2. Creates a new session if none exists
    3. Returns the session ID or None on failure
    """
    try:
        # Get session from thread-safe pool
        session = await _pool.get_session()
        
        async with get_db_headers() as headers:
            # Check for existing active session
            url = f"{config.supabase.url}/rest/v1/room_sessions"
            params = {
                "room_id": f"eq.{room_id}",
                "status": "eq.active",
                "select": "id,started_at",
                "order": "started_at.desc",
                "limit": "1"
            }
            
            timeout = aiohttp.ClientTimeout(total=config.supabase.http_timeout)
            
            try:
                async with session.get(url, headers=headers, params=params, timeout=timeout) as response:
                    if response.status == 200:
                        sessions = await response.json()
                        if sessions and len(sessions) > 0:
                            session_id = sessions[0]["id"]
                            logger.debug(f"ðŸ“ Using existing active session: {session_id}")
                            return session_id
                    else:
                        error_text = await response.text()
                        logger.warning(f"Failed to check existing sessions: {response.status} - {error_text}")
            except asyncio.TimeoutError:
                logger.warning("Timeout checking for existing sessions")
            except Exception as e:
                logger.error(f"Error checking sessions: {e}")
            
            # Create new session if none exists
            new_session_id = str(uuid.uuid4())
            session_data = {
                "id": new_session_id,
                "room_id": room_id,
                "mosque_id": mosque_id,
                "status": "active",
                "started_at": datetime.utcnow().isoformat() + "Z",
                "logging_enabled": True
            }
            
            try:
                async with session.post(
                    url,
                    json=session_data,
                    headers={**headers, 'Prefer': 'return=minimal'},
                    timeout=timeout
                ) as response:
                    if response.status in [200, 201]:
                        logger.info(f"ðŸ“ Created new session: {new_session_id}")
                        return new_session_id
                    else:
                        error_text = await response.text()
                        logger.error(f"âŒ Failed to create session: {response.status} - {error_text}")
                        return None
            except asyncio.TimeoutError:
                logger.error("Timeout creating new session")
                return None
            except Exception as e:
                logger.error(f"Error creating session: {e}")
                return None
                    
    except Exception as e:
        logger.error(f"âŒ Session management failed: {e}")
        return None


async def store_transcript_in_database(
    message_type: str, 
    language: str, 
    text: str, 
    tenant_context: Dict[str, Any],
    sentence_context: Optional[Dict[str, Any]] = None
) -> bool:
    """
    Store transcription/translation in Supabase database.
    
    Args:
        message_type: Either "transcription" or "translation"
        language: Language code (e.g., "ar", "nl")
        text: The text to store
        tenant_context: Context containing room_id, mosque_id, session_id
        sentence_context: Optional context containing sentence_id, is_complete, is_fragment
        
    Returns:
        bool: True if successful, False otherwise
    """
    try:
        if not config.supabase.service_role_key:
            logger.error("âŒ SUPABASE_SERVICE_ROLE_KEY not found - cannot store transcripts")
            return False
            
        room_id = tenant_context.get("room_id")
        mosque_id = tenant_context.get("mosque_id")
        session_id = tenant_context.get("session_id")
        
        if not room_id or not mosque_id:
            logger.warning(f"âš ï¸ Missing room context: room_id={room_id}, mosque_id={mosque_id}")
            return False
            
        # Ensure we have an active session
        if not session_id:
            session_id = await ensure_active_session(room_id, mosque_id)
            if session_id:
                tenant_context["session_id"] = session_id
            else:
                logger.error("âŒ Could not establish session - skipping database storage")
                return False
        
        # Prepare transcript data
        transcript_data = {
            "room_id": room_id,
            "session_id": session_id,
            "timestamp": datetime.utcnow().isoformat() + "Z",
        }
        
        # Add sentence context if provided
        if sentence_context:
            transcript_data["sentence_id"] = sentence_context.get("sentence_id")
            transcript_data["is_complete"] = sentence_context.get("is_complete", False)
            transcript_data["is_fragment"] = sentence_context.get("is_fragment", True)
        
        # Set appropriate field based on message type
        if message_type == "transcription":
            transcript_data["transcription_segment"] = text
        else:  # translation
            transcript_data["translation_segment"] = text
            
        # Store in database
        session = await _pool.get_session()
        async with get_db_headers() as headers:
            timeout = aiohttp.ClientTimeout(total=config.supabase.http_timeout)
            
            try:
                async with session.post(
                    f"{config.supabase.url}/rest/v1/transcripts",
                    json=transcript_data,
                    headers={**headers, 'Prefer': 'return=minimal'},
                    timeout=timeout
                ) as response:
                    if response.status in [200, 201]:
                        logger.debug(f"âœ… Stored {message_type} in database: room_id={room_id}, session_id={session_id[:8]}")
                        return True
                    else:
                        error_text = await response.text()
                        logger.warning(f"âš ï¸ Database storage failed with status {response.status}: {error_text}")
                        return False
            except asyncio.TimeoutError:
                logger.warning("Timeout storing transcript")
                return False
            except Exception as e:
                logger.error(f"Error storing transcript: {e}")
                return False
                    
    except Exception as e:
        logger.error(f"âŒ Database storage error: {e}")
        return False


async def query_room_by_name(room_name: str) -> Optional[Dict[str, Any]]:
    """
    Query room information by LiveKit room name.
    
    Args:
        room_name: The LiveKit room name
        
    Returns:
        Room data dictionary or None if not found
    """
    try:
        session = await _pool.get_session()
        async with get_db_headers() as headers:
            url = f"{config.supabase.url}/rest/v1/rooms"
            params = {"Livekit_room_name": f"eq.{room_name}"}
            
            timeout = aiohttp.ClientTimeout(total=config.supabase.http_timeout)
            
            try:
                async with session.get(url, headers=headers, params=params, timeout=timeout) as response:
                    if response.status == 200:
                        rooms = await response.json()
                        if rooms and len(rooms) > 0:
                            return rooms[0]
                    else:
                        error_text = await response.text()
                        logger.warning(f"Failed to query room: {response.status} - {error_text}")
            except asyncio.TimeoutError:
                logger.warning("Timeout querying room")
            except Exception as e:
                logger.error(f"Error querying room: {e}")
        
        return None
        
    except Exception as e:
        logger.error(f"âŒ Room query failed: {e}")
        return None


async def get_active_session_for_room(room_id: int) -> Optional[str]:
    """
    Get the active session ID for a room if one exists.
    
    Args:
        room_id: The room ID
        
    Returns:
        Session ID or None if no active session
    """
    try:
        session = await _pool.get_session()
        async with get_db_headers() as headers:
            url = f"{config.supabase.url}/rest/v1/room_sessions"
            params = {
                "room_id": f"eq.{room_id}",
                "status": "eq.active",
                "select": "id",
                "order": "started_at.desc",
                "limit": "1"
            }
            
            timeout = aiohttp.ClientTimeout(total=config.supabase.http_timeout)
            
            try:
                async with session.get(url, headers=headers, params=params, timeout=timeout) as response:
                    if response.status == 200:
                        sessions = await response.json()
                        if sessions and len(sessions) > 0:
                            return sessions[0].get("id")
            except asyncio.TimeoutError:
                logger.warning("Timeout getting active session")
            except Exception as e:
                logger.error(f"Error getting active session: {e}")
        
        return None
        
    except Exception as e:
        logger.error(f"âŒ Active session query failed: {e}")
        return None


async def broadcast_to_channel(
    channel_name: str,
    event_type: str,
    payload: Dict[str, Any]
) -> bool:
    """
    Broadcast a message to a Supabase channel.
    
    Args:
        channel_name: The channel to broadcast to
        event_type: The event type (e.g., "transcription", "translation")
        payload: The data to broadcast
        
    Returns:
        bool: True if successful, False otherwise
    """
    try:
        if not config.supabase.service_role_key:
            logger.warning("âš ï¸ SUPABASE_SERVICE_ROLE_KEY not found - skipping broadcast")
            return False
            
        session = await _pool.get_session()
        
        async with get_db_headers() as headers:
            # Use broadcast-specific timeout
            broadcast_timeout = aiohttp.ClientTimeout(total=config.supabase.broadcast_timeout)
            
            try:
                async with session.post(
                    f"{config.supabase.url}/functions/v1/broadcast",
                    json={
                        "channel": channel_name,
                        "event": event_type,
                        "payload": payload
                    },
                    headers=headers,
                    timeout=broadcast_timeout
                ) as response:
                    if response.status == 200:
                        return True
                    else:
                        error_text = await response.text()
                        logger.warning(f"âš ï¸ Broadcast failed: {response.status} - {error_text}")
                        return False
            except asyncio.TimeoutError:
                logger.warning(f"âš ï¸ Broadcast timeout for channel {channel_name}")
                return False
            except Exception as e:
                logger.error(f"Error broadcasting: {e}")
                return False
                    
    except Exception as e:
        logger.error(f"âŒ Broadcast error: {e}")
        return False


async def query_prompt_template_for_room(room_id: int) -> Optional[Dict[str, Any]]:
    """
    Query the prompt template for a specific room.
    
    Args:
        room_id: The room ID
        
    Returns:
        Template data dictionary or None if not found
    """
    try:
        session = await _pool.get_session()
        async with get_db_headers() as headers:
            # Use the database function to get the appropriate template
            url = f"{config.supabase.url}/rest/v1/rpc/get_room_prompt_template"
            data = {"room_id": room_id}
            
            timeout = aiohttp.ClientTimeout(total=config.supabase.http_timeout)
            
            try:
                async with session.post(url, headers=headers, json=data, timeout=timeout) as response:
                    if response.status == 200:
                        result = await response.json()
                        if result and len(result) > 0:
                            template = result[0]
                            # Parse template_variables if it's a string
                            if isinstance(template.get('template_variables'), str):
                                try:
                                    import json
                                    template['template_variables'] = json.loads(template['template_variables'])
                                except:
                                    template['template_variables'] = {}
                            return template
                    else:
                        error_text = await response.text()
                        logger.warning(f"Failed to query prompt template: {response.status} - {error_text}")
            except asyncio.TimeoutError:
                logger.warning("Timeout querying prompt template")
            except Exception as e:
                logger.error(f"Error querying prompt template: {e}")
        
        return None
        
    except Exception as e:
        logger.error(f"âŒ Prompt template query failed: {e}")
        return None


async def update_session_heartbeat(session_id: str) -> bool:
    """
    Update the last_active timestamp for a session to prevent it from being cleaned up.
    
    Args:
        session_id: The session ID to update
        
    Returns:
        True if successful, False otherwise
    """
    if not session_id:
        return False
        
    try:
        session = await _pool.get_session()
        async with get_db_headers() as headers:
            # Update the last_active timestamp
            url = f"{config.supabase.url}/rest/v1/room_sessions"
            params = {"id": f"eq.{session_id}"}
            data = {"last_active": datetime.utcnow().isoformat()}
            
            timeout = aiohttp.ClientTimeout(total=config.supabase.http_timeout)
            
            try:
                async with session.patch(url, headers=headers, params=params, json=data, timeout=timeout) as response:
                    if response.status in [200, 204]:
                        logger.debug(f"ðŸ’“ Session heartbeat updated for {session_id}")
                        return True
                    else:
                        error_text = await response.text()
                        logger.warning(f"Failed to update session heartbeat: {response.status} - {error_text}")
                        return False
            except asyncio.TimeoutError:
                logger.warning(f"Timeout updating session heartbeat {session_id}")
                return False
            except Exception as e:
                logger.error(f"Error updating session heartbeat {session_id}: {e}")
                return False
                
    except Exception as e:
        logger.error(f"âŒ Failed to update session heartbeat {session_id}: {e}")
        return False


async def close_room_session(session_id: str) -> bool:
    """
    Close a room session by marking it as completed in the database.
    
    Args:
        session_id: The session ID to close
        
    Returns:
        True if successful, False otherwise
    """
    if not session_id:
        logger.warning("No session_id provided to close_room_session")
        return False
        
    try:
        session = await _pool.get_session()
        async with get_db_headers() as headers:
            # Call the cleanup_session_idempotent function
            url = f"{config.supabase.url}/rest/v1/rpc/cleanup_session_idempotent"
            data = {
                "p_session_id": session_id,
                "p_source": "agent_disconnect"
            }
            
            timeout = aiohttp.ClientTimeout(total=config.supabase.http_timeout)
            
            try:
                async with session.post(url, headers=headers, json=data, timeout=timeout) as response:
                    if response.status == 200:
                        result = await response.json()
                        logger.info(f"âœ… Session {session_id} closed successfully")
                        return True
                    else:
                        error_text = await response.text()
                        logger.error(f"Failed to close session: {response.status} - {error_text}")
                        return False
            except asyncio.TimeoutError:
                logger.warning(f"Timeout closing session {session_id}")
                return False
            except Exception as e:
                logger.error(f"Error closing session {session_id}: {e}")
                return False
                
    except Exception as e:
        logger.error(f"âŒ Failed to close session {session_id}: {e}")
        return False


async def close_database_connections():
    """Close all database connections. Call this on shutdown."""
    await _pool.close()
    logger.info("âœ… Database connections closed")


================================================
FILE: database_enhanced.py
================================================
"""
Enhanced Database operations with ghost session prevention.
This module should replace the ensure_active_session function in database.py
"""
import asyncio
import logging
from typing import Optional, Dict, Any
from datetime import datetime, timedelta
import aiohttp

from config import get_config

logger = logging.getLogger("transcriber.database_enhanced")
config = get_config()


async def ensure_active_session_atomic(
    room_id: int, 
    mosque_id: int,
    session: aiohttp.ClientSession,
    headers: Dict[str, str]
) -> Optional[str]:
    """
    Atomically ensure there's an active session for the room.
    Uses database-level locking to prevent ghost sessions.
    
    Args:
        room_id: The room ID
        mosque_id: The mosque ID
        session: Active aiohttp session
        headers: Supabase headers with auth
        
    Returns:
        Session ID or None on failure
    """
    try:
        # Use the atomic database function
        url = f"{config.supabase.url}/rest/v1/rpc/ensure_room_session_atomic"
        data = {
            "p_room_id": room_id,
            "p_mosque_id": mosque_id,
            "p_source": "livekit_agent"
        }
        
        timeout = aiohttp.ClientTimeout(total=config.supabase.http_timeout)
        
        async with session.post(
            url, 
            json=data, 
            headers=headers, 
            timeout=timeout
        ) as response:
            if response.status == 200:
                result = await response.json()
                if result and isinstance(result, list) and len(result) > 0:
                    result_data = result[0]
                elif isinstance(result, dict):
                    result_data = result
                else:
                    logger.error(f"Unexpected response format: {result}")
                    return None
                
                if 'error' in result_data:
                    logger.error(f"Database error: {result_data['error']}")
                    return None
                
                session_id = result_data.get('session_id')
                cleaned = result_data.get('cleaned_sessions', 0)
                
                if cleaned > 0:
                    logger.info(f"ðŸ§¹ Cleaned {cleaned} stale sessions before creating new one")
                
                if session_id:
                    logger.info(f"âœ… Active session ensured: {session_id}")
                    return session_id
                else:
                    logger.error("No session_id returned from atomic function")
                    return None
            else:
                error_text = await response.text()
                logger.error(f"Failed to ensure session: {response.status} - {error_text}")
                return None
                
    except asyncio.TimeoutError:
        logger.error("Timeout in ensure_active_session_atomic")
        return None
    except Exception as e:
        logger.error(f"Error in ensure_active_session_atomic: {e}")
        return None


async def update_session_heartbeat_enhanced(
    session_id: str,
    session: aiohttp.ClientSession,
    headers: Dict[str, str]
) -> bool:
    """
    Update session heartbeat with enhanced tracking.
    
    Args:
        session_id: The session ID to update
        session: Active aiohttp session
        headers: Supabase headers with auth
        
    Returns:
        True if successful, False otherwise
    """
    if not session_id:
        return False
        
    try:
        url = f"{config.supabase.url}/rest/v1/rpc/update_session_heartbeat_enhanced"
        data = {"p_session_id": session_id}
        
        timeout = aiohttp.ClientTimeout(total=5.0)  # Quick timeout for heartbeats
        
        async with session.post(
            url, 
            json=data, 
            headers=headers, 
            timeout=timeout
        ) as response:
            if response.status == 200:
                result = await response.json()
                if result and isinstance(result, list) and len(result) > 0:
                    result_data = result[0]
                elif isinstance(result, dict):
                    result_data = result
                else:
                    return False
                
                success = result_data.get('success', False)
                if success:
                    logger.debug(f"ðŸ’“ Heartbeat updated for session {session_id}")
                else:
                    reason = result_data.get('reason', 'unknown')
                    logger.warning(f"Heartbeat failed for {session_id}: {reason}")
                
                return success
            else:
                error_text = await response.text()
                logger.warning(f"Heartbeat update failed: {response.status} - {error_text}")
                return False
                
    except asyncio.TimeoutError:
        logger.warning(f"Heartbeat timeout for session {session_id}")
        return False
    except Exception as e:
        logger.error(f"Error updating heartbeat for {session_id}: {e}")
        return False


class SessionHealthMonitor:
    """
    Monitors session health and handles automatic recovery.
    """
    
    def __init__(self):
        self.missed_heartbeats: Dict[str, int] = {}
        self.recovery_attempts: Dict[str, int] = {}
        self.last_heartbeat: Dict[str, datetime] = {}
        
    async def monitor_heartbeat(
        self, 
        session_id: str,
        session: aiohttp.ClientSession,
        headers: Dict[str, str]
    ) -> bool:
        """
        Monitor heartbeat with automatic recovery on failure.
        
        Returns:
            True if healthy, False if recovery needed
        """
        try:
            # Update heartbeat
            success = await update_session_heartbeat_enhanced(
                session_id, session, headers
            )
            
            if success:
                self.missed_heartbeats[session_id] = 0
                self.recovery_attempts[session_id] = 0
                self.last_heartbeat[session_id] = datetime.utcnow()
                return True
            else:
                self.missed_heartbeats[session_id] = \
                    self.missed_heartbeats.get(session_id, 0) + 1
                
                # Check if we need recovery
                if self.missed_heartbeats[session_id] >= 3:
                    logger.warning(
                        f"Session {session_id} missed {self.missed_heartbeats[session_id]} heartbeats"
                    )
                    return False
                    
                return True
                
        except Exception as e:
            logger.error(f"Heartbeat monitoring error: {e}")
            return False
    
    def should_force_cleanup(self, session_id: str) -> bool:
        """
        Determine if a session should be forcefully cleaned up.
        """
        # Too many recovery attempts
        if self.recovery_attempts.get(session_id, 0) >= 3:
            return True
            
        # No heartbeat for too long
        last_beat = self.last_heartbeat.get(session_id)
        if last_beat and (datetime.utcnow() - last_beat) > timedelta(minutes=10):
            return True
            
        # Too many missed heartbeats
        if self.missed_heartbeats.get(session_id, 0) >= 10:
            return True
            
        return False
    
    def increment_recovery_attempt(self, session_id: str):
        """Track recovery attempts."""
        self.recovery_attempts[session_id] = \
            self.recovery_attempts.get(session_id, 0) + 1
    
    def cleanup_session_tracking(self, session_id: str):
        """Remove session from tracking."""
        self.missed_heartbeats.pop(session_id, None)
        self.recovery_attempts.pop(session_id, None)
        self.last_heartbeat.pop(session_id, None)


# Example integration in your main code:
"""
# In database.py, replace ensure_active_session with:
from database_enhanced import ensure_active_session_atomic

async def ensure_active_session(room_id: int, mosque_id: int) -> Optional[str]:
    session = await _pool.get_session()
    async with get_db_headers() as headers:
        return await ensure_active_session_atomic(
            room_id, mosque_id, session, headers
        )

# In main.py, add health monitoring:
from database_enhanced import SessionHealthMonitor

# Initialize monitor
health_monitor = SessionHealthMonitor()

# In your heartbeat periodic function:
async def update_session_heartbeat_periodic():
    while not stop_heartbeat:
        try:
            if tenant_context and tenant_context.get('session_id'):
                session_id = tenant_context['session_id']
                
                # Use health monitor
                healthy = await health_monitor.monitor_heartbeat(
                    session_id, session, headers
                )
                
                if not healthy:
                    if health_monitor.should_force_cleanup(session_id):
                        logger.error(f"Force cleanup needed for {session_id}")
                        await close_room_session(session_id)
                        break
                    else:
                        health_monitor.increment_recovery_attempt(session_id)
                        
            await asyncio.sleep(30)
        except Exception as e:
            logger.error(f"Heartbeat error: {e}")
            await asyncio.sleep(30)
"""


================================================
FILE: database_integration.patch
================================================
# Add these imports at the top of database.py after existing imports:
from database_enhanced import (
    ensure_active_session_atomic as _ensure_active_session_atomic,
    SessionHealthMonitor,
    update_session_heartbeat_enhanced
)

# Initialize health monitor (add after _pool initialization)
_health_monitor = SessionHealthMonitor()

# Replace the existing ensure_active_session function with:
async def ensure_active_session(room_id: int, mosque_id: int) -> Optional[str]:
    """
    Enhanced version with atomic session creation and ghost prevention.
    """
    try:
        session = await _pool.get_session()
        async with get_db_headers() as headers:
            return await _ensure_active_session_atomic(
                room_id, mosque_id, session, headers
            )
    except Exception as e:
        logger.error(f"Failed to ensure active session: {e}")
        return None

# Add this new function for enhanced heartbeat:
async def update_session_heartbeat_with_monitor(session_id: str) -> bool:
    """
    Update heartbeat with health monitoring.
    """
    if not session_id:
        return False
        
    try:
        session = await _pool.get_session()
        async with get_db_headers() as headers:
            # Use health monitor
            healthy = await _health_monitor.monitor_heartbeat(
                session_id, session, headers
            )
            
            if not healthy:
                if _health_monitor.should_force_cleanup(session_id):
                    logger.error(f"Session {session_id} needs force cleanup")
                    return False
                else:
                    _health_monitor.increment_recovery_attempt(session_id)
                    
            return healthy
    except Exception as e:
        logger.error(f"Heartbeat monitor error: {e}")
        return False

# Export health monitor for use in main.py
def get_health_monitor():
    return _health_monitor



================================================
FILE: DEPLOYMENT_INSTRUCTIONS.md
================================================
# Deployment Instructions - Bayaan Server

## Quick Deploy (Production-Ready)

### Step 1: Use the Correct Requirements File
```bash
# IMPORTANT: Use the pinned versions, NOT requirements.txt
pip install -r requirements-pinned.txt
```

### Step 2: Deploy Your July 28 Code
Use the code from commit `2ec438247866c62bc2c0d259767e9e5bd089de8f` or your backup:
- Keep the domain patch imports
- Keep the TranscriptionConfig wrapper
- Don't change anything - it works perfectly

### Step 3: Verify Deployment
After deployment, check logs for:
```
âœ… "registered worker, id=AW_..." 
âœ… "received job request"
âœ… "Accepted job request for room"
âœ… "Speechmatics domain configured: broadcast"
```

## What's Actually Happening

### Why Deployments Break
When you use `requirements.txt` with `>=1.0.0`, pip installs the latest versions:
- Gets LiveKit 1.2.6+ instead of 1.2.1
- New version doesn't receive job requests with current frontend
- Agent registers but never connects to rooms

### Why July 28 Works
Your July 28 deployment has:
- LiveKit 1.2.1 frozen in the container
- Compatible job dispatch mechanism
- Domain patch working correctly
- Perfect integration with your frontend

## Files to Use

### requirements-pinned.txt (USE THIS)
```python
livekit-agents==1.2.1
livekit-plugins-openai==0.8.1
livekit-plugins-speechmatics==0.6.1
livekit-plugins-silero==0.6.1
# ... rest of dependencies
```

### main.py (July 28 Version)
- Keep ALL domain patch code
- Keep TranscriptionConfig wrapper style
- Don't remove any imports
- The deprecation warnings don't matter

## Render/Railway Deployment

### Environment Variables
No changes needed - use your existing:
```
LIVEKIT_API_KEY=your_key
LIVEKIT_API_SECRET=your_secret
LIVEKIT_URL=wss://your-url
OPENAI_API_KEY=your_key
SPEECHMATICS_API_KEY=your_key
SUPABASE_URL=your_url
SUPABASE_SERVICE_ROLE_KEY=your_key
```

### Build Command
```bash
pip install -r requirements-pinned.txt
```

### Start Command
```bash
python main.py
```

## Common Mistakes to Avoid

âŒ **DON'T** use `requirements.txt` with `>=` operators
âŒ **DON'T** remove the domain patch - it works fine
âŒ **DON'T** change to direct parameters (that was for 1.2.6+)
âŒ **DON'T** update LiveKit versions "just because"

âœ… **DO** use exact version pins
âœ… **DO** keep your July 28 code as-is
âœ… **DO** verify job requests in logs
âœ… **DO** trust that 1.2.1 is stable

## Troubleshooting

### If Agent Doesn't Connect
1. Check you're using `requirements-pinned.txt`
2. Verify logs show "received job request"
3. Ensure frontend hasn't changed

### If Domain Patch Error Appears
1. You're accidentally using newer LiveKit version
2. Redeploy with `requirements-pinned.txt`

### If Transcripts Lag
1. Keep the TranscriptionConfig wrapper (July 28 style)
2. Don't use direct parameters (that's for 1.2.6+)

## Summary

Your July 28 code + LiveKit 1.2.1 = Perfect Working System

Just pin your versions and deploy. No code changes needed!


================================================
FILE: deployment_report_sentence_context_20250728_031206.md
================================================
# Sentence Context Update Deployment Report
**Date:** Mon Jul 28 03:12:06 CEST 2025
**Feature:** Sentence context tracking for improved session replay quality
**Source:** /mnt/c/Users/hassa/OneDrive/Desktop/0.2 Bayan/Dev/bayan-platform-admin-login/Backend/LiveKit-ai-translation/server
**Target:** /mnt/c/Users/hassa/OneDrive/Desktop/0.2 Bayan/Dev/bayan-platform-admin-login/Backend/LiveKit-ai-translation/bayaan-server-production

## Deployment Summary

### Changes Being Deployed:
1. **database.py** - Added sentence_context parameter to store_transcript_in_database
2. **broadcasting.py** - Updated to pass sentence_context through to storage

### Database Changes Required:
- Migration: 20250128_add_sentence_context_to_transcripts.sql
- Adds: sentence_id, is_complete, is_fragment columns to transcripts table

### Backup Location\n`/mnt/c/Users/hassa/OneDrive/Desktop/0.2 Bayan/Dev/bayan-platform-admin-login/Backend/LiveKit-ai-translation/bayaan-server-production/backup_sentence_context_20250728_031206`\n
\n### File Changes Analysis\n
#### database.py Changes:
```diff
     message_type: str, 
     language: str, 
     text: str, 
-    tenant_context: Dict[str, Any]
+    tenant_context: Dict[str, Any],
+    sentence_context: Optional[Dict[str, Any]] = None
 ) -> bool:
     """
     Store transcription/translation in Supabase database.
@@ -162,6 +163,7 @@
         language: Language code (e.g., "ar", "nl")
         text: The text to store
         tenant_context: Context containing room_id, mosque_id, session_id
+        sentence_context: Optional context containing sentence_id, is_complete, is_fragment
         
     Returns:
         bool: True if successful, False otherwise
@@ -195,6 +197,12 @@
             "timestamp": datetime.utcnow().isoformat() + "Z",
         }
         
+        # Add sentence context if provided
+        if sentence_context:
+            transcript_data["sentence_id"] = sentence_context.get("sentence_id")
+            transcript_data["is_complete"] = sentence_context.get("is_complete", False)
+            transcript_data["is_fragment"] = sentence_context.get("is_fragment", True)
+        
         # Set appropriate field based on message type
         if message_type == "transcription":
             transcript_data["transcription_segment"] = text
@@ -417,6 +425,98 @@
```
#### broadcasting.py Changes:
```diff
@@ -107,7 +107,7 @@
             # Store directly in database using existing function
             # Use create_task to avoid blocking the broadcast with proper error handling
             task = asyncio.create_task(
-                _store_with_error_handling(message_type, language, text, tenant_context)
+                _store_with_error_handling(message_type, language, text, tenant_context, sentence_context)
             )
             task.add_done_callback(lambda t: None if not t.exception() else logger.error(f"Storage task failed: {t.exception()}"))
             logger.debug(
@@ -126,7 +126,8 @@
     message_type: str, 
     language: str, 
     text: str, 
-    tenant_context: Dict[str, Any]
+    tenant_context: Dict[str, Any],
+    sentence_context: Optional[Dict[str, Any]] = None
 ) -> None:
     """
     Store transcript with proper error handling.
@@ -139,10 +140,11 @@
         language: Language code
         text: The text content to store
         tenant_context: Context containing room_id, mosque_id, etc.
+        sentence_context: Optional context containing sentence_id, is_complete, is_fragment
     """
     try:
         success = await store_transcript_in_database(
-            message_type, language, text, tenant_context
+            message_type, language, text, tenant_context, sentence_context
         )
         if not success:
             logger.warning(
```
\n### Deployment Actions\n
- âœ… Updated database.py with sentence_context support
- âœ… Updated broadcasting.py to pass sentence_context to storage
\n## Post-Deployment Steps\n
### 1. Database Migration
Apply the following migration to your Supabase database:
```sql
-- Add sentence context columns to transcripts table
ALTER TABLE transcripts 
ADD COLUMN IF NOT EXISTS sentence_id UUID;

ALTER TABLE transcripts 
ADD COLUMN IF NOT EXISTS is_complete BOOLEAN DEFAULT false;

ALTER TABLE transcripts 
ADD COLUMN IF NOT EXISTS is_fragment BOOLEAN DEFAULT true;

-- Add indexes for efficient querying
CREATE INDEX IF NOT EXISTS idx_transcripts_sentence_id 
ON transcripts(sentence_id) 
WHERE sentence_id IS NOT NULL;

CREATE INDEX IF NOT EXISTS idx_transcripts_session_sentence 
ON transcripts(session_id, sentence_id) 
WHERE sentence_id IS NOT NULL;
```
\n### 2. Frontend Deployment
The frontend changes are already in the Dev folder. Deploy these files:
- `src/integrations/supabase/types.ts` - Updated TypeScript types
- `src/components/SessionReplay.tsx` - Enhanced sentence processing
\n### 3. Server Restart
After deploying:
1. Commit and push the changes to your production repository
2. Render will automatically redeploy the service
3. Monitor logs for any errors during startup
\n### 4. Verification
After deployment:
- [ ] Start a new live monitoring session
- [ ] Check database for new columns in transcript entries
- [ ] Verify session replay shows same quality as live monitoring
\n### 5. Rollback Instructions
If issues occur:
1. Run: `bash deploy-sentence-context-update.sh --rollback`
2. Or manually restore from: `/mnt/c/Users/hassa/OneDrive/Desktop/0.2 Bayan/Dev/bayan-platform-admin-login/Backend/LiveKit-ai-translation/bayaan-server-production/backup_sentence_context_20250728_031206`



================================================
FILE: Dockerfile
================================================
# Use Python 3.10 as base image
FROM python:3.10-slim

# Set working directory
WORKDIR /app

# Install system dependencies for audio processing and LiveKit
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    portaudio19-dev \
    python3-dev \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements and install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir --root-user-action=ignore -r requirements.txt

# Copy application code
COPY . .

# Create non-root user for security
RUN useradd -m -u 1000 agent && chown -R agent:agent /app
USER agent

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
  CMD python -c "import asyncio; import sys; print('Agent is running')" || exit 1

# Default command to run the agent in production mode
CMD ["python", "main_production.py", "start"] 


================================================
FILE: environment.template
================================================
# Production Environment Configuration for Bayaan LiveKit Agent

# Environment
ENVIRONMENT=production

# LiveKit Configuration
LIVEKIT_URL=wss://your-livekit-server.com
LIVEKIT_API_KEY=your-livekit-api-key
LIVEKIT_API_SECRET=your-livekit-api-secret

# OpenAI Configuration
OPENAI_API_KEY=your-openai-api-key

# Speechmatics Configuration (optional)
SPEECHMATICS_API_KEY=your-speechmatics-api-key

# Supabase Configuration
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_SERVICE_ROLE_KEY=your-service-role-key
SUPABASE_ANON_KEY=your-anon-key

# Agent Configuration
AGENT_NAME=bayaan-transcriber
WORKER_TYPE=background
MAX_WORKERS=2
IDLE_TIMEOUT=300
PERSISTENT_MODE=true

# Logging Configuration
LOG_LEVEL=INFO

# Python Configuration
PYTHONPATH=/app
PYTHONUNBUFFERED=1 


================================================
FILE: FIX_AGENT_DISPATCH.md
================================================
# Agent Dispatch Fix - Root Cause Analysis

## Problem Summary
The July 28 version works because it successfully receives and accepts job requests from LiveKit. Newer deployments register but never receive job requests.

## Working Version (July 28) Characteristics:
- **Agent receives job request**: `received job request` appears in logs
- **Agent accepts request**: `âœ… Accepted job request for room`
- **Agent joins room**: Successfully connects with 2 participants
- **Real-time updates work**: Each word is sent immediately to frontend

## Broken Version Characteristics:
- **Agent registers**: `registered worker` appears in logs
- **No job requests**: Never sees `received job request`
- **Stuck waiting**: Agent ready but never called
- **Frontend shows 1 participant**: Agent never joins

## The Real Fix Needed:

### Option 1: Pin LiveKit Versions (Recommended)
Update `requirements.txt` to use exact versions from July 28:
```python
# LiveKit Core Dependencies - PINNED VERSIONS
livekit-agents==1.2.1  # Was >=1.0.0
livekit-plugins-openai==0.8.1  # Was >=0.8.0
livekit-plugins-speechmatics==0.6.1  # Was >=0.6.0
livekit-plugins-silero==0.6.1  # Was >=0.6.0
```

### Option 2: Fix Agent Request Function
The issue might be in how the agent accepts requests. Check if newer LiveKit versions changed the API:

```python
async def request_fnc(req: JobRequest):
    logger.info(f"ðŸŽ¯ Received job request for room: {req.room.name if req.room else 'unknown'}")
    logger.info(f"ðŸ“‹ Request details: job_id={req.id}, room_name={req.room.name if req.room else 'unknown'}")
    
    # Newer versions might need different accept parameters
    await req.accept(
        name="agent",
        identity="agent",
        # Add this for newer versions:
        auto_subscribe=AutoSubscribe.AUDIO_ONLY  # Might be required now
    )
    logger.info(f"âœ… Accepted job request for room: {req.room.name if req.room else 'unknown'}")
```

### Option 3: Check Agent Namespace
LiveKit might have changed how agents are dispatched. The working version shows:
- Worker ID: `AW_MKDehXBn9y5N` 
- Protocol: 16

Check if newer versions need explicit namespace or dispatch configuration.

## Why Domain Patch Isn't The Issue:

The July 28 version HAS the domain patch active and working:
```
[INFO] Speechmatics domain configured: broadcast
ðŸ“‹ Creating TranscriptionConfig with domain: broadcast
[INFO] TranscriptionConfig initialized with domain: broadcast
```

This proves the domain patch itself isn't causing the connection issue.

## Immediate Action Items:

1. **Compare installed package versions**:
   ```bash
   pip freeze | grep livekit
   ```
   Compare between working and broken deployments.

2. **Check LiveKit Cloud logs**:
   - See if job requests are being sent
   - Check for any dispatch errors

3. **Test with pinned versions**:
   - Use exact versions from July 28
   - This should immediately fix the issue

## The Core Issue:
**The agent registration/dispatch mechanism changed between LiveKit SDK versions.** The July 28 deployment uses an older SDK version that's compatible with how your frontend requests agents. Newer SDK versions have a different dispatch mechanism that's not receiving the job requests.

## Recommended Fix:
1. Pin all LiveKit dependencies to July 28 versions
2. Keep the domain patch (it works fine)
3. Don't worry about the deprecated API warnings
4. Deploy with these exact versions

This will give you a stable, working system while you investigate the proper way to use newer LiveKit SDK versions.


================================================
FILE: GHOST_FIX_DEPLOYMENT.md
================================================
# Ghost Session Fix Deployment Checklist

## Pre-Deployment:
- [ ] Run SQL migration in Supabase
- [ ] Backup production database.py
- [ ] Review database_integration.patch
- [ ] Test in development environment

## Integration Steps:
1. [ ] Apply database.py integration patch
2. [ ] Update main.py with health monitoring
3. [ ] Verify Python syntax: `python3 -m py_compile *.py`
4. [ ] Run local tests if available

## Deployment:
1. [ ] Commit changes to git
2. [ ] Push to production branch
3. [ ] Monitor Render deployment logs
4. [ ] Check for startup errors

## Post-Deployment Verification:
1. [ ] Check application logs for:
   - "âœ… Active session ensured" messages
   - "ðŸ§¹ Cleaned X stale sessions" messages
   - "ðŸ’“ Heartbeat updated" messages

2. [ ] Monitor database for ghost sessions:
   ```sql
   SELECT * FROM ghost_session_monitor;
   ```

3. [ ] Test session creation and cleanup:
   - Connect to a room
   - Verify session created
   - Disconnect
   - Verify session cleaned up

## Rollback Plan:
If issues occur:
1. Restore from backup: `backup_ghost_fix_*/`
2. Redeploy previous version on Render
3. Investigate logs for root cause



================================================
FILE: main.py
================================================
import asyncio
import logging
import json
import time
import re
import os
import uuid
from typing import Set, Any, Dict, Optional
from collections import defaultdict, deque
from datetime import datetime, timedelta

from enum import Enum
from dataclasses import dataclass, asdict

# Apply domain support patch BEFORE importing speechmatics
from speechmatics_domain_patch import patch_speechmatics_for_domain_support
patch_speechmatics_for_domain_support()

from livekit import rtc
from livekit.agents import (
    AutoSubscribe,
    JobContext,
    JobProcess,
    JobRequest,
    WorkerOptions,
    cli,
    stt,
    utils,
)
from livekit.plugins import silero, speechmatics
from livekit.plugins.speechmatics.types import TranscriptionConfig

# Import configuration
from config import get_config, ApplicationConfig

# Import database operations
from database import (
    ensure_active_session,
    store_transcript_in_database,
    query_room_by_name,
    get_active_session_for_room,
    broadcast_to_channel,
    close_database_connections,
    close_room_session,
    update_session_heartbeat
)

# Import text processing and translation helpers
from text_processing import extract_complete_sentences
from translation_helpers import translate_sentences

# Import Translator class
from translator import Translator

# Import broadcasting function
from broadcasting import broadcast_to_displays

# Import resource management
from resource_management import ResourceManager, TaskManager, STTStreamManager

# Import webhook handler for room context
try:
    from webhook_handler import get_room_context as get_webhook_room_context
except ImportError:
    # Webhook handler not available, use empty context
    def get_webhook_room_context(room_name: str):
        return {}


# Load configuration
config = get_config()

logger = logging.getLogger("transcriber")


@dataclass
class Language:
    code: str
    name: str
    flag: str


# Build languages dictionary from config
languages = {}
for code, lang_info in config.translation.supported_languages.items():
    languages[code] = Language(
        code=code,
        name=lang_info["name"],
        flag=lang_info["flag"]
    )

LanguageCode = Enum(
    "LanguageCode",  # Name of the Enum
    {lang.name: code for code, lang in languages.items()},  # Enum entries: name -> code mapping
)


# Translator class has been moved to translator.py


def prewarm(proc: JobProcess):
    proc.userdata["vad"] = silero.VAD.load()


async def entrypoint(job: JobContext):
    # Configure source language - ARABIC as default
    # This will be the language that users are actually speaking (host/speaker language)
    source_language = config.translation.default_source_language
    
    # Initialize resource manager
    resource_manager = ResourceManager()
    
    # Register heartbeat timeout callback
    async def on_participant_timeout(participant_id: str):
        logger.warning(f"ðŸ’” Participant {participant_id} timed out - initiating cleanup")
        # Could trigger cleanup here if needed
    
    resource_manager.heartbeat_monitor.register_callback(on_participant_timeout)
    
    # Start periodic session heartbeat task
    heartbeat_task = None
    async def update_session_heartbeat_periodic():
        """Periodically update the session heartbeat in the database."""
        while True:
            try:
                await asyncio.sleep(20)  # Update every 20 seconds
                
                session_id = tenant_context.get('session_id')
                if session_id:
                    success = await update_session_heartbeat(session_id)
                    if success:
                        logger.debug(f"ðŸ’“ Updated database session heartbeat for {session_id}")
                    else:
                        logger.warning(f"âš ï¸ Failed to update session heartbeat for {session_id}")
                else:
                    logger.debug("No session_id available for heartbeat update")
                    
            except asyncio.CancelledError:
                logger.info("ðŸ’” Session heartbeat task cancelled")
                break
            except Exception as e:
                logger.error(f"âŒ Error in session heartbeat task: {e}")
                await asyncio.sleep(5)  # Wait before retrying
    
    # Extract tenant context from room metadata or webhook handler
    tenant_context = {}
    try:
        # Try to query Supabase directly for room information
        if job.room and job.room.name:
            logger.info(f"ðŸ” Looking up room context for: {job.room.name}")
            
            # Check if database is configured
            logger.info(f"ðŸ”‘ Supabase URL: {config.supabase.url}")
            logger.info(f"ðŸ”‘ Supabase key available: {'Yes' if config.supabase.service_role_key else 'No'}")
            
            if config.supabase.service_role_key:
                try:
                    # Query room by LiveKit room name using the new database module
                    logger.info(f"ðŸ” Querying database for room: {job.room.name}")
                    # Query room directly without task wrapper
                    room_data = await query_room_by_name(job.room.name)
                    
                    if room_data:
                        tenant_context = {
                            "room_id": room_data.get("id"),
                            "mosque_id": room_data.get("mosque_id"),
                            "room_title": room_data.get("Title"),
                            "transcription_language": room_data.get("transcription_language", "ar"),
                            "translation_language": room_data.get("translation__language", "nl"),
                            "context_window_size": room_data.get("context_window_size", 6),
                            "created_at": room_data.get("created_at")
                        }
                        # Also store the double underscore version for compatibility
                        if room_data.get("translation__language"):
                            tenant_context["translation__language"] = room_data.get("translation__language")
                        
                        logger.info(f"âœ… Found room in database: room_id={tenant_context.get('room_id')}, mosque_id={tenant_context.get('mosque_id')}")
                        logger.info(f"ðŸ—£ï¸ Languages: transcription={tenant_context.get('transcription_language')}, translation={tenant_context.get('translation_language')} (or {tenant_context.get('translation__language')})")
                        
                        # Try to get active session for this room
                        session_id = await get_active_session_for_room(tenant_context['room_id'])
                        if session_id:
                            tenant_context["session_id"] = session_id
                            logger.info(f"ðŸ“ Found active session: {tenant_context['session_id']}")
                except Exception as e:
                    logger.warning(f"âš ï¸ Could not query Supabase: {e}")
        
        # Fallback to webhook handler if available
        if not tenant_context:
            webhook_context = get_webhook_room_context(job.room.name if job.room else "")
            if webhook_context:
                tenant_context = {
                    "room_id": webhook_context.get("room_id"),
                    "mosque_id": webhook_context.get("mosque_id"),
                    "session_id": webhook_context.get("session_id"),
                    "room_title": webhook_context.get("room_title"),
                    "transcription_language": webhook_context.get("transcription_language", "ar"),
                    "translation_language": webhook_context.get("translation_language", "nl"),
                    "created_at": webhook_context.get("created_at")
                }
                logger.info(f"ðŸ¢ Tenant context from webhook handler: mosque_id={tenant_context.get('mosque_id')}, room_id={tenant_context.get('room_id')}")
        
        # Fallback to room metadata if available
        if not tenant_context and job.room and job.room.metadata:
            try:
                metadata = json.loads(job.room.metadata)
                tenant_context = {
                    "room_id": metadata.get("room_id"),
                    "mosque_id": metadata.get("mosque_id"),
                    "session_id": metadata.get("session_id"),
                    "room_title": metadata.get("room_title"),
                    "transcription_language": metadata.get("transcription_language", "ar"),
                    "translation_language": metadata.get("translation_language", "nl"),
                    "created_at": metadata.get("created_at")
                }
                logger.info(f"ðŸ¢ Tenant context from room metadata: mosque_id={tenant_context.get('mosque_id')}, room_id={tenant_context.get('room_id')}")
            except:
                pass
        
        # Final fallback to default context with hardcoded values for testing
        if not tenant_context:
            logger.warning(f"âš ï¸ No tenant context available for room: {job.room.name if job.room else 'unknown'}")
            # TEMPORARY: Use hardcoded values for mosque_546012 rooms
            if job.room and f"mosque_{config.test_mosque_id}" in job.room.name:
                tenant_context = {
                    "room_id": config.test_room_id,
                    "mosque_id": config.test_mosque_id,
                    "session_id": None,
                    "transcription_language": "ar",
                    "translation_language": "nl"
                }
                logger.info(f"ðŸ”§ Using hardcoded tenant context for testing: mosque_id={tenant_context['mosque_id']}, room_id={tenant_context['room_id']}")
            else:
                tenant_context = {
                    "room_id": None,
                    "mosque_id": int(os.getenv('DEFAULT_MOSQUE_ID', str(config.default_mosque_id))),
                    "session_id": None,
                    "transcription_language": "ar",
                    "translation_language": "nl"
                }
    except Exception as e:
        logger.warning(f"âš ï¸ Could not extract tenant context: {e}")
    
    # Configure Speechmatics STT with room-specific settings
    # Use tenant_context which already has room configuration
    room_config = None
    if tenant_context and tenant_context.get('room_id'):
        # We already have the room data in tenant_context from earlier query
        room_config = tenant_context
        logger.info(f"ðŸ“‹ Using room-specific configuration from context: "
                  f"lang={room_config.get('transcription_language', 'ar')}, "
                  f"target={room_config.get('translation_language', 'nl')}, "
                  f"delay={room_config.get('max_delay', 2.0)}, "
                  f"punct={room_config.get('punctuation_sensitivity', 0.5)}, "
                  f"context_window={room_config.get('context_window_size', 6)}")
        
        # If we need full room data and it's not in context, query it
        if not room_config.get('max_delay'):
            try:
                full_room_data = await query_room_by_name(job.room.name if job.room else None)
                if full_room_data:
                    # Merge the full room data with tenant context
                    room_config.update({
                        'max_delay': full_room_data.get('max_delay'),
                        'punctuation_sensitivity': full_room_data.get('punctuation_sensitivity'),
                        'translation__language': full_room_data.get('translation__language'),
                        'context_window_size': full_room_data.get('context_window_size', 6)
                    })
                    logger.info(f"ðŸ“‹ Fetched additional room config: delay={room_config.get('max_delay')}, punct={room_config.get('punctuation_sensitivity')}, context_window={room_config.get('context_window_size')}")
            except Exception as e:
                logger.warning(f"Failed to fetch additional room config: {e}")
    
    # Create STT configuration with room-specific overrides
    stt_config = config.speechmatics.with_room_settings(room_config)
    
    # Get domain from room config (default to 'broadcast' for sermons)
    domain = room_config.get('speechmatics_domain', 'broadcast') if room_config else 'broadcast'
    logger.info(f"[INFO] Speechmatics domain configured: {domain}")
    
    # Build TranscriptionConfig parameters with domain support
    transcription_params = {
        "language": stt_config.language,
        "operating_point": stt_config.operating_point,
        "enable_partials": stt_config.enable_partials,
        "max_delay": stt_config.max_delay,
        "punctuation_overrides": {"sensitivity": stt_config.punctuation_sensitivity},
        "diarization": stt_config.diarization,
        "domain": domain  # Our patch makes this work!
    }
    
    logger.info(f"ðŸ“‹ Creating TranscriptionConfig with domain: {domain}")
    
    # Initialize STT provider with configured settings
    stt_provider = speechmatics.STT(
        transcription_config=TranscriptionConfig(**transcription_params)
    )
    
    # Update source language based on room config
    source_language = config.translation.get_source_language(room_config)
    logger.info(f"ðŸ—£ï¸ STT configured for {languages[source_language].name} speech recognition")
    
    translators = {}
    
    # Get target language from room config or use default
    target_language = config.translation.get_target_language(room_config)
    logger.info(f"ðŸŽ¯ Target language resolved to: '{target_language}' (from room_config: {room_config.get('translation_language') if room_config else 'None'} or {room_config.get('translation__language') if room_config else 'None'})")
    
    # Create translator for the configured target language
    if target_language in languages:
        # Get language enum dynamically
        lang_info = languages[target_language]
        lang_enum = getattr(LanguageCode, lang_info.name)
        translators[target_language] = Translator(job.room, lang_enum, tenant_context, broadcast_to_displays)
        logger.info(f"ðŸ“ Initialized {lang_info.name} translator ({target_language})")
    else:
        logger.warning(f"âš ï¸ Target language '{target_language}' not supported, falling back to Dutch")
        dutch_enum = getattr(LanguageCode, 'Dutch')
        translators["nl"] = Translator(job.room, dutch_enum, tenant_context, broadcast_to_displays)
    
    # Sentence accumulation for proper sentence-by-sentence translation
    accumulated_text = ""  # Accumulates text until we get a complete sentence
    last_final_transcript = ""  # Keep track of the last final transcript to avoid duplicates
    current_sentence_id = None  # Track current sentence being built
    
    # Track participant tasks for cleanup
    participant_tasks = {}  # participant_id -> task
    
    logger.info(f"ðŸš€ Starting entrypoint for room: {job.room.name if job.room else 'unknown'}")
    logger.info(f"ðŸ” Translators dict ID: {id(translators)}")
    logger.info(f"ðŸŽ¯ Configuration: {languages[source_language].name} â†’ {languages.get(target_language, languages['nl']).name}")
    logger.info(f"âš™ï¸ STT Settings: delay={stt_config.max_delay}s, punctuation={stt_config.punctuation_sensitivity}")

    async def _forward_transcription(
        stt_stream: stt.SpeechStream,
        track: rtc.Track,
    ):
        """Forward the transcription and log the transcript in the console"""
        nonlocal accumulated_text, last_final_transcript, current_sentence_id
        
        try:
            async for ev in stt_stream:
                # Only process final transcripts since partials are disabled
                if ev.type == stt.SpeechEventType.FINAL_TRANSCRIPT:
                    final_text = ev.alternatives[0].text.strip()
                    print(" -> ", final_text)
                    logger.info(f"Final Arabic transcript: {final_text}")

                    if final_text and final_text != last_final_transcript:
                        last_final_transcript = final_text
                        
                        # Publish final transcription for the original language (Arabic)
                        try:
                            final_segment = rtc.TranscriptionSegment(
                                id=utils.misc.shortuuid("SG_"),
                                text=final_text,
                                start_time=0,
                                end_time=0,
                                language=source_language,  # Arabic
                                final=True,
                            )
                            final_transcription = rtc.Transcription(
                                job.room.local_participant.identity, "", [final_segment]
                            )
                            await job.room.local_participant.publish_transcription(final_transcription)
                            
                            logger.info(f"âœ… Published final {languages[source_language].name} transcription: '{final_text}'")
                        except Exception as e:
                            logger.error(f"âŒ Failed to publish final transcription: {str(e)}")
                        
                        # Generate sentence ID if we don't have one for this sentence
                        if not current_sentence_id:
                            current_sentence_id = str(uuid.uuid4())
                        
                        # Broadcast final transcription text for real-time display
                        print(f"ðŸ“¡ Broadcasting Arabic text to frontend: '{final_text}'")
                        # Broadcast directly without task wrapper
                        await broadcast_to_displays(
                            "transcription", 
                            source_language, 
                            final_text, 
                            tenant_context,
                            sentence_context={
                                "sentence_id": current_sentence_id,
                                "is_complete": False,  # Will be marked complete when sentence ends
                                "is_fragment": True
                            }
                        )
                        
                        # Handle translation logic
                        if translators:
                            # SIMPLE ACCUMULATION LOGIC - ONLY APPEND, NEVER REPLACE
                            if accumulated_text:
                                # ALWAYS append new final transcript to existing accumulated text
                                accumulated_text = accumulated_text.strip() + " " + final_text
                            else:
                                # First transcript - start accumulation
                                accumulated_text = final_text
                            
                            logger.info(f"ðŸ“ Updated accumulated Arabic text: '{accumulated_text}'")
                            
                            # Extract complete sentences from accumulated text
                            complete_sentences, remaining_text = extract_complete_sentences(accumulated_text)
                            
                            # Handle special punctuation completion signal
                            if complete_sentences and complete_sentences[0] == "PUNCTUATION_COMPLETE":
                                if accumulated_text.strip():
                                    # Complete the accumulated sentence with this punctuation
                                    print(f"ðŸ“ PUNCTUATION SIGNAL: Completing accumulated text: '{accumulated_text}'")
                                    
                                    # Broadcast sentence completion
                                    if current_sentence_id:
                                        await broadcast_to_displays(
                                            "transcription", 
                                            source_language, 
                                            accumulated_text, 
                                            tenant_context,
                                            sentence_context={
                                                "sentence_id": current_sentence_id,
                                                "is_complete": True,
                                                "is_fragment": False
                                            }
                                        )
                                    
                                    # Translate the completed sentence (don't include the punctuation marker)
                                    await translate_sentences([accumulated_text], translators, source_language, current_sentence_id)
                                    
                                    # Clear accumulated text as sentence is now complete
                                    accumulated_text = ""
                                    current_sentence_id = None
                                    print(f"ðŸ“ Cleared accumulated text after punctuation completion")
                                else:
                                    print(f"âš ï¸ Received punctuation completion signal but no accumulated text")
                            elif complete_sentences:
                                # We have complete sentences - translate them immediately
                                print(f"ðŸŽ¯ Found {len(complete_sentences)} complete Arabic sentences: {complete_sentences}")
                                
                                # Broadcast each complete sentence
                                for sentence in complete_sentences:
                                    sentence_id = current_sentence_id if len(complete_sentences) == 1 else str(uuid.uuid4())
                                    await broadcast_to_displays(
                                        "transcription", 
                                        source_language, 
                                        sentence, 
                                        tenant_context,
                                        sentence_context={
                                            "sentence_id": sentence_id,
                                            "is_complete": True,
                                            "is_fragment": False
                                        }
                                    )
                                    # Translate complete sentences with sentence ID
                                    await translate_sentences([sentence], translators, source_language, sentence_id)
                                
                                # Update accumulated text to only remaining incomplete text
                                accumulated_text = remaining_text
                                # Generate new sentence ID for the remaining text
                                current_sentence_id = str(uuid.uuid4()) if remaining_text else None
                                print(f"ðŸ“ Updated accumulated Arabic text after sentence extraction: '{accumulated_text}'")
                            
                            # Log remaining incomplete text (no delayed translation)
                            if accumulated_text.strip():
                                logger.info(f"ðŸ“ Incomplete Arabic text remaining: '{accumulated_text}'")
                                # Note: Incomplete text will be translated when the next sentence completes
                        else:
                            logger.warning(f"âš ï¸ No translators available in room {job.room.name}, only {languages[source_language].name} transcription published")
                    else:
                        logger.debug("Empty or duplicate transcription, skipping")
        except Exception as e:
            logger.error(f"STT transcription error: {str(e)}")
            raise

    async def transcribe_track(participant: rtc.RemoteParticipant, track: rtc.Track):
        logger.info(f"ðŸŽ¤ Starting Arabic transcription for participant {participant.identity}, track {track.sid}")
        
        try:
            audio_stream = rtc.AudioStream(track)
            
            # Use context manager for STT stream
            async with resource_manager.stt_manager.create_stream(stt_provider, participant.identity) as stt_stream:
                # Create transcription task with tracking
                stt_task = resource_manager.task_manager.create_task(
                    _forward_transcription(stt_stream, track),
                    name=f"transcribe-{participant.identity}",
                    metadata={"participant": participant.identity, "track": track.sid}
                )
                
                frame_count = 0
                try:
                    async for ev in audio_stream:
                        frame_count += 1
                        if frame_count % 100 == 0:  # Log every 100 frames to avoid spam
                            logger.debug(f"ðŸ”Š Received audio frame #{frame_count} from {participant.identity}")
                            # Update heartbeat every 100 frames
                            await resource_manager.heartbeat_monitor.update_heartbeat(
                                participant.identity, 
                                tenant_context.get('session_id')
                            )
                        stt_stream.push_frame(ev.frame)
                except asyncio.CancelledError:
                    logger.debug(f"Audio stream cancelled for {participant.identity}")
                    raise
                finally:
                    logger.warning(f"ðŸ”‡ Audio stream ended for {participant.identity}")
                
                # Cancel the transcription task if still running
                if not stt_task.done():
                    stt_task.cancel()
                    try:
                        await stt_task
                    except asyncio.CancelledError:
                        logger.debug(f"STT task cancelled for {participant.identity}")
                        
        except Exception as e:
            logger.error(f"âŒ Transcription track error for {participant.identity}: {str(e)}")
        
        logger.info(f"ðŸ§¹ Transcription cleanup completed for {participant.identity}")

    @job.room.on("track_subscribed")
    def on_track_subscribed(
        track: rtc.Track,
        publication: rtc.TrackPublication,
        participant: rtc.RemoteParticipant,
    ):
        logger.info(f"ðŸŽµ Track subscribed: {track.kind} from {participant.identity} (track: {track.sid})")
        logger.info(f"Track details - muted: {publication.muted}")
        if track.kind == rtc.TrackKind.KIND_AUDIO:
            logger.info(f"âœ… Adding Arabic transcriber for participant: {participant.identity}")
            task = resource_manager.task_manager.create_task(
                transcribe_track(participant, track),
                name=f"track-handler-{participant.identity}",
                metadata={"participant": participant.identity, "track": track.sid}
            )
            # Store task reference for cleanup
            participant_tasks[participant.identity] = task
        else:
            logger.info(f"âŒ Ignoring non-audio track: {track.kind}")

    @job.room.on("track_published")
    def on_track_published(publication: rtc.TrackPublication, participant: rtc.RemoteParticipant):
        logger.info(f"ðŸ“¡ Track published: {publication.kind} from {participant.identity} (track: {publication.sid})")
        logger.info(f"Publication details - muted: {publication.muted}")

    @job.room.on("track_unpublished") 
    def on_track_unpublished(publication: rtc.TrackPublication, participant: rtc.RemoteParticipant):
        logger.info(f"ðŸ“¡ Track unpublished: {publication.kind} from {participant.identity}")

    @job.room.on("participant_connected")
    def on_participant_connected(participant: rtc.RemoteParticipant):
        logger.info(f"ðŸ‘¥ Participant connected: {participant.identity}")
        
        # Try to extract metadata from participant if available
        if hasattr(participant, 'metadata') and participant.metadata:
            try:
                participant_metadata = json.loads(participant.metadata)
                if participant_metadata:
                    # Update tenant context with participant metadata
                    tenant_context.update({
                        "room_id": participant_metadata.get("room_id", tenant_context.get("room_id")),
                        "mosque_id": participant_metadata.get("mosque_id", tenant_context.get("mosque_id")),
                        "session_id": participant_metadata.get("session_id", tenant_context.get("session_id")),
                        "room_title": participant_metadata.get("room_title", tenant_context.get("room_title"))
                    })
                    logger.info(f"ðŸ“‹ Updated tenant context from participant metadata: {tenant_context}")
                    
                    # Update all translators with new context
                    for translator in translators.values():
                        translator.tenant_context = tenant_context
                        
                    # If we got a new session_id and don't have heartbeat task, start it
                    if participant_metadata.get("session_id") and not heartbeat_task:
                        heartbeat_task = resource_manager.task_manager.create_task(
                            update_session_heartbeat_periodic(),
                            name="session-heartbeat",
                            metadata={"session_id": participant_metadata.get("session_id")}
                        )
                        logger.info(f"ðŸ’“ Started session heartbeat task for new session {participant_metadata.get('session_id')}")
            except Exception as e:
                logger.debug(f"Could not parse participant metadata: {e}")

    @job.room.on("participant_disconnected")
    def on_participant_disconnected(participant: rtc.RemoteParticipant):
        logger.info(f"ðŸ‘¥ Participant disconnected: {participant.identity}")
        
        # Cancel participant's transcription task if it exists
        if participant.identity in participant_tasks:
            task = participant_tasks[participant.identity]
            if not task.done():
                logger.info(f"ðŸš« Cancelling transcription task for {participant.identity}")
                task.cancel()
            del participant_tasks[participant.identity]
        
        # Remove from heartbeat monitoring
        resource_manager.heartbeat_monitor.remove_participant(participant.identity)
        
        # Immediately close any STT streams for this participant
        async def cleanup_participant_streams():
            try:
                await resource_manager.stt_manager.close_participant_stream(participant.identity)
                logger.info(f"âœ… STT stream closed for disconnected participant {participant.identity}")
            except Exception as e:
                logger.error(f"Error closing STT stream for {participant.identity}: {e}")
        
        # Schedule immediate cleanup
        resource_manager.task_manager.create_task(
            cleanup_participant_streams(),
            name=f"cleanup-stt-{participant.identity}"
        )
        
        # Log current resource statistics
        resource_manager.log_stats()
        logger.info(f"ðŸ§¹ Participant cleanup initiated for {participant.identity}")

    @job.room.on("participant_attributes_changed")
    def on_attributes_changed(
        changed_attributes: dict[str, str], participant: rtc.Participant
    ):
        """
        When participant attributes change, handle new translation requests.
        """
        logger.info(f"ðŸŒ Participant {participant.identity} attributes changed: {changed_attributes}")
        lang = changed_attributes.get("captions_language", None)
        if lang:
            if lang == source_language:
                logger.info(f"âœ… Participant {participant.identity} requested {languages[source_language].name} (source language - Arabic)")
            elif lang in translators:
                logger.info(f"âœ… Participant {participant.identity} requested existing language: {lang}")
                logger.info(f"ðŸ“Š Current translators for this room: {list(translators.keys())}")
            else:
                # Check if the language is supported and different from source language
                if lang in languages:
                    try:
                        # Create a translator for the requested language using the language enum
                        language_obj = languages[lang]
                        language_enum = getattr(LanguageCode, language_obj.name)
                        translators[lang] = Translator(job.room, language_enum, tenant_context, broadcast_to_displays)
                        logger.info(f"ðŸ†• Added translator for ROOM {job.room.name} (requested by {participant.identity}), language: {language_obj.name}")
                        logger.info(f"ðŸ¢ Translator created with tenant context: mosque_id={tenant_context.get('mosque_id')}")
                        logger.info(f"ðŸ“Š Total translators for room {job.room.name}: {len(translators)} -> {list(translators.keys())}")
                        logger.info(f"ðŸ” Translators dict ID: {id(translators)}")
                        
                        # Debug: Verify the translator was actually added
                        if lang in translators:
                            logger.info(f"âœ… Translator verification: {lang} successfully added to room translators")
                        else:
                            logger.error(f"âŒ Translator verification FAILED: {lang} not found in translators dict")
                            
                    except Exception as e:
                        logger.error(f"âŒ Error creating translator for {lang}: {str(e)}")
                else:
                    logger.warning(f"âŒ Unsupported language requested by {participant.identity}: {lang}")
                    logger.info(f"ðŸ’¡ Supported languages: {list(languages.keys())}")
        else:
            logger.debug(f"No caption language change for participant {participant.identity}")

    logger.info("Connecting to room...")
    await job.connect(auto_subscribe=AutoSubscribe.AUDIO_ONLY)
    logger.info(f"Successfully connected to room: {job.room.name}")
    logger.info(f"ðŸ“¡ Real-time transcription data will be sent via Supabase Broadcast")
    
    # Start the heartbeat task after connection
    if tenant_context.get('session_id'):
        heartbeat_task = resource_manager.task_manager.create_task(
            update_session_heartbeat_periodic(),
            name="session-heartbeat",
            metadata={"session_id": tenant_context.get('session_id')}
        )
        logger.info(f"ðŸ’“ Started session heartbeat task for session {tenant_context.get('session_id')}")
    
    # Debug room state after connection
    logger.info(f"Room participants: {len(job.room.remote_participants)}")
    for participant in job.room.remote_participants.values():
        logger.info(f"Participant: {participant.identity}")
        logger.info(f"  Audio tracks: {len(participant.track_publications)}")
        for sid, pub in participant.track_publications.items():
            logger.info(f"    Track {sid}: {pub.kind}, muted: {pub.muted}")

    # Also check local participant
    logger.info(f"Local participant: {job.room.local_participant.identity}")
    logger.info(f"Local participant tracks: {len(job.room.local_participant.track_publications)}")

    @job.room.local_participant.register_rpc_method("get/languages")
    async def get_languages(data: rtc.RpcInvocationData):
        languages_list = [asdict(lang) for lang in languages.values()]
        return json.dumps(languages_list)
    
    @job.room.local_participant.register_rpc_method("request/cleanup")
    async def request_cleanup(data: rtc.RpcInvocationData):
        """Handle cleanup request from frontend"""
        try:
            payload = json.loads(data.payload)
            reason = payload.get('reason', 'unknown')
            session_id = payload.get('session_id')
            
            logger.info(f"ðŸ§¹ Cleanup requested by frontend: reason={reason}, session_id={session_id}")
            
            # Initiate graceful shutdown
            asyncio.create_task(perform_graceful_cleanup(reason, session_id))
            
            return json.dumps({
                "success": True,
                "message": "Cleanup initiated"
            })
        except Exception as e:
            logger.error(f"Error handling cleanup request: {e}")
            return json.dumps({
                "success": False,
                "error": str(e)
            })
    
    async def perform_graceful_cleanup(reason: str, session_id: Optional[str]):
        """Perform graceful cleanup when requested by frontend"""
        logger.info(f"ðŸ›‘ Starting graceful cleanup: {reason}")
        
        # Log current resource state
        resource_manager.log_stats()
        
        # If session_id provided, update it in database
        if session_id and tenant_context.get('room_id'):
            try:
                from database import query_database
                result = await query_database(
                    "SELECT cleanup_session_idempotent(%s, %s, %s)",
                    [session_id, f"frontend_{reason}", datetime.utcnow()]
                )
                logger.info(f"Session cleanup result: {result}")
            except Exception as e:
                logger.error(f"Error updating session: {e}")
        
        # Shutdown all resources
        await resource_manager.shutdown()
        
        # Verify cleanup is complete
        verification = await resource_manager.verify_cleanup_complete()
        logger.info(f"ðŸ” Cleanup verification: {verification}")
        
        # Disconnect from room (this will trigger on_room_disconnected)
        await job.room.disconnect()
        
        logger.info("âœ… Graceful cleanup completed")

    @job.room.on("disconnected")
    def on_room_disconnected():
        """Handle room disconnection - cleanup all resources"""
        logger.info("ðŸšª Room disconnected, starting cleanup...")
        
        # Create async task for cleanup
        async def cleanup():
            # Log final resource statistics
            resource_manager.log_stats()
            
            # Cancel heartbeat task first if it exists
            if heartbeat_task and not heartbeat_task.done():
                heartbeat_task.cancel()
                try:
                    await heartbeat_task
                except asyncio.CancelledError:
                    logger.debug("Heartbeat task cancelled")
            
            # Shutdown resource manager (cancels all tasks, closes all streams)
            await resource_manager.shutdown()
            
            # Close the room session in database if we have a session_id
            try:
                session_id = tenant_context.get('session_id') if tenant_context else None
                if session_id:
                    logger.info(f"ðŸ”’ Closing database session: {session_id}")
                    await close_room_session(session_id)
                else:
                    logger.warning("âš ï¸ No session_id found to close")
            except Exception as e:
                logger.error(f"Failed to close room session: {e}")
            
            # Close database connections
            try:
                await close_database_connections()
                logger.info("âœ… Database connections closed")
                
                # Force cleanup of any remaining aiohttp sessions
                import gc
                import aiohttp
                
                # Find and close any unclosed ClientSessions
                for obj in gc.get_objects():
                    if isinstance(obj, aiohttp.ClientSession) and not obj.closed:
                        logger.warning(f"âš ï¸ Found unclosed ClientSession, closing it: {obj}")
                        await obj.close()
                
                # Force garbage collection
                gc.collect()
                await asyncio.sleep(0.1)  # Give time for cleanup
            except Exception as e:
                logger.debug(f"Database cleanup error: {e}")
            
            # Final verification
            verification = await resource_manager.verify_cleanup_complete()
            logger.info(f"ðŸ” Final cleanup verification: {verification}")
            
            logger.info("âœ… Room cleanup completed")
        
        # Run cleanup in the event loop
        asyncio.create_task(cleanup())


async def request_fnc(req: JobRequest):
    logger.info(f"ðŸŽ¯ Received job request for room: {req.room.name if req.room else 'unknown'}")
    logger.info(f"ðŸ“‹ Request details: job_id={req.id}, room_name={req.room.name if req.room else 'unknown'}")
    await req.accept(
        name="agent",
        identity="agent",
    )
    logger.info(f"âœ… Accepted job request for room: {req.room.name if req.room else 'unknown'}")


if __name__ == "__main__":
    cli.run_app(
        WorkerOptions(
            entrypoint_fnc=entrypoint, prewarm_fnc=prewarm, request_fnc=request_fnc
        )
    )


================================================
FILE: main_integration.md
================================================
# Main.py Integration for Ghost Session Fix

## Required Changes:

### 1. Import the health monitor (at top of file):
```python
from database import get_health_monitor, update_session_heartbeat_with_monitor
```

### 2. Initialize health monitor (in entrypoint function):
```python
# After initializing resource_manager
health_monitor = get_health_monitor()
```

### 3. Update the heartbeat periodic function:
Replace the existing `update_session_heartbeat_periodic` function with:

```python
async def update_session_heartbeat_periodic():
    """Periodically update session heartbeat with health monitoring."""
    while not stop_heartbeat:
        try:
            if tenant_context and tenant_context.get('session_id'):
                session_id = tenant_context['session_id']
                
                # Use enhanced heartbeat with monitoring
                success = await update_session_heartbeat_with_monitor(session_id)
                
                if not success:
                    logger.error(f"Session {session_id} health check failed - may need cleanup")
                    # The monitor will handle cleanup if needed
                    
            await asyncio.sleep(30)
        except Exception as e:
            logger.error(f"Heartbeat task error: {e}")
            await asyncio.sleep(30)
```

### 4. Add cleanup on connect (in entrypoint, after room join):
```python
# Clean any stale sessions for this room on connect
if tenant_context.get('room_id'):
    logger.info("Checking for stale sessions on connect...")
    # The atomic session creation will handle cleanup automatically
```

## Testing the Integration:

1. Check logs for "ðŸ§¹ Cleaned X stale sessions" messages
2. Monitor for "Session health check failed" warnings
3. Verify no ghost sessions in database after disconnects



================================================
FILE: main_production.py
================================================
#!/usr/bin/env python3
"""
Production-ready LiveKit Agent for Bayaan Translation Service
Optimized for Render deployment as a background worker
"""

import asyncio
import logging
import os
import sys
import signal
from datetime import datetime

# Setup production logging
def setup_production_logging():
    """Configure logging for production."""
    log_level = os.getenv('LOG_LEVEL', 'INFO').upper()
    
    logging.basicConfig(
        level=getattr(logging, log_level),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(sys.stdout)
        ]
    )

def health_check() -> bool:
    """Health check endpoint for Render."""
    try:
        # Basic health check - verify environment variables are set
        required_vars = [
            'LIVEKIT_URL', 'LIVEKIT_API_KEY', 'LIVEKIT_API_SECRET',
            'OPENAI_API_KEY', 'SUPABASE_URL', 'SUPABASE_SERVICE_ROLE_KEY'
        ]
        
        for var in required_vars:
            if not os.getenv(var):
                print(f"Missing required environment variable: {var}")
                return False
        
        print("Health check passed - all required environment variables are set")
        return True
        
    except Exception as e:
        print(f"Health check failed: {e}")
        return False

def main():
    """Main production entry point."""
    # Setup production logging
    setup_production_logging()
    
    logger = logging.getLogger(__name__)
    logger.info("Starting Bayaan LiveKit Agent for production deployment")
    
    # Set production environment
    os.environ['ENVIRONMENT'] = 'production'
    
    try:
        # Import and run the agent directly
        from livekit.agents import WorkerOptions, cli
        from main import entrypoint, prewarm, request_fnc
        
        # Production worker configuration
        worker_opts = WorkerOptions(
            entrypoint_fnc=entrypoint,
            prewarm_fnc=prewarm,
            request_fnc=request_fnc
        )
        
        logger.info("Starting LiveKit CLI with production configuration")
        
        # Run the agent - this handles its own event loop
        cli.run_app(worker_opts)
        
    except Exception as e:
        logger.error(f"Fatal error in production agent: {e}")
        sys.exit(1)

if __name__ == "__main__":
    # Handle different command line arguments
    if len(sys.argv) > 1:
        if sys.argv[1] == "health":
            # Health check endpoint
            is_healthy = health_check()
            sys.exit(0 if is_healthy else 1)
        elif sys.argv[1] == "start":
            # Start the agent
            main()
        else:
            # Default to original main.py behavior
            from main import *
            # Run with original CLI
            from livekit.agents import cli, WorkerOptions
            cli.run_app(WorkerOptions(
                entrypoint_fnc=entrypoint,
                prewarm_fnc=prewarm,
                request_fnc=request_fnc
            ))
    else:
        # Run the main agent
        main() 


================================================
FILE: merge_report_20250727_213400.md
================================================
# Server_Dev â†’ Production Update Report
**Date:** Sun Jul 27 21:34:00 CEST 2025
**Source:** /mnt/c/Users/hassa/OneDrive/Desktop/0.2 Bayan/bayaan-server-production/server_dev
**Target:** /mnt/c/Users/hassa/OneDrive/Desktop/0.2 Bayan/bayaan-server-production

## Update Summary

### Backup Location\n`/mnt/c/Users/hassa/OneDrive/Desktop/0.2 Bayan/bayaan-server-production/backup_20250727_213400`\n
\n### Simple File Updates\n
- **Updated:** `config.py`
- **Updated:** `prompt_builder.py`
- **Updated:** `translator.py`
\n### Complex File Handling\n
- **main.py:** Differences detected - production optimizations preserved
  - Kept production's interim transcript handling
  - Kept production's simplified cleanup approach
  - Review `/tmp/main_diff.txt` for detailed differences
- **database.py:** New session management functions detected
  - Contains heartbeat monitoring functions not used in production
  - Manual review recommended
\n### New Files Analysis\n
- **Excluded:** `database_cleanup_fix.py` (development/cleanup file)
\n### Dependencies Update\n
- New dependency: `aiohttp
`
- New dependency: `asyncpg`
- New dependency: `fastapi
`
- New dependency: `openai
`
- New dependency: `uvicorn
`
\nâš ï¸  **Action Required:** Review and add new dependencies to production requirements.txt
\n## Post-Update Recommendations\n
### Manual Review Required:
1. **main.py** - Review differences between dev and production versions
2. **database.py** - Check if new session management functions are needed
3. **New files** - Evaluate any new files from server_dev for inclusion
4. **Dependencies** - Review and update requirements.txt if needed

### Testing Checklist:
- [ ] Run local tests with updated code
- [ ] Verify WebSocket connections work correctly
- [ ] Test transcript handling (both final and interim)
- [ ] Confirm database operations function properly
- [ ] Check resource cleanup on disconnection

### Deployment Steps:
1. Review this report and the update log
2. Manually review complex files if needed
3. Run `git status` to see all changes
4. Test locally if possible
5. Commit changes with descriptive message
6. Deploy to Render following standard procedure

### Rollback Instructions:
If issues occur, run: `bash update_from_server_dev.sh --rollback`

**Backup Location:** `/mnt/c/Users/hassa/OneDrive/Desktop/0.2 Bayan/bayaan-server-production/backup_20250727_213400`
**Log File:** `/mnt/c/Users/hassa/OneDrive/Desktop/0.2 Bayan/bayaan-server-production/update_server_dev_20250727_213400.log`



================================================
FILE: prompt_builder.py
================================================
"""
Prompt Builder for customizable translation prompts.
Handles loading and formatting of prompt templates with variable substitution.
"""
import logging
from typing import Dict, Optional, Any
import json

from config import get_config
from database import query_prompt_template_for_room

logger = logging.getLogger("transcriber.prompt_builder")
config = get_config()


class PromptBuilder:
    """
    Builds customized translation prompts based on templates and room configuration.
    """
    
    # Default fallback prompt if no template is found
    DEFAULT_PROMPT = (
        "You are an expert simultaneous interpreter. Your task is to translate from {source_lang} to {target_lang}. "
        "Provide a direct and accurate translation of the user's input. "
        "Do not add any additional commentary, explanations, or introductory phrases. "
        "Be concise for real-time delivery."
    )
    
    def __init__(self):
        """Initialize the prompt builder."""
        logger.info("ðŸŽ¨ PromptBuilder initialized")
    
    async def get_prompt_for_room(
        self, 
        room_id: Optional[int],
        source_lang: str,
        target_lang: str,
        room_config: Optional[Dict[str, Any]] = None
    ) -> str:
        """
        Get the appropriate prompt for a room with variable substitution.
        
        Args:
            room_id: The room ID to get prompt for
            source_lang: Source language name (e.g., "Arabic")
            target_lang: Target language name (e.g., "Dutch")
            room_config: Optional room configuration with additional context
            
        Returns:
            Formatted prompt string ready for use
        """
        try:
            # Try to get template from database if room_id provided
            template = None
            if room_id:
                template = await self._fetch_template_for_room(room_id)
            
            if template:
                prompt = template.get('prompt_template', self.DEFAULT_PROMPT)
                # Ensure template_variables is always a dict, never None
                variables = template.get('template_variables') or {}
                if not isinstance(variables, dict):
                    logger.warning(f"template_variables is not a dict: {type(variables)}, using empty dict")
                    variables = {}
                logger.info(f"ðŸ“‹ Using prompt template: {template.get('name', 'Unknown')}")
                logger.info(f"ðŸ“‹ Template text: {prompt[:100]}...")
                logger.info(f"ðŸ“‹ Template variables from DB: {variables}")
            else:
                # Use default prompt
                prompt = self.DEFAULT_PROMPT
                variables = {}
                logger.info("ðŸ“‹ Using default prompt template")
            
            # Prepare substitution variables
            # Support both naming conventions for backwards compatibility
            substitutions = {
                'source_lang': source_lang,
                'source_language': source_lang,  # Also provide source_language for templates
                'target_lang': target_lang,
                'target_language': target_lang,  # Also provide target_language for templates
            }
            
            # Merge template variables, ensuring we handle nested values
            if variables:
                substitutions.update(variables)
                # Log what variables we're using
                logger.debug(f"Template variables: {variables}")
            
            # Add room-specific context if available
            if room_config:
                if room_config.get('mosque_name'):
                    substitutions['mosque_name'] = room_config['mosque_name']
                if room_config.get('speaker_role'):
                    substitutions['speaker_role'] = room_config['speaker_role']
            
            # Format the prompt with variables
            try:
                formatted_prompt = prompt.format(**substitutions)
            except KeyError as e:
                logger.warning(f"Missing variable in template: {e}. Using partial formatting.")
                # Use safe substitution that ignores missing keys
                from string import Template
                safe_template = Template(prompt.replace('{', '${'))
                formatted_prompt = safe_template.safe_substitute(**substitutions)
                # Replace any remaining ${var} with empty string
                import re
                formatted_prompt = re.sub(r'\$\{[^}]+\}', '', formatted_prompt)
            
            # Add critical translation instruction if missing
            if template and not any(word in formatted_prompt.lower() for word in ["translate", "translating", "translation", "translator"]):
                logger.warning("Template missing explicit translation instruction, adding prefix")
                formatted_prompt = f"IMPORTANT: You must ONLY translate the text, do not respond or react to it. {formatted_prompt}"
            
            # Log successful template usage
            if template:
                logger.info(f"âœ… Successfully formatted custom prompt template: {template.get('name', 'Unknown')}")
            
            # Log the generated prompt for debugging
            logger.info(f"Generated prompt: {formatted_prompt}")
            
            return formatted_prompt
            
        except Exception as e:
            logger.error(f"âŒ Error building prompt: {e}")
            # Fallback to basic default
            return self.DEFAULT_PROMPT.format(
                source_lang=source_lang,
                target_lang=target_lang
            )
    
    async def _fetch_template_for_room(self, room_id: int) -> Optional[Dict[str, Any]]:
        """
        Fetch prompt template for a specific room.
        
        Args:
            room_id: The room ID
            
        Returns:
            Template dictionary or None
        """
        try:
            # Always fetch fresh from database - no caching for production
            template = await query_prompt_template_for_room(room_id)
            
            if template:
                # Log what we received for debugging
                logger.info(f"ðŸ“‹ Fetched template for room {room_id}: {template.get('name', 'Unknown')}")
                logger.debug(f"Template data: {template}")
            else:
                logger.info(f"ðŸ“‹ No template found for room {room_id}")
                
            return template
            
        except Exception as e:
            logger.warning(f"Failed to fetch template for room {room_id}: {e}")
            return None
    
    def build_prompt_with_context(
        self,
        base_prompt: str,
        context_type: str,
        additional_context: Optional[Dict[str, str]] = None
    ) -> str:
        """
        Enhance a prompt with additional context based on content type.
        
        Args:
            base_prompt: The base prompt template
            context_type: Type of content (sermon, announcement, etc.)
            additional_context: Additional context variables
            
        Returns:
            Enhanced prompt with context
        """
        context_additions = {
            'sermon': (
                " Remember this is a religious sermon, so maintain appropriate "
                "reverence and formality. Preserve the spiritual tone."
            ),
            'announcement': (
                " This is a community announcement, so prioritize clarity and "
                "practical information over stylistic concerns."
            ),
            'dua': (
                " This is a prayer or supplication. Maintain the devotional "
                "atmosphere and emotional depth of the original."
            ),
            'lecture': (
                " This is an educational lecture. You may add brief clarifications "
                "in parentheses for complex religious terms if needed."
            )
        }
        
        # Add context-specific guidance
        addition = context_additions.get(context_type, "")
        
        # Add any additional context
        if additional_context:
            for key, value in additional_context.items():
                if value:
                    addition += f" {key}: {value}."
        
        return base_prompt + addition
    
    def get_preserved_terms_for_template(self, template_variables: Dict) -> list:
        """
        Extract list of terms to preserve from template variables.
        
        Args:
            template_variables: Template variables dictionary
            
        Returns:
            List of terms to preserve in original language
        """
        return template_variables.get('preserve_terms', [])


# Global instance
_prompt_builder: Optional[PromptBuilder] = None


def get_prompt_builder() -> PromptBuilder:
    """Get or create the global prompt builder instance."""
    global _prompt_builder
    if _prompt_builder is None:
        _prompt_builder = PromptBuilder()
    return _prompt_builder


================================================
FILE: render.yaml
================================================
services:
  - type: worker
    name: bayaan-livekit-agent
    env: docker
    dockerfilePath: ./Dockerfile
    plan: starter
    region: oregon
    scaling:
      minInstances: 1
      maxInstances: 3
      targetMemoryPercent: 80
      targetCPUPercent: 80
    envVars:
      - key: ENVIRONMENT
        value: production
      - key: LIVEKIT_URL
        fromSecret: livekit_url
      - key: LIVEKIT_API_KEY
        fromSecret: livekit_api_key
      - key: LIVEKIT_API_SECRET
        fromSecret: livekit_api_secret
      - key: OPENAI_API_KEY
        fromSecret: openai_api_key
      - key: SPEECHMATICS_API_KEY
        fromSecret: speechmatics_api_key
      - key: SUPABASE_URL
        fromSecret: supabase_url
      - key: SUPABASE_SERVICE_ROLE_KEY
        fromSecret: supabase_service_role_key
      - key: SUPABASE_ANON_KEY
        fromSecret: supabase_anon_key
      - key: PYTHONPATH
        value: /app
      - key: AGENT_NAME
        value: bayaan-transcriber
      - key: WORKER_TYPE
        value: background
      - key: LOG_LEVEL
        value: INFO
      - key: PERSISTENT_MODE
        value: "true"
      - key: MAX_WORKERS
        value: "3"
      - key: IDLE_TIMEOUT
        value: "300" 


================================================
FILE: requirements.txt
================================================
# LiveKit Core Dependencies - EXACT VERSIONS FROM JULY 28 DEPLOYMENT
livekit==1.0.12
livekit-agents==1.2.1
livekit-plugins-openai==1.2.1
livekit-plugins-speechmatics==1.2.1
livekit-plugins-silero==1.2.1

# AI/ML Dependencies
openai>=1.0.0

# Web Framework (for health checks)
fastapi>=0.104.0
uvicorn[standard]>=0.24.0

# HTTP Client
aiohttp>=3.8.0

# Database
asyncpg>=0.29.0

# Environment & Configuration
python-dotenv>=1.0.0

# Logging & Monitoring
# structlog>=23.0.0  # Removed - using standard logging

# Production Dependencies
gunicorn>=21.0.0
psutil>=5.9.0

# Audio Processing
pyaudio>=0.2.11

# Async utilities
asyncio-throttle>=1.0.0


================================================
FILE: resource_management.py
================================================
"""
Resource management module for LiveKit AI Translation Server.
Handles tracking and cleanup of async tasks, STT streams, and other resources.
"""
import asyncio
import logging
from typing import Set, List, Dict, Any, Optional, Callable
from contextlib import asynccontextmanager
from dataclasses import dataclass, field
from datetime import datetime, timedelta
import time
import weakref

logger = logging.getLogger("transcriber.resources")


@dataclass
class ResourceStats:
    """Statistics about managed resources."""
    tasks_created: int = 0
    tasks_completed: int = 0
    tasks_failed: int = 0
    tasks_cancelled: int = 0
    streams_opened: int = 0
    streams_closed: int = 0
    active_tasks: int = 0
    active_streams: int = 0


class TaskManager:
    """
    Manages async tasks with proper tracking and cleanup.
    
    Features:
    - Automatic task tracking
    - Graceful cancellation
    - Resource leak prevention
    - Statistics tracking
    """
    
    def __init__(self, name: str = "default"):
        self.name = name
        self._tasks: Set[asyncio.Task] = set()
        self._task_metadata: Dict[asyncio.Task, Dict[str, Any]] = {}
        self._stats = ResourceStats()
        self._cleanup_interval = 30.0  # seconds
        self._cleanup_task: Optional[asyncio.Task] = None
        self._shutdown = False
        logger.info(f"ðŸ“‹ TaskManager '{self.name}' initialized")
    
    async def __aenter__(self):
        """Context manager entry."""
        self._cleanup_task = asyncio.create_task(self._periodic_cleanup())
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit with cleanup."""
        await self.shutdown()
    
    def create_task(
        self, 
        coro, 
        name: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None
    ) -> asyncio.Task:
        """
        Create and track an async task.
        
        Args:
            coro: Coroutine to run
            name: Optional task name
            metadata: Optional metadata for the task
            
        Returns:
            Created task
        """
        if self._shutdown:
            raise RuntimeError("TaskManager is shutting down")
        
        task = asyncio.create_task(coro, name=name)
        self._tasks.add(task)
        
        if metadata:
            self._task_metadata[task] = metadata
        
        # Add callback to clean up when done
        task.add_done_callback(self._task_done_callback)
        
        self._stats.tasks_created += 1
        self._stats.active_tasks = len(self._tasks)
        
        logger.debug(f"ðŸ“Œ Created task: {name or task.get_name()} (total: {len(self._tasks)})")
        return task
    
    def _task_done_callback(self, task: asyncio.Task):
        """Callback when a task completes."""
        self._tasks.discard(task)
        self._task_metadata.pop(task, None)
        
        try:
            if task.cancelled():
                self._stats.tasks_cancelled += 1
                logger.debug(f"ðŸš« Task cancelled: {task.get_name()}")
            elif task.exception():
                self._stats.tasks_failed += 1
                logger.error(f"âŒ Task failed: {task.get_name()}", exc_info=task.exception())
            else:
                self._stats.tasks_completed += 1
                logger.debug(f"âœ… Task completed: {task.get_name()}")
        except Exception as e:
            logger.debug(f"Error in task callback: {e}")
        
        self._stats.active_tasks = len(self._tasks)
    
    async def _periodic_cleanup(self):
        """Periodically clean up completed tasks."""
        while not self._shutdown:
            try:
                await asyncio.sleep(self._cleanup_interval)
                
                # Clean up any references to completed tasks
                completed = [t for t in self._tasks if t.done()]
                for task in completed:
                    self._tasks.discard(task)
                    self._task_metadata.pop(task, None)
                
                if completed:
                    logger.debug(f"ðŸ§¹ Cleaned up {len(completed)} completed tasks")
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"Error in periodic cleanup: {e}")
    
    async def cancel_all(self, timeout: float = 5.0) -> int:
        """
        Cancel all active tasks.
        
        Args:
            timeout: Maximum time to wait for cancellation
            
        Returns:
            Number of tasks cancelled
        """
        if not self._tasks:
            return 0
        
        tasks_to_cancel = list(self._tasks)
        cancelled_count = 0
        
        logger.info(f"ðŸš« Cancelling {len(tasks_to_cancel)} tasks...")
        
        # Cancel all tasks
        for task in tasks_to_cancel:
            if not task.done():
                task.cancel()
                cancelled_count += 1
        
        # Wait for cancellation with timeout
        if tasks_to_cancel:
            try:
                await asyncio.wait_for(
                    asyncio.gather(*tasks_to_cancel, return_exceptions=True),
                    timeout=timeout
                )
            except asyncio.TimeoutError:
                logger.warning(f"â° Timeout waiting for {len(tasks_to_cancel)} tasks to cancel")
        
        logger.info(f"âœ… Cancelled {cancelled_count} tasks")
        return cancelled_count
    
    async def shutdown(self):
        """Shutdown the task manager and cleanup all resources."""
        if self._shutdown:
            return
        
        self._shutdown = True
        logger.info(f"ðŸ›‘ Shutting down TaskManager '{self.name}'...")
        
        # Cancel cleanup task
        if self._cleanup_task and not self._cleanup_task.done():
            self._cleanup_task.cancel()
            try:
                await self._cleanup_task
            except asyncio.CancelledError:
                pass
        
        # Cancel all managed tasks
        await self.cancel_all()
        
        logger.info(f"âœ… TaskManager '{self.name}' shutdown complete")
    
    def get_stats(self) -> ResourceStats:
        """Get current statistics."""
        return self._stats
    
    def get_active_tasks(self) -> List[asyncio.Task]:
        """Get list of active tasks."""
        return list(self._tasks)


class STTStreamManager:
    """
    Manages STT (Speech-to-Text) streams with proper cleanup.
    """
    
    def __init__(self):
        self._streams: Set[Any] = set()
        self._stream_metadata: Dict[Any, Dict[str, Any]] = {}
        self._participant_streams: Dict[str, Any] = {}  # Track active streams per participant
        self._cleanup_locks: Dict[str, asyncio.Lock] = {}  # Prevent race conditions
        self._participant_disconnect_times: Dict[str, float] = {}  # Track disconnect times
        self._reconnect_grace_period = 3.0  # seconds to wait before allowing reconnect
        self._stats = ResourceStats()
        logger.info("ðŸŽ¤ STTStreamManager initialized")
    
    @asynccontextmanager
    async def create_stream(self, stt_provider, participant_id: str):
        """
        Create and manage an STT stream.
        
        Args:
            stt_provider: STT provider instance
            participant_id: ID of the participant
            
        Yields:
            STT stream instance
        """
        # Get or create lock for this participant
        if participant_id not in self._cleanup_locks:
            self._cleanup_locks[participant_id] = asyncio.Lock()
        
        async with self._cleanup_locks[participant_id]:
            # Check for recent disconnect - implement debouncing
            last_disconnect = self._participant_disconnect_times.get(participant_id, 0)
            time_since_disconnect = time.time() - last_disconnect
            
            if time_since_disconnect < self._reconnect_grace_period:
                wait_time = self._reconnect_grace_period - time_since_disconnect
                logger.warning(f"â³ Participant {participant_id} reconnecting too quickly. Waiting {wait_time:.1f}s")
                await asyncio.sleep(wait_time)
            
            # Check if there's already an active stream for this participant
            existing_stream = self._participant_streams.get(participant_id)
            if existing_stream:
                logger.warning(f"âš ï¸ Active STT stream already exists for {participant_id}, closing it first")
                try:
                    await existing_stream.aclose()
                except Exception as e:
                    logger.error(f"Error closing existing stream: {e}")
                finally:
                    self._streams.discard(existing_stream)
                    self._stream_metadata.pop(existing_stream, None)
                    self._participant_streams.pop(participant_id, None)
            
            stream = None
            try:
                # Create stream
                stream = stt_provider.stream()
                self._streams.add(stream)
                self._stream_metadata[stream] = {
                    "participant_id": participant_id,
                    "created_at": datetime.utcnow()
                }
                self._participant_streams[participant_id] = stream
                self._stats.streams_opened += 1
                self._stats.active_streams = len(self._streams)
                
                logger.info(f"ðŸŽ¤ Created STT stream for {participant_id}")
                yield stream
            
            finally:
                # Cleanup stream
                if stream:
                    try:
                        # Force close the stream
                        await stream.aclose()
                        logger.info(f"âœ… STT stream closed for {participant_id}")
                    except Exception as e:
                        logger.error(f"Error closing STT stream: {e}")
                    finally:
                        self._streams.discard(stream)
                        self._stream_metadata.pop(stream, None)
                        self._participant_streams.pop(participant_id, None)
                        # Record disconnect time for debouncing
                        self._participant_disconnect_times[participant_id] = time.time()
                        self._stats.streams_closed += 1
                        self._stats.active_streams = len(self._streams)
    
    async def close_all(self):
        """Close all active streams."""
        if not self._streams:
            return
        
        logger.info(f"ðŸš« Closing {len(self._streams)} STT streams...")
        
        streams_to_close = list(self._streams)
        for stream in streams_to_close:
            try:
                await stream.aclose()
            except Exception as e:
                logger.error(f"Error closing stream: {e}")
            finally:
                self._streams.discard(stream)
                self._stream_metadata.pop(stream, None)
        
        self._participant_streams.clear()
        self._cleanup_locks.clear()
        self._participant_disconnect_times.clear()
        self._stats.active_streams = 0
        logger.info("âœ… All STT streams closed")
    
    async def close_participant_stream(self, participant_id: str):
        """Close a specific participant's stream."""
        if participant_id not in self._participant_streams:
            return
        
        if participant_id not in self._cleanup_locks:
            self._cleanup_locks[participant_id] = asyncio.Lock()
        
        async with self._cleanup_locks[participant_id]:
            stream = self._participant_streams.get(participant_id)
            if stream:
                logger.info(f"ðŸš« Closing STT stream for participant {participant_id}")
                try:
                    await stream.aclose()
                except Exception as e:
                    logger.error(f"Error closing participant stream: {e}")
                finally:
                    self._streams.discard(stream)
                    self._stream_metadata.pop(stream, None)
                    self._participant_streams.pop(participant_id, None)
                    # Record disconnect time for debouncing
                    self._participant_disconnect_times[participant_id] = time.time()
                    self._stats.streams_closed += 1
                    self._stats.active_streams = len(self._streams)
    
    def get_stats(self) -> ResourceStats:
        """Get current statistics."""
        return self._stats


class HeartbeatMonitor:
    """
    Monitors participant activity and detects stuck sessions.
    """
    
    def __init__(self, timeout: float = 30.0):
        self.timeout = timeout
        self.participants: Dict[str, datetime] = {}
        self.session_info: Dict[str, Dict[str, Any]] = {}
        self._monitor_task: Optional[asyncio.Task] = None
        self._callbacks: List[Callable[[str], Any]] = []
        logger.info(f"ðŸ’“ HeartbeatMonitor initialized with {timeout}s timeout")
    
    def register_callback(self, callback: Callable[[str], Any]):
        """Register a callback to be called when a participant times out."""
        self._callbacks.append(callback)
    
    async def update_heartbeat(self, participant_id: str, session_id: Optional[str] = None):
        """Update the heartbeat timestamp for a participant."""
        self.participants[participant_id] = datetime.utcnow()
        if session_id:
            self.session_info[participant_id] = {
                "session_id": session_id,
                "last_seen": datetime.utcnow()
            }
        logger.debug(f"ðŸ’“ Heartbeat updated for {participant_id}")
    
    async def check_timeouts(self) -> List[str]:
        """Check for timed-out participants."""
        now = datetime.utcnow()
        timed_out = []
        
        for participant_id, last_seen in list(self.participants.items()):
            elapsed = (now - last_seen).total_seconds()
            if elapsed > self.timeout:
                timed_out.append(participant_id)
                logger.warning(f"â° Participant {participant_id} timed out (last seen {elapsed:.1f}s ago)")
                
                # Remove from tracking
                self.participants.pop(participant_id, None)
                session_info = self.session_info.pop(participant_id, None)
                
                # Call registered callbacks
                for callback in self._callbacks:
                    try:
                        if asyncio.iscoroutinefunction(callback):
                            await callback(participant_id)
                        else:
                            callback(participant_id)
                    except Exception as e:
                        logger.error(f"Error in heartbeat callback: {e}")
        
        return timed_out
    
    async def start_monitoring(self):
        """Start the heartbeat monitoring loop."""
        if self._monitor_task and not self._monitor_task.done():
            return
        
        async def monitor_loop():
            while True:
                try:
                    await asyncio.sleep(10)  # Check every 10 seconds
                    timed_out = await self.check_timeouts()
                    if timed_out:
                        logger.info(f"ðŸ’” {len(timed_out)} participants timed out")
                except asyncio.CancelledError:
                    break
                except Exception as e:
                    logger.error(f"Error in heartbeat monitor: {e}")
        
        self._monitor_task = asyncio.create_task(monitor_loop())
        logger.info("ðŸ’“ Heartbeat monitoring started")
    
    async def stop_monitoring(self):
        """Stop the heartbeat monitoring loop."""
        if self._monitor_task and not self._monitor_task.done():
            self._monitor_task.cancel()
            try:
                await self._monitor_task
            except asyncio.CancelledError:
                pass
        logger.info("ðŸ’” Heartbeat monitoring stopped")
    
    def remove_participant(self, participant_id: str):
        """Remove a participant from monitoring."""
        self.participants.pop(participant_id, None)
        self.session_info.pop(participant_id, None)
        logger.debug(f"ðŸ’” Participant {participant_id} removed from heartbeat monitoring")


class ResourceManager:
    """
    Central resource manager for the application.
    Coordinates TaskManager and STTStreamManager.
    """
    
    def __init__(self):
        self.task_manager = TaskManager("main")
        self.stt_manager = STTStreamManager()
        self.heartbeat_monitor = HeartbeatMonitor(timeout=45.0)  # 45 seconds timeout
        self._shutdown_handlers: List[Callable] = []
        logger.info("ðŸ—ï¸ ResourceManager initialized")
    
    async def __aenter__(self):
        """Context manager entry."""
        await self.task_manager.__aenter__()
        await self.heartbeat_monitor.start_monitoring()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit with cleanup."""
        await self.shutdown()
    
    def add_shutdown_handler(self, handler: Callable):
        """Add a handler to be called on shutdown."""
        self._shutdown_handlers.append(handler)
    
    async def shutdown(self):
        """Shutdown all managed resources."""
        logger.info("ðŸ›‘ Starting ResourceManager shutdown...")
        
        # Run shutdown handlers
        for handler in self._shutdown_handlers:
            try:
                if asyncio.iscoroutinefunction(handler):
                    await handler()
                else:
                    handler()
            except Exception as e:
                logger.error(f"Error in shutdown handler: {e}")
        
        # Shutdown managers
        await self.heartbeat_monitor.stop_monitoring()
        await self.task_manager.shutdown()
        await self.stt_manager.close_all()
        
        logger.info("âœ… ResourceManager shutdown complete")
    
    def get_all_stats(self) -> Dict[str, Any]:
        """Get statistics from all managers."""
        return {
            "tasks": self.task_manager.get_stats(),
            "stt_streams": self.stt_manager.get_stats(),
            "heartbeat": {
                "active_participants": len(self.heartbeat_monitor.participants),
                "timeout": self.heartbeat_monitor.timeout
            }
        }
    
    def log_stats(self):
        """Log current resource statistics."""
        stats = self.get_all_stats()
        logger.info(
            f"ðŸ“Š Resource Stats - "
            f"Tasks: {stats['tasks'].active_tasks} active "
            f"({stats['tasks'].tasks_completed} completed, "
            f"{stats['tasks'].tasks_failed} failed, "
            f"{stats['tasks'].tasks_cancelled} cancelled), "
            f"STT Streams: {stats['stt_streams'].active_streams} active, "
            f"Heartbeat: {stats['heartbeat']['active_participants']} participants"
        )
    
    async def verify_cleanup_complete(self) -> Dict[str, Any]:
        """Verify all resources are properly cleaned up."""
        active_tasks = self.task_manager.get_active_tasks()
        active_streams = len(self.stt_manager._streams)
        active_participants = len(self.heartbeat_monitor.participants)
        
        # Check if connection pool is closed
        db_closed = True
        try:
            from database import _pool
            if hasattr(_pool, '_local') and hasattr(_pool._local, 'session'):
                db_closed = _pool._local.session is None or _pool._local.session.closed
        except:
            pass
        
        cleanup_complete = (
            len(active_tasks) == 0 and 
            active_streams == 0 and 
            active_participants == 0 and
            db_closed
        )
        
        result = {
            "cleanup_complete": cleanup_complete,
            "tasks_remaining": len(active_tasks),
            "active_task_names": [t.get_name() for t in active_tasks],
            "streams_remaining": active_streams,
            "participants_remaining": active_participants,
            "participant_ids": list(self.heartbeat_monitor.participants.keys()),
            "db_connections_closed": db_closed,
            "timestamp": datetime.utcnow().isoformat()
        }
        
        if not cleanup_complete:
            logger.warning(f"âš ï¸ Cleanup verification failed: {result}")
        else:
            logger.info("âœ… Cleanup verification passed - all resources cleaned up")
        
        return result


================================================
FILE: speechmatics_advanced.py
================================================
"""
Advanced Speechmatics configuration and features for improved accuracy.
This module contains implementations for custom vocabulary, domain models,
audio enhancement, confidence scoring, and alternative transcriptions.

NOTE: Domain models are not yet supported in the current LiveKit Speechmatics plugin.
This module shows how they would be implemented when support is added.
The UI and database already support storing domain preferences for future use.
"""
import logging
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from livekit.plugins.speechmatics.types import TranscriptionConfig

logger = logging.getLogger("transcriber.speechmatics_advanced")


@dataclass
class AdvancedSpeechmaticsConfig:
    """Extended Speechmatics configuration with all accuracy features."""
    
    # Basic settings (inherited from main config)
    language: str = "ar"
    operating_point: str = "enhanced"
    enable_partials: bool = False
    max_delay: float = 3.5
    punctuation_sensitivity: float = 0.5
    diarization: str = "speaker"
    
    # Custom vocabulary settings
    enable_custom_vocab: bool = True
    custom_vocab_items: List[Dict[str, Any]] = None
    custom_dictionary_id: Optional[str] = None
    
    # Domain-specific settings
    domain: str = "broadcast"  # Best for sermons/lectures
    output_locale: str = "ar-SA"  # Saudi Arabic for religious content
    
    # Audio enhancement settings
    enable_automatic_audio_enhancement: bool = True
    sample_rate: int = 16000  # Optimal for speech
    
    # Advanced features
    max_alternatives: int = 3  # Get top 3 alternatives
    enable_word_level_confidence: bool = True
    enable_entities: bool = True  # Detect names, places
    
    def __post_init__(self):
        if self.custom_vocab_items is None:
            self.custom_vocab_items = self._get_default_islamic_vocabulary()
    
    def _get_default_islamic_vocabulary(self) -> List[Dict[str, Any]]:
        """Get default Islamic/Arabic religious vocabulary."""
        return [
            # Common Islamic phrases
            {"content": "Ø¨Ø³Ù… Ø§Ù„Ù„Ù‡ Ø§Ù„Ø±Ø­Ù…Ù† Ø§Ù„Ø±Ø­ÙŠÙ…", "sounds_like": ["bismillahir rahmanir raheem"]},
            {"content": "Ø§Ù„Ø­Ù…Ø¯ Ù„Ù„Ù‡ Ø±Ø¨ Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠÙ†", "sounds_like": ["alhamdulillahi rabbil alameen"]},
            {"content": "ØµÙ„Ù‰ Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙ‡ ÙˆØ³Ù„Ù…", "sounds_like": ["sallallahu alayhi wasallam"]},
            {"content": "Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ… ÙˆØ±Ø­Ù…Ø© Ø§Ù„Ù„Ù‡ ÙˆØ¨Ø±ÙƒØ§ØªÙ‡", "sounds_like": ["assalamu alaikum wa rahmatullahi wa barakatuh"]},
            
            # Prayer-related terms
            {"content": "ØµÙ„Ø§Ø© Ø§Ù„ÙØ¬Ø±", "sounds_like": ["salatul fajr", "fajr prayer"]},
            {"content": "ØµÙ„Ø§Ø© Ø§Ù„Ø¸Ù‡Ø±", "sounds_like": ["salatul dhuhr", "dhuhr prayer"]},
            {"content": "ØµÙ„Ø§Ø© Ø§Ù„Ø¹ØµØ±", "sounds_like": ["salatul asr", "asr prayer"]},
            {"content": "ØµÙ„Ø§Ø© Ø§Ù„Ù…ØºØ±Ø¨", "sounds_like": ["salatul maghrib", "maghrib prayer"]},
            {"content": "ØµÙ„Ø§Ø© Ø§Ù„Ø¹Ø´Ø§Ø¡", "sounds_like": ["salatul isha", "isha prayer"]},
            {"content": "ØµÙ„Ø§Ø© Ø§Ù„ØªØ±Ø§ÙˆÙŠØ­", "sounds_like": ["salatul tarawih", "tarawih prayer"]},
            {"content": "ØµÙ„Ø§Ø© Ø§Ù„Ø¬Ù…Ø¹Ø©", "sounds_like": ["salatul jumuah", "friday prayer"]},
            
            # Quranic terms
            {"content": "Ø§Ù„Ù‚Ø±Ø¢Ù† Ø§Ù„ÙƒØ±ÙŠÙ…", "sounds_like": ["al quran al kareem", "the holy quran"]},
            {"content": "Ø³ÙˆØ±Ø© Ø§Ù„ÙØ§ØªØ­Ø©", "sounds_like": ["surat al fatihah"]},
            {"content": "Ø¢ÙŠØ© Ø§Ù„ÙƒØ±Ø³ÙŠ", "sounds_like": ["ayatul kursi"]},
            
            # Titles and honorifics
            {"content": "Ø§Ù„Ø´ÙŠØ®", "sounds_like": ["sheikh", "shaykh"]},
            {"content": "Ø§Ù„Ø¥Ù…Ø§Ù…", "sounds_like": ["imam", "al imam"]},
            {"content": "Ø§Ù„Ø­Ø§Ø¬", "sounds_like": ["hajj", "al hajj"]},
            {"content": "Ø§Ù„Ø£Ø³ØªØ§Ø°", "sounds_like": ["ustadh", "al ustadh"]},
        ]
    
    def build_transcription_config(self, room_config: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Build complete TranscriptionConfig with all advanced features."""
        config = {
            "language": self.language,
            "operating_point": self.operating_point,
            "enable_partials": self.enable_partials,
            "max_delay": self.max_delay,
            "punctuation_overrides": {"sensitivity": self.punctuation_sensitivity},
            "diarization": self.diarization,
        }
        
        # Add custom vocabulary if enabled
        if self.enable_custom_vocab:
            if self.custom_dictionary_id:
                config["custom_dictionary"] = {
                    "language": self.language,
                    "dictionary_id": self.custom_dictionary_id,
                }
            elif self.custom_vocab_items:
                config["additional_vocab"] = self.custom_vocab_items[:1000]  # Max 1000 items
        
        # Add domain configuration
        if self.domain:
            config["domain"] = self.domain
        
        # Add output locale for regional variations
        if self.output_locale:
            config["output_locale"] = self.output_locale
        
        # Audio enhancement
        if self.enable_automatic_audio_enhancement:
            config["enable_automatic_audio_enhancement"] = True
            
        # Alternative transcriptions
        if self.max_alternatives > 1:
            config["max_alternatives"] = self.max_alternatives
            
        # Entity detection
        if self.enable_entities:
            config["enable_entities"] = True
            
        # Room-specific overrides
        if room_config:
            # Override domain based on content type
            content_type = room_config.get('content_type')
            if content_type:
                config["domain"] = self._get_domain_for_content_type(content_type)
            
            # Add room-specific vocabulary
            room_vocab = room_config.get('custom_vocabulary', [])
            if room_vocab and self.enable_custom_vocab:
                config["additional_vocab"].extend(room_vocab)
        
        return config
    
    def _get_domain_for_content_type(self, content_type: str) -> str:
        """Map content type to Speechmatics domain."""
        domain_mapping = {
            'sermon': 'broadcast',
            'lecture': 'broadcast',
            'announcement': 'contact_centre',
            'discussion': 'conversational',
            'recitation': 'broadcast',
            'interview': 'conversational',
        }
        return domain_mapping.get(content_type, 'general')


class TranscriptionProcessor:
    """Process transcriptions with confidence scoring and alternatives."""
    
    def __init__(self, confidence_threshold: float = 0.7):
        self.confidence_threshold = confidence_threshold
        self.low_confidence_words = []
        self.alternative_selections = []
    
    async def process_transcription_event(self, ev) -> Dict[str, Any]:
        """Process a transcription event with confidence analysis."""
        if not hasattr(ev, 'alternatives') or not ev.alternatives:
            return None
            
        # Get primary transcription
        primary = ev.alternatives[0]
        result = {
            "text": primary.text,
            "confidence": getattr(primary, 'confidence', 1.0),
            "is_final": ev.type == "FINAL_TRANSCRIPT",
            "alternatives": [],
            "low_confidence_words": [],
            "requires_review": False
        }
        
        # Check overall confidence
        if result["confidence"] < self.confidence_threshold:
            result["requires_review"] = True
            logger.warning(f"Low confidence transcript: {result['confidence']:.2f}")
        
        # Process alternatives
        for i, alt in enumerate(ev.alternatives[1:], 1):
            result["alternatives"].append({
                "rank": i + 1,
                "text": alt.text,
                "confidence": getattr(alt, 'confidence', 0.0),
                "diff_from_primary": self._calculate_diff(primary.text, alt.text)
            })
        
        # Process word-level confidence if available
        if hasattr(primary, 'words'):
            for word in primary.words:
                if hasattr(word, 'confidence') and word.confidence < 0.6:
                    result["low_confidence_words"].append({
                        "text": word.text,
                        "confidence": word.confidence,
                        "position": getattr(word, 'start_time', 0)
                    })
        
        # Log significant alternatives
        if result["alternatives"] and result["alternatives"][0]["confidence"] > result["confidence"] - 0.1:
            logger.info(f"Close alternatives detected: Primary vs Alt1 confidence diff < 0.1")
        
        return result
    
    def _calculate_diff(self, text1: str, text2: str) -> float:
        """Calculate difference ratio between two texts."""
        # Simple character-based difference
        if not text1 or not text2:
            return 1.0
        
        common = sum(1 for a, b in zip(text1, text2) if a == b)
        return 1.0 - (common / max(len(text1), len(text2)))
    
    def get_statistics(self) -> Dict[str, Any]:
        """Get transcription quality statistics."""
        return {
            "total_low_confidence_words": len(self.low_confidence_words),
            "alternative_selection_count": len(self.alternative_selections),
            "average_confidence": sum(w["confidence"] for w in self.low_confidence_words) / max(1, len(self.low_confidence_words))
        }


class AudioEnhancer:
    """Audio enhancement for better transcription accuracy."""
    
    @staticmethod
    def get_audio_format_config() -> Dict[str, Any]:
        """Get optimal audio format configuration."""
        return {
            "type": "raw",
            "encoding": "pcm_s16le",  # 16-bit PCM
            "sample_rate": 16000,      # 16kHz optimal for speech
            "channels": 1,             # Mono for single speaker
        }
    
    @staticmethod
    def get_audio_events_config(content_type: str = "sermon") -> Dict[str, Any]:
        """Get audio event detection configuration based on content type."""
        configs = {
            "sermon": {
                "enable_music": False,
                "enable_applause": True,
                "enable_laughter": False,
                "enable_speech": True,
            },
            "lecture": {
                "enable_music": False,
                "enable_applause": True,
                "enable_laughter": True,
                "enable_speech": True,
            },
            "announcement": {
                "enable_music": False,
                "enable_applause": False,
                "enable_laughter": False,
                "enable_speech": True,
            }
        }
        return configs.get(content_type, configs["sermon"])


# Usage example in main.py:
"""
# Import advanced config
from speechmatics_advanced import AdvancedSpeechmaticsConfig, TranscriptionProcessor

# Create advanced config
advanced_config = AdvancedSpeechmaticsConfig(
    language=stt_config.language,
    max_delay=stt_config.max_delay,
    punctuation_sensitivity=stt_config.punctuation_sensitivity,
)

# Build transcription config with all features
transcription_config_dict = advanced_config.build_transcription_config(room_config)

# Initialize STT with advanced config
stt_provider = speechmatics.STT(
    transcription_config=TranscriptionConfig(**transcription_config_dict)
)

# Create processor for handling confidence scores
processor = TranscriptionProcessor(confidence_threshold=0.75)

# In transcription loop:
result = await processor.process_transcription_event(ev)
if result and result["requires_review"]:
    # Flag for human review or use alternative
    pass
"""


================================================
FILE: speechmatics_domain_patch.py
================================================
"""
Monkey patch to add domain support to LiveKit's Speechmatics plugin
This allows us to pass domain configuration to Speechmatics API
without waiting for LiveKit to add official support.
"""
import logging
from typing import Any, Dict

logger = logging.getLogger("transcriber.domain_patch")

def patch_speechmatics_for_domain_support():
    """
    Patches the LiveKit Speechmatics plugin to support domain parameter.
    Call this before importing speechmatics STT.
    """
    try:
        from livekit.plugins.speechmatics.types import TranscriptionConfig
        
        # Store original asdict method
        original_asdict = TranscriptionConfig.asdict
        
        # Create new asdict that includes domain if present
        def patched_asdict(self) -> Dict[str, Any]:
            # Get original dict
            result = original_asdict(self)
            
            # Add domain if it exists as an attribute
            if hasattr(self, '_domain') and self._domain:
                result['domain'] = self._domain
                logger.info(f"[OK] Domain '{self._domain}' added to transcription config")
            
            return result
        
        # Store original __init__
        original_init = TranscriptionConfig.__init__
        
        # Create new __init__ that accepts domain
        def patched_init(self, **kwargs):
            # Extract domain if provided
            domain = kwargs.pop('domain', None)
            
            # Call original init with remaining kwargs
            original_init(self, **kwargs)
            
            # Store domain as private attribute
            if domain:
                self._domain = domain
                logger.info(f"[INFO] TranscriptionConfig initialized with domain: {domain}")
        
        # Apply patches
        TranscriptionConfig.__init__ = patched_init
        TranscriptionConfig.asdict = patched_asdict
        
        logger.info("[OK] Speechmatics domain support patch applied successfully")
        return True
        
    except Exception as e:
        logger.error(f"[ERROR] Failed to apply domain support patch: {e}")
        return False

def test_domain_patch():
    """Test if the patch works correctly"""
    try:
        from livekit.plugins.speechmatics.types import TranscriptionConfig
        
        # Test creating config with domain
        config = TranscriptionConfig(
            language="ar",
            operating_point="enhanced",
            domain="broadcast"  # This should work now
        )
        
        # Test asdict includes domain
        config_dict = config.asdict()
        
        if 'domain' in config_dict and config_dict['domain'] == 'broadcast':
            logger.info("[OK] Domain patch test passed!")
            return True
        else:
            logger.error("[ERROR] Domain patch test failed - domain not in dict")
            return False
            
    except Exception as e:
        logger.error(f"[ERROR] Domain patch test failed with error: {e}")
        return False


================================================
FILE: sync_dev_to_production.sh
================================================
#!/bin/bash

# Dev Server to Production Server Update Script
# This script syncs changes from the development server to production
# while preserving production-specific optimizations
# 
# Author: Bayaan DevOps Team
# Date: $(date +%Y-%m-%d)
# Last Updated: 2025-01-28

set -euo pipefail  # Exit on error, undefined variables, pipe failures

# Configuration
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
DEV_DIR="${SCRIPT_DIR}/../server"
PROD_DIR="${SCRIPT_DIR}"
BACKUP_DIR="${PROD_DIR}/backup_$(date +%Y%m%d_%H%M%S)"
LOG_FILE="${PROD_DIR}/sync_dev_production_$(date +%Y%m%d_%H%M%S).log"
REPORT_FILE="${PROD_DIR}/sync_report_$(date +%Y%m%d_%H%M%S).md"

# Color codes for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color

# Logging function
log() {
    echo -e "$1" | tee -a "$LOG_FILE"
}

# Error handling
error_exit() {
    log "${RED}ERROR: $1${NC}"
    log "${YELLOW}Rolling back changes...${NC}"
    rollback
    exit 1
}

# Initialize sync report
init_report() {
    cat > "$REPORT_FILE" << EOF
# Dev Server â†’ Production Server Sync Report
**Date:** $(date)
**Source:** $DEV_DIR
**Target:** $PROD_DIR

## Summary of Changes

EOF
}

# Add to report
report() {
    echo "$1" >> "$REPORT_FILE"
}

# Rollback function
rollback() {
    if [ -d "$BACKUP_DIR" ]; then
        log "${YELLOW}Restoring from backup: $BACKUP_DIR${NC}"
        
        # Restore all backed up files
        for file in "$BACKUP_DIR"/*; do
            if [ -f "$file" ]; then
                filename=$(basename "$file")
                cp -f "$file" "$PROD_DIR/$filename" 2>/dev/null || true
                log "Restored: $filename"
            fi
        done
        
        log "${GREEN}Rollback completed${NC}"
    else
        log "${RED}No backup found for rollback${NC}"
    fi
}

# Pre-flight checks
preflight_checks() {
    log "${CYAN}=== Starting Pre-flight Checks ===${NC}"
    
    # Check if dev server directory exists
    if [ ! -d "$DEV_DIR" ]; then
        error_exit "Dev server directory not found: $DEV_DIR"
    fi
    
    # Check if we're in the production directory
    if [ ! -d "$PROD_DIR" ]; then
        error_exit "Production directory not found: $PROD_DIR"
    fi
    
    # Verify Python is available
    if ! command -v python3 &> /dev/null; then
        error_exit "Python3 is required but not installed"
    fi
    
    log "${GREEN}Pre-flight checks passed${NC}"
}

# Create backup
create_backup() {
    log "${CYAN}=== Creating Backup ===${NC}"
    
    # Create backup directory
    mkdir -p "$BACKUP_DIR" || error_exit "Cannot create backup directory"
    
    # Backup all Python files and critical configs
    local files_to_backup=(
        "*.py"
        ".env"
        "requirements.txt"
        "Dockerfile"
        "render.yaml"
        ".gitignore"
        "*.sh"
    )
    
    for pattern in "${files_to_backup[@]}"; do
        for file in $pattern; do
            if [ -f "$file" ]; then
                cp -p "$file" "$BACKUP_DIR/" 2>/dev/null || true
                log "Backed up: $file"
            fi
        done
    done
    
    log "${GREEN}Backup created at: $BACKUP_DIR${NC}"
    report "### Backup Location"
    report "\`$BACKUP_DIR\`"
    report ""
}

# Update core files that have been modified
update_core_files() {
    log "${CYAN}=== Updating Core Files ===${NC}"
    report "### Updated Files"
    report ""
    
    # Files that need to be updated (modified in both)
    local core_files=(
        "broadcasting.py"
        "config.py"
        "database.py"
        "database_enhanced.py"
        "main.py"
        "prompt_builder.py"
        "resource_management.py"
        "text_processing.py"
        "translation_helpers.py"
        "translator.py"
        "webhook_handler.py"
    )
    
    for file in "${core_files[@]}"; do
        if [ -f "$DEV_DIR/$file" ]; then
            if [ -f "$PROD_DIR/$file" ]; then
                # Check if files are different
                if ! diff -q "$DEV_DIR/$file" "$PROD_DIR/$file" > /dev/null 2>&1; then
                    cp -f "$DEV_DIR/$file" "$PROD_DIR/$file"
                    log "${GREEN}Updated: $file${NC}"
                    report "- **Updated:** \`$file\`"
                else
                    log "${BLUE}No changes: $file${NC}"
                fi
            else
                # File doesn't exist in production, copy it
                cp -f "$DEV_DIR/$file" "$PROD_DIR/$file"
                log "${GREEN}Added: $file${NC}"
                report "- **Added:** \`$file\` (was missing in production)"
            fi
        fi
    done
}

# Add new files from dev server
add_new_files() {
    log "${CYAN}=== Adding New Files from Dev Server ===${NC}"
    report ""
    report "### New Files Added"
    report ""
    
    # New files to add (exist in dev but not in production)
    local new_files=(
        "speechmatics_advanced.py"
        "speechmatics_domain_patch.py"
    )
    
    # Test/utility files to optionally add
    local optional_files=(
        "check_stt_params.py"
        "database_cleanup_fix.py"
        "simple_domain_test.py"
        "test_domain_patch.py"
        "test_domain_support.py"
        "test_room_domain.py"
        "verify_domain_config.py"
    )
    
    # Add essential new files
    for file in "${new_files[@]}"; do
        if [ -f "$DEV_DIR/$file" ] && [ ! -f "$PROD_DIR/$file" ]; then
            cp -f "$DEV_DIR/$file" "$PROD_DIR/$file"
            log "${GREEN}Added new file: $file${NC}"
            report "- **Added:** \`$file\` (Speechmatics domain support)"
        fi
    done
    
    # Report optional files (but don't copy automatically)
    report ""
    report "### Optional Files (Not Copied)"
    report "These files exist in dev but are test/utility files:"
    report ""
    
    for file in "${optional_files[@]}"; do
        if [ -f "$DEV_DIR/$file" ] && [ ! -f "$PROD_DIR/$file" ]; then
            log "${YELLOW}Optional file available: $file${NC}"
            report "- \`$file\` - $(get_file_purpose "$file")"
        fi
    done
}

# Get file purpose for documentation
get_file_purpose() {
    local file=$1
    case $file in
        "check_stt_params.py") echo "STT parameter verification utility" ;;
        "database_cleanup_fix.py") echo "Database cleanup script" ;;
        "simple_domain_test.py") echo "Domain testing utility" ;;
        "test_domain_patch.py") echo "Domain patch testing" ;;
        "test_domain_support.py") echo "Domain support testing" ;;
        "test_room_domain.py") echo "Room domain configuration test" ;;
        "verify_domain_config.py") echo "Domain configuration verification" ;;
        *) echo "Utility/test file" ;;
    esac
}

# Handle production-specific files
handle_production_files() {
    log "${CYAN}=== Checking Production-Specific Files ===${NC}"
    report ""
    report "### Production-Specific Files"
    report ""
    
    # main_production.py exists only in production
    if [ -f "$PROD_DIR/main_production.py" ]; then
        log "${YELLOW}Note: main_production.py is production-specific and was not modified${NC}"
        report "- \`main_production.py\` - Production entry point (preserved)"
    fi
    
    # Check if render.yaml or other deployment configs need updates
    if [ -f "$PROD_DIR/render.yaml" ]; then
        report "- \`render.yaml\` - Deployment configuration (preserved)"
    fi
}

# Update requirements.txt if needed
check_requirements() {
    log "${CYAN}=== Checking Requirements ===${NC}"
    report ""
    report "### Dependencies"
    report ""
    
    if [ -f "$DEV_DIR/requirements.txt" ] && [ -f "$PROD_DIR/requirements.txt" ]; then
        # Create sorted unique lists
        sort "$DEV_DIR/requirements.txt" | grep -v "^#" | grep -v "^$" > /tmp/dev_reqs_sorted.txt
        sort "$PROD_DIR/requirements.txt" | grep -v "^#" | grep -v "^$" > /tmp/prod_reqs_sorted.txt
        
        # Find differences
        if ! diff -q /tmp/dev_reqs_sorted.txt /tmp/prod_reqs_sorted.txt > /dev/null 2>&1; then
            log "${YELLOW}Requirements differ between dev and production${NC}"
            report "**Note:** requirements.txt files differ. Manual review recommended."
            
            # Show new requirements in dev
            comm -23 /tmp/dev_reqs_sorted.txt /tmp/prod_reqs_sorted.txt > /tmp/new_reqs.txt
            if [ -s /tmp/new_reqs.txt ]; then
                report ""
                report "New dependencies in dev:"
                while IFS= read -r req; do
                    report "- \`$req\`"
                done < /tmp/new_reqs.txt
            fi
        else
            log "${GREEN}Requirements are in sync${NC}"
            report "Requirements files are identical."
        fi
        
        # Cleanup
        rm -f /tmp/dev_reqs_sorted.txt /tmp/prod_reqs_sorted.txt /tmp/new_reqs.txt
    fi
}

# Verify Python syntax
verify_syntax() {
    log "${CYAN}=== Verifying Python Syntax ===${NC}"
    
    local all_good=true
    
    for file in "$PROD_DIR"/*.py; do
        if [ -f "$file" ]; then
            filename=$(basename "$file")
            if python3 -m py_compile "$file" 2>/dev/null; then
                log "${GREEN}âœ“ Syntax OK: $filename${NC}"
            else
                log "${RED}âœ— Syntax error in: $filename${NC}"
                all_good=false
            fi
        fi
    done
    
    # Clean up __pycache__
    rm -rf "$PROD_DIR/__pycache__" 2>/dev/null || true
    
    if [ "$all_good" = false ]; then
        error_exit "Python syntax verification failed"
    fi
    
    log "${GREEN}All Python files passed syntax check${NC}"
}

# Generate final recommendations
generate_recommendations() {
    log "${CYAN}=== Generating Recommendations ===${NC}"
    
    report ""
    report "## Post-Sync Recommendations"
    report ""
    report "### Important Notes"
    report ""
    report "1. **STT Stream Fix**: The recent STT stream reconnection fix has been applied to both environments"
    report "2. **Domain Support**: New Speechmatics domain support files have been added"
    report "3. **Resource Management**: Enhanced resource cleanup and debouncing implemented"
    report ""
    report "### Testing Checklist"
    report ""
    report "- [ ] Test STT stream reconnection scenarios"
    report "- [ ] Verify duplicate transcription prevention"
    report "- [ ] Test participant disconnect/reconnect within 3 seconds"
    report "- [ ] Verify resource cleanup on disconnect"
    report "- [ ] Test Speechmatics domain configuration (when enabled)"
    report ""
    report "### Deployment Steps"
    report ""
    report "1. Review this report and the sync log"
    report "2. Run local tests if possible"
    report "3. Commit changes: \`git add . && git commit -m \"Sync dev changes: STT fixes and domain support\"\`"
    report "4. Deploy to Render: \`git push\`"
    report "5. Monitor logs after deployment"
    report ""
    report "### Rollback Instructions"
    report ""
    report "If issues occur, run: \`bash $0 --rollback\`"
    report ""
    report "**Backup Location:** \`$BACKUP_DIR\`"
    report "**Log File:** \`$LOG_FILE\`"
}

# Main execution
main() {
    log "${GREEN}=== Dev Server â†’ Production Server Sync Script ===${NC}"
    log "Started at: $(date)"
    log ""
    
    # Initialize report
    init_report
    
    # Execute sync steps
    preflight_checks
    create_backup
    update_core_files
    add_new_files
    handle_production_files
    check_requirements
    verify_syntax
    generate_recommendations
    
    # Final summary
    log ""
    log "${GREEN}=== Sync Completed Successfully ===${NC}"
    log ""
    log "${CYAN}Important Files:${NC}"
    log "  ðŸ“„ Sync Report: ${YELLOW}$REPORT_FILE${NC}"
    log "  ðŸ“‹ Log File: ${YELLOW}$LOG_FILE${NC}"
    log "  ðŸ’¾ Backup: ${YELLOW}$BACKUP_DIR${NC}"
    log ""
    log "Review the sync report for detailed changes and recommendations."
    
    # Show the report
    echo ""
    echo "Opening sync report..."
    cat "$REPORT_FILE"
}

# Handle rollback option
if [ "${1:-}" == "--rollback" ]; then
    # Find the most recent backup
    LATEST_BACKUP=$(ls -td ${PROD_DIR}/backup_* 2>/dev/null | head -1)
    if [ -n "$LATEST_BACKUP" ]; then
        BACKUP_DIR="$LATEST_BACKUP"
        LOG_FILE="${PROD_DIR}/rollback_$(date +%Y%m%d_%H%M%S).log"
        log "${YELLOW}Rolling back to: $BACKUP_DIR${NC}"
        rollback
        log "${GREEN}Rollback completed successfully${NC}"
        exit 0
    else
        echo "${RED}No backup found for rollback${NC}"
        exit 1
    fi
fi

# Run main function
main "$@"


================================================
FILE: sync_ghost_session_fix.sh
================================================
#!/bin/bash

# Ghost Session Fix Sync Script for Bayaan Production Server
# This script syncs the ghost session fixes from server to production
# Author: SuperClaude DevOps
# Date: $(date +%Y-%m-%d)

set -euo pipefail

# Configuration
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
SERVER_DIR="${SCRIPT_DIR}/../server"
PROD_DIR="${SCRIPT_DIR}"
BACKUP_DIR="${PROD_DIR}/backup_ghost_fix_$(date +%Y%m%d_%H%M%S)"
LOG_FILE="${PROD_DIR}/ghost_fix_sync_$(date +%Y%m%d_%H%M%S).log"

# Color codes
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
CYAN='\033[0;36m'
NC='\033[0m'

# Logging function
log() {
    echo -e "$1" | tee -a "$LOG_FILE"
}

# Create backup
create_backup() {
    log "${CYAN}=== Creating Backup ===${NC}"
    mkdir -p "$BACKUP_DIR"
    
    # Backup critical files
    for file in database.py main.py requirements.txt; do
        if [ -f "$PROD_DIR/$file" ]; then
            cp -p "$PROD_DIR/$file" "$BACKUP_DIR/"
            log "Backed up: $file"
        fi
    done
    
    log "${GREEN}Backup created at: $BACKUP_DIR${NC}"
}

# Main sync function
main() {
    log "${GREEN}=== Ghost Session Fix Sync Script ===${NC}"
    log "Started at: $(date)"
    log ""
    
    # Create backup first
    create_backup
    
    # Step 1: Check if database_enhanced.py exists in production
    if [ -f "$PROD_DIR/database_enhanced.py" ]; then
        log "${GREEN}âœ… database_enhanced.py already in production${NC}"
    else
        log "${RED}âŒ database_enhanced.py missing - please run the main fix script first${NC}"
        exit 1
    fi
    
    # Step 2: Create integration patch for database.py
    log "${CYAN}=== Creating Database Integration Patch ===${NC}"
    
    cat > "$PROD_DIR/database_integration.patch" << 'EOF'
# Add these imports at the top of database.py after existing imports:
from database_enhanced import (
    ensure_active_session_atomic as _ensure_active_session_atomic,
    SessionHealthMonitor,
    update_session_heartbeat_enhanced
)

# Initialize health monitor (add after _pool initialization)
_health_monitor = SessionHealthMonitor()

# Replace the existing ensure_active_session function with:
async def ensure_active_session(room_id: int, mosque_id: int) -> Optional[str]:
    """
    Enhanced version with atomic session creation and ghost prevention.
    """
    try:
        session = await _pool.get_session()
        async with get_db_headers() as headers:
            return await _ensure_active_session_atomic(
                room_id, mosque_id, session, headers
            )
    except Exception as e:
        logger.error(f"Failed to ensure active session: {e}")
        return None

# Add this new function for enhanced heartbeat:
async def update_session_heartbeat_with_monitor(session_id: str) -> bool:
    """
    Update heartbeat with health monitoring.
    """
    if not session_id:
        return False
        
    try:
        session = await _pool.get_session()
        async with get_db_headers() as headers:
            # Use health monitor
            healthy = await _health_monitor.monitor_heartbeat(
                session_id, session, headers
            )
            
            if not healthy:
                if _health_monitor.should_force_cleanup(session_id):
                    logger.error(f"Session {session_id} needs force cleanup")
                    return False
                else:
                    _health_monitor.increment_recovery_attempt(session_id)
                    
            return healthy
    except Exception as e:
        logger.error(f"Heartbeat monitor error: {e}")
        return False

# Export health monitor for use in main.py
def get_health_monitor():
    return _health_monitor
EOF
    
    log "${GREEN}âœ… Integration patch created${NC}"
    
    # Step 3: Create main.py integration instructions
    log "${CYAN}=== Creating Main.py Integration Instructions ===${NC}"
    
    cat > "$PROD_DIR/main_integration.md" << 'EOF'
# Main.py Integration for Ghost Session Fix

## Required Changes:

### 1. Import the health monitor (at top of file):
```python
from database import get_health_monitor, update_session_heartbeat_with_monitor
```

### 2. Initialize health monitor (in entrypoint function):
```python
# After initializing resource_manager
health_monitor = get_health_monitor()
```

### 3. Update the heartbeat periodic function:
Replace the existing `update_session_heartbeat_periodic` function with:

```python
async def update_session_heartbeat_periodic():
    """Periodically update session heartbeat with health monitoring."""
    while not stop_heartbeat:
        try:
            if tenant_context and tenant_context.get('session_id'):
                session_id = tenant_context['session_id']
                
                # Use enhanced heartbeat with monitoring
                success = await update_session_heartbeat_with_monitor(session_id)
                
                if not success:
                    logger.error(f"Session {session_id} health check failed - may need cleanup")
                    # The monitor will handle cleanup if needed
                    
            await asyncio.sleep(30)
        except Exception as e:
            logger.error(f"Heartbeat task error: {e}")
            await asyncio.sleep(30)
```

### 4. Add cleanup on connect (in entrypoint, after room join):
```python
# Clean any stale sessions for this room on connect
if tenant_context.get('room_id'):
    logger.info("Checking for stale sessions on connect...")
    # The atomic session creation will handle cleanup automatically
```

## Testing the Integration:

1. Check logs for "ðŸ§¹ Cleaned X stale sessions" messages
2. Monitor for "Session health check failed" warnings
3. Verify no ghost sessions in database after disconnects
EOF
    
    log "${GREEN}âœ… Integration instructions created${NC}"
    
    # Step 4: Check for SQL migration status
    log "${CYAN}=== Checking SQL Migration Status ===${NC}"
    
    log "${YELLOW}âš ï¸  IMPORTANT: Make sure you've run the SQL migration:${NC}"
    log "  ${CYAN}20250128_fix_ghost_sessions_comprehensive.sql${NC}"
    log ""
    log "The migration adds:"
    log "  - ensure_room_session_atomic() function"
    log "  - update_session_heartbeat_enhanced() function"
    log "  - cleanup_ghost_sessions() function"
    log "  - ghost_session_monitor view"
    log ""
    
    # Step 5: Create deployment checklist
    log "${CYAN}=== Creating Deployment Checklist ===${NC}"
    
    cat > "$PROD_DIR/GHOST_FIX_DEPLOYMENT.md" << 'EOF'
# Ghost Session Fix Deployment Checklist

## Pre-Deployment:
- [ ] Run SQL migration in Supabase
- [ ] Backup production database.py
- [ ] Review database_integration.patch
- [ ] Test in development environment

## Integration Steps:
1. [ ] Apply database.py integration patch
2. [ ] Update main.py with health monitoring
3. [ ] Verify Python syntax: `python3 -m py_compile *.py`
4. [ ] Run local tests if available

## Deployment:
1. [ ] Commit changes to git
2. [ ] Push to production branch
3. [ ] Monitor Render deployment logs
4. [ ] Check for startup errors

## Post-Deployment Verification:
1. [ ] Check application logs for:
   - "âœ… Active session ensured" messages
   - "ðŸ§¹ Cleaned X stale sessions" messages
   - "ðŸ’“ Heartbeat updated" messages

2. [ ] Monitor database for ghost sessions:
   ```sql
   SELECT * FROM ghost_session_monitor;
   ```

3. [ ] Test session creation and cleanup:
   - Connect to a room
   - Verify session created
   - Disconnect
   - Verify session cleaned up

## Rollback Plan:
If issues occur:
1. Restore from backup: `backup_ghost_fix_*/`
2. Redeploy previous version on Render
3. Investigate logs for root cause
EOF
    
    log "${GREEN}âœ… Deployment checklist created${NC}"
    
    # Step 6: Summary
    log ""
    log "${GREEN}=== Sync Complete ===${NC}"
    log ""
    log "${CYAN}Files Created:${NC}"
    log "  ðŸ“„ database_integration.patch - Integration code for database.py"
    log "  ðŸ“„ main_integration.md - Instructions for main.py updates"
    log "  ðŸ“„ GHOST_FIX_DEPLOYMENT.md - Deployment checklist"
    log ""
    log "${YELLOW}Next Steps:${NC}"
    log "  1. Review the integration files"
    log "  2. Apply patches to database.py and main.py"
    log "  3. Test the changes locally"
    log "  4. Follow GHOST_FIX_DEPLOYMENT.md for deployment"
    log ""
    log "${CYAN}Backup Location:${NC} $BACKUP_DIR"
    log "${CYAN}Log File:${NC} $LOG_FILE"
    log ""
    log "Completed at: $(date)"
}

# Run main function
main "$@"


================================================
FILE: sync_report_20250728_212515.md
================================================
# Dev Server â†’ Production Server Sync Report
**Date:** Mon Jul 28 21:25:15 CEST 2025
**Source:** /mnt/c/Users/hassa/OneDrive/Desktop/0.2 Bayan/Dev/bayan-platform-admin-login/Backend/LiveKit-ai-translation/bayaan-server-production/../server
**Target:** /mnt/c/Users/hassa/OneDrive/Desktop/0.2 Bayan/Dev/bayan-platform-admin-login/Backend/LiveKit-ai-translation/bayaan-server-production

## Summary of Changes

### Backup Location
`/mnt/c/Users/hassa/OneDrive/Desktop/0.2 Bayan/Dev/bayan-platform-admin-login/Backend/LiveKit-ai-translation/bayaan-server-production/backup_20250728_212515`

### Updated Files

- **Updated:** `config.py`
- **Updated:** `database.py`
- **Updated:** `main.py`

### New Files Added

- **Added:** `speechmatics_advanced.py` (Speechmatics domain support)
- **Added:** `speechmatics_domain_patch.py` (Speechmatics domain support)

### Optional Files (Not Copied)
These files exist in dev but are test/utility files:

- `check_stt_params.py` - STT parameter verification utility
- `database_cleanup_fix.py` - Database cleanup script
- `simple_domain_test.py` - Domain testing utility
- `test_domain_patch.py` - Domain patch testing
- `test_domain_support.py` - Domain support testing
- `test_room_domain.py` - Room domain configuration test
- `verify_domain_config.py` - Domain configuration verification

### Production-Specific Files

- `main_production.py` - Production entry point (preserved)
- `render.yaml` - Deployment configuration (preserved)

### Dependencies

**Note:** requirements.txt files differ. Manual review recommended.

New dependencies in dev:
- `aiohttp
`
- `asyncpg`
- `fastapi
`
- `livekit-agents>=1.2.2
`
- `livekit-plugins-openai>=1.2.2
`
- `livekit-plugins-silero>=1.2.2
`
- `livekit-plugins-speechmatics>=1.2.2
`
- `openai
`
- `uvicorn
`

## Post-Sync Recommendations

### Important Notes

1. **STT Stream Fix**: The recent STT stream reconnection fix has been applied to both environments
2. **Domain Support**: New Speechmatics domain support files have been added
3. **Resource Management**: Enhanced resource cleanup and debouncing implemented

### Testing Checklist

- [ ] Test STT stream reconnection scenarios
- [ ] Verify duplicate transcription prevention
- [ ] Test participant disconnect/reconnect within 3 seconds
- [ ] Verify resource cleanup on disconnect
- [ ] Test Speechmatics domain configuration (when enabled)

### Deployment Steps

1. Review this report and the sync log
2. Run local tests if possible
3. Commit changes: `git add . && git commit -m "Sync dev changes: STT fixes and domain support"`
4. Deploy to Render: `git push`
5. Monitor logs after deployment

### Rollback Instructions

If issues occur, run: `bash sync_dev_to_production.sh --rollback`

**Backup Location:** `/mnt/c/Users/hassa/OneDrive/Desktop/0.2 Bayan/Dev/bayan-platform-admin-login/Backend/LiveKit-ai-translation/bayaan-server-production/backup_20250728_212515`
**Log File:** `/mnt/c/Users/hassa/OneDrive/Desktop/0.2 Bayan/Dev/bayan-platform-admin-login/Backend/LiveKit-ai-translation/bayaan-server-production/sync_dev_production_20250728_212515.log`



================================================
FILE: text_processing.py
================================================
"""
Text processing utilities for LiveKit AI Translation Server.
Handles sentence extraction and text manipulation for Arabic and other languages.
"""
import re
import logging
from typing import Tuple, List

logger = logging.getLogger("transcriber.text_processing")


def extract_complete_sentences(text: str) -> Tuple[List[str], str]:
    """
    Extract complete sentences from text and return them along with remaining incomplete text.
    
    This function is designed to work with Arabic text and handles Arabic punctuation marks.
    It identifies complete sentences based on punctuation and returns both the complete
    sentences and any remaining incomplete text.
    
    Args:
        text: The input text to process
        
    Returns:
        A tuple containing:
        - List of complete sentences
        - Remaining incomplete text
    """
    if not text.strip():
        return [], ""
    
    # Arabic sentence ending punctuation marks
    sentence_endings = ['.', '!', '?', 'ØŸ']  # Including Arabic question mark
    
    complete_sentences = []
    remaining_text = ""
    
    logger.debug(f"ðŸ” Processing text for sentence extraction: '{text}'")
    
    # Check if this is standalone punctuation
    if text.strip() in sentence_endings:
        logger.debug(f"ðŸ“ Detected standalone punctuation: '{text.strip()}'")
        # This is standalone punctuation - signal to complete any accumulated sentence
        return ["PUNCTUATION_COMPLETE"], ""
    
    # Split text into parts ending with punctuation
    # This regex splits on punctuation but keeps the punctuation in the result
    parts = re.split(r'([.!?ØŸ])', text)
    
    current_building = ""
    i = 0
    while i < len(parts):
        part = parts[i].strip()
        if not part:
            i += 1
            continue
            
        if part in sentence_endings:
            # Found punctuation - complete the current sentence
            if current_building.strip():
                complete_sentence = current_building.strip() + part
                complete_sentences.append(complete_sentence)
                logger.debug(f"âœ… Complete sentence found: '{complete_sentence}'")
                current_building = ""
            i += 1
        else:
            # Regular text - add to current building
            current_building += (" " + part if current_building else part)
            i += 1
    
    # Any remaining text becomes the incomplete part
    if current_building.strip():
        remaining_text = current_building.strip()
        logger.debug(f"ðŸ”„ Remaining incomplete text: '{remaining_text}'")
    
    logger.debug(f"ðŸ“Š Extracted {len(complete_sentences)} complete sentences, remaining: '{remaining_text}'")
    return complete_sentences, remaining_text


def is_sentence_ending(text: str) -> bool:
    """
    Check if the text ends with a sentence-ending punctuation mark.
    
    Args:
        text: The text to check
        
    Returns:
        True if the text ends with sentence-ending punctuation
    """
    if not text.strip():
        return False
    
    sentence_endings = ['.', '!', '?', 'ØŸ']
    return text.strip()[-1] in sentence_endings


def clean_text(text: str) -> str:
    """
    Clean and normalize text for processing.
    
    Args:
        text: The text to clean
        
    Returns:
        Cleaned text
    """
    # Remove extra whitespace
    text = ' '.join(text.split())
    # Remove leading/trailing whitespace
    text = text.strip()
    return text


def split_into_chunks(text: str, max_length: int = 500) -> List[str]:
    """
    Split text into chunks of maximum length, preferring to split at sentence boundaries.
    
    Args:
        text: The text to split
        max_length: Maximum length of each chunk
        
    Returns:
        List of text chunks
    """
    if len(text) <= max_length:
        return [text]
    
    chunks = []
    sentences, _ = extract_complete_sentences(text)
    
    current_chunk = ""
    for sentence in sentences:
        if len(current_chunk) + len(sentence) + 1 <= max_length:
            current_chunk += (" " + sentence if current_chunk else sentence)
        else:
            if current_chunk:
                chunks.append(current_chunk)
            current_chunk = sentence
    
    if current_chunk:
        chunks.append(current_chunk)
    
    return chunks


================================================
FILE: translation_helpers.py
================================================
"""
Translation helper functions for LiveKit AI Translation Server.
Handles translation orchestration and related utilities.
"""
import asyncio
import logging
from typing import List, Dict, Any, Optional

logger = logging.getLogger("transcriber.translation")


async def translate_sentences(
    sentences: List[str], 
    translators: Dict[str, Any],
    source_language: str = "ar",
    sentence_id: Optional[str] = None
) -> None:
    """
    Translate complete sentences to all target languages.
    
    This function takes a list of sentences and sends them to all available
    translators concurrently for better performance.
    
    Args:
        sentences: List of sentences to translate
        translators: Dictionary of language code to translator instances
        source_language: Source language code (default: "ar" for Arabic)
        sentence_id: Optional sentence ID for tracking
    """
    if not sentences or not translators:
        return
        
    for sentence in sentences:
        if sentence.strip():
            logger.info(f"ðŸŽ¯ TRANSLATING COMPLETE {source_language.upper()} SENTENCE: '{sentence}'")
            logger.info(f"ðŸ“Š Processing sentence for {len(translators)} translators")
            
            # Send to all translators concurrently for better performance
            translation_tasks = []
            for lang, translator in translators.items():
                logger.info(f"ðŸ“¤ Sending complete {source_language.upper()} sentence '{sentence}' to {lang} translator")
                translation_tasks.append(translator.translate(sentence, sentence_id))
            
            # Execute all translations concurrently
            if translation_tasks:
                results = await asyncio.gather(*translation_tasks, return_exceptions=True)
                # Check for any exceptions
                for i, result in enumerate(results):
                    if isinstance(result, Exception):
                        logger.error(f"âŒ Translation failed: {result}")


async def translate_single_sentence(
    sentence: str,
    translator: Any,
    target_language: str
) -> Optional[str]:
    """
    Translate a single sentence to a specific target language.
    
    Args:
        sentence: The sentence to translate
        translator: The translator instance to use
        target_language: Target language code
        
    Returns:
        Translated text or None if translation failed
    """
    try:
        if not sentence.strip():
            return None
            
        logger.debug(f"Translating to {target_language}: '{sentence}'")
        result = await translator.translate(sentence, None)
        return result
    except Exception as e:
        logger.error(f"Translation to {target_language} failed: {e}")
        return None


def should_translate_text(text: str, min_length: int = 3) -> bool:
    """
    Determine if text should be translated based on various criteria.
    
    Args:
        text: The text to evaluate
        min_length: Minimum length for translation (default: 3 characters)
        
    Returns:
        True if text should be translated
    """
    if not text or not text.strip():
        return False
    
    # Don't translate very short text
    if len(text.strip()) < min_length:
        return False
    
    # Don't translate if it's only punctuation
    if all(c in '.!?ØŸ,ØŒ;Ø›:' for c in text.strip()):
        return False
    
    return True


def format_translation_output(
    original_text: str,
    translated_text: str,
    source_lang: str,
    target_lang: str
) -> Dict[str, str]:
    """
    Format translation output for consistent structure.
    
    Args:
        original_text: Original text
        translated_text: Translated text
        source_lang: Source language code
        target_lang: Target language code
        
    Returns:
        Formatted translation dictionary
    """
    return {
        "original": original_text,
        "translated": translated_text,
        "source_language": source_lang,
        "target_language": target_lang,
        "type": "translation"
    }


async def batch_translate(
    texts: List[str],
    translators: Dict[str, Any],
    batch_size: int = 5
) -> Dict[str, List[str]]:
    """
    Translate multiple texts in batches for efficiency.
    
    Args:
        texts: List of texts to translate
        translators: Dictionary of language code to translator instances
        batch_size: Number of texts to process in each batch
        
    Returns:
        Dictionary mapping language codes to lists of translations
    """
    results = {lang: [] for lang in translators.keys()}
    
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]
        
        # Process each batch
        for text in batch:
            if should_translate_text(text):
                await translate_sentences([text], translators)
    
    return results


================================================
FILE: translator.py
================================================
"""
Translator module for LiveKit AI Translation Server.
Handles real-time translation with context management and error handling.
"""
import asyncio
import logging
from typing import Optional, Dict, Any, List, Callable
from collections import deque
from enum import Enum

from livekit import rtc
from livekit.agents import llm, utils
from livekit.plugins import openai

from config import get_config
from prompt_builder import get_prompt_builder

logger = logging.getLogger("transcriber.translator")
config = get_config()
prompt_builder = get_prompt_builder()


class TranslationError(Exception):
    """Custom exception for translation-related errors."""
    pass


class Translator:
    """
    Handles translation from source language to target language with context management.
    
    Features:
    - Sliding window context for better translation coherence
    - Automatic retry on failures
    - Comprehensive error handling
    - Real-time broadcasting to displays
    """
    
    # Class-level configuration from config module
    use_context = config.translation.use_context
    default_max_context_pairs = config.translation.max_context_pairs
    
    def __init__(self, room: rtc.Room, lang: Enum, tenant_context: Optional[Dict[str, Any]] = None, broadcast_callback: Optional[Callable] = None):
        """
        Initialize the Translator.
        
        Args:
            room: LiveKit room instance
            lang: Target language enum
            tenant_context: Optional context containing room_id, mosque_id, etc.
            broadcast_callback: Optional callback function for broadcasting translations
        """
        self.room = room
        self.lang = lang
        self.tenant_context = tenant_context or {}
        self.broadcast_callback = broadcast_callback
        self.llm = openai.LLM()
        
        # Initialize system prompt as None - will be built dynamically
        self.system_prompt = None
        self._prompt_template = None
        
        # Get context window size from room config or use default
        self.max_context_pairs = config.translation.get_context_window_size(tenant_context)
        
        # Use deque for automatic sliding window (old messages auto-removed)
        if self.use_context:
            self.message_history: deque = deque(maxlen=(self.max_context_pairs * 2))
        
        # Track translation statistics
        self.translation_count = 0
        self.error_count = 0
        
        # Log the context mode being used
        context_mode = f"TRUE SLIDING WINDOW ({self.max_context_pairs}-pair memory)" if self.use_context else "FRESH CONTEXT (no memory)"
        logger.info(f"ðŸ§  Translator initialized for {lang.value} with {context_mode} mode")
        
        # Initialize prompt asynchronously on first use
        self._prompt_initialized = False

    async def translate(self, message: str, sentence_id: Optional[str] = None, max_retries: int = 2) -> str:
        """
        Translate a message from source to target language.
        
        Args:
            message: Text to translate
            sentence_id: Optional sentence ID for tracking
            max_retries: Maximum number of retry attempts on failure
            
        Returns:
            Translated text (empty string on failure)
            
        Raises:
            TranslationError: If translation fails after all retries
        """
        if not message or not message.strip():
            logger.debug("Empty message, skipping translation")
            return ""
        
        retry_count = 0
        last_error = None
        
        while retry_count <= max_retries:
            try:
                translated_message = await self._perform_translation(message)
                
                if translated_message:
                    # Publish transcription to LiveKit room
                    await self._publish_transcription(translated_message, None)
                    
                    # Broadcast to displays
                    await self._broadcast_translation(translated_message, sentence_id)
                    
                    # Update statistics
                    self.translation_count += 1
                    
                    # Log successful translation
                    logger.info(f"âœ… Translated to {self.lang.value}: '{message}' â†’ '{translated_message}'")
                    
                    return translated_message
                else:
                    logger.warning(f"Empty translation result for: '{message}'")
                    return ""
                    
            except Exception as e:
                last_error = e
                retry_count += 1
                self.error_count += 1
                
                if retry_count <= max_retries:
                    wait_time = retry_count * 0.5  # Exponential backoff
                    logger.warning(
                        f"Translation attempt {retry_count} failed: {e}. "
                        f"Retrying in {wait_time}s..."
                    )
                    await asyncio.sleep(wait_time)
                else:
                    logger.error(
                        f"âŒ Translation failed after {max_retries} retries: {e}\n"
                        f"Message: '{message}'"
                    )
                    
        # If we get here, all retries failed
        error_msg = f"Translation failed for '{message}' after {max_retries} retries"
        if last_error:
            error_msg += f": {last_error}"
        
        # Don't raise exception - return empty string to keep stream going
        logger.error(error_msg)
        return ""

    async def _initialize_prompt(self):
        """Initialize the system prompt using the prompt builder."""
        if self._prompt_initialized:
            return
            
        try:
            # Get room ID from tenant context
            room_id = self.tenant_context.get('room_id')
            
            # Get source language from room config
            source_lang_code = self.tenant_context.get('transcription_language', 'ar')
            source_lang_name = config.translation.supported_languages.get(
                source_lang_code, 
                {"name": "Arabic"}
            )["name"]
            
            # Get target language name from the enum value
            target_lang_name = self.lang.name  # This should give us "Dutch" instead of "nl"
            
            # Build the prompt using prompt builder
            self.system_prompt = await prompt_builder.get_prompt_for_room(
                room_id=room_id,
                source_lang=source_lang_name,
                target_lang=target_lang_name,
                room_config=self.tenant_context
            )
            
            logger.info(f"ðŸ“ Initialized translation prompt for room {room_id}: {source_lang_name} â†’ {target_lang_name} (code: {self.lang.value})")
            self._prompt_initialized = True
            
        except Exception as e:
            logger.error(f"Failed to initialize prompt: {e}")
            # Fallback to default prompt with dynamic source language
            source_lang_code = self.tenant_context.get('transcription_language', 'ar')
            source_lang_name = config.translation.supported_languages.get(
                source_lang_code, 
                {"name": "Arabic"}
            )["name"]
            
            self.system_prompt = (
                f"You are an expert simultaneous interpreter. Your task is to translate from {source_lang_name} to {self.lang.value}. "
                f"Provide a direct and accurate translation of the user's input. Be concise and use natural-sounding language. "
                f"Do not add any additional commentary, explanations, or introductory phrases."
            )
            self._prompt_initialized = True
    
    async def _perform_translation(self, message: str) -> str:
        """
        Perform the actual translation using the LLM.
        
        Args:
            message: Text to translate
            
        Returns:
            Translated text
        """
        # Ensure prompt is initialized
        await self._initialize_prompt()
        # Build a fresh context for every translation (rebuild method)
        temp_context = llm.ChatContext()
        temp_context.add_message(role="system", content=self.system_prompt)
        
        # If using context, add the message history from our deque
        if self.use_context and hasattr(self, 'message_history'):
            logger.debug(f"ðŸ”„ Building context with {len(self.message_history)} historical messages")
            for msg in self.message_history:
                temp_context.add_message(role=msg['role'], content=msg['content'])
        
        # Add the current message to translate
        temp_context.add_message(content=message, role="user")
        
        # Get translation from LLM with the freshly built context
        stream = self.llm.chat(chat_ctx=temp_context)
        
        translated_message = ""
        async for chunk in stream:
            if chunk.delta is None:
                continue
            content = chunk.delta.content
            if content is None:
                break
            translated_message += content
        
        # If using context, update our history (deque will auto-remove old messages)
        if self.use_context and translated_message:
            self.message_history.append({"role": "user", "content": message})
            self.message_history.append({"role": "assistant", "content": translated_message})
            logger.debug(f"ðŸ’¾ History updated. Current size: {len(self.message_history)} messages")
        
        return translated_message

    async def _publish_transcription(self, translated_text: str, track: Optional[rtc.Track]) -> None:
        """
        Publish the translation as a transcription to the LiveKit room.
        
        Args:
            translated_text: The translated text to publish
            track: Optional track reference
        """
        try:
            segment = rtc.TranscriptionSegment(
                id=utils.misc.shortuuid("SG_"),
                text=translated_text,
                start_time=0,
                end_time=0,
                language=self.lang.value,
                final=True,
            )
            transcription = rtc.Transcription(
                self.room.local_participant.identity, 
                track.sid if track else "", 
                [segment]
            )
            await self.room.local_participant.publish_transcription(transcription)
            logger.debug(f"ðŸ“¤ Published {self.lang.value} transcription to LiveKit room")
        except Exception as e:
            logger.error(f"Failed to publish transcription: {e}")
            # Don't re-raise - translation was successful even if publishing failed

    async def _broadcast_translation(self, translated_text: str, sentence_id: Optional[str] = None) -> None:
        """
        Broadcast the translation to WebSocket displays.
        
        Args:
            translated_text: The translated text to broadcast
            sentence_id: Optional sentence ID for tracking
        """
        if self.broadcast_callback:
            try:
                # Use asyncio.create_task to avoid blocking
                # Include sentence context if provided
                sentence_context = None
                if sentence_id:
                    sentence_context = {
                        "sentence_id": sentence_id,
                        "is_complete": True,
                        "is_fragment": False
                    }
                
                asyncio.create_task(
                    self.broadcast_callback(
                        "translation", 
                        self.lang.value, 
                        translated_text, 
                        self.tenant_context,
                        sentence_context
                    )
                )
                logger.debug(f"ðŸ“¡ Broadcasted {self.lang.value} translation to displays")
            except Exception as e:
                logger.error(f"Failed to broadcast translation: {e}")
                # Don't re-raise - translation was successful even if broadcasting failed
        else:
            logger.debug("No broadcast callback provided, skipping broadcast")

    def get_statistics(self) -> Dict[str, Any]:
        """
        Get translation statistics.
        
        Returns:
            Dictionary containing translation stats
        """
        return {
            "language": self.lang.value,
            "translation_count": self.translation_count,
            "error_count": self.error_count,
            "error_rate": self.error_count / max(1, self.translation_count),
            "context_enabled": self.use_context,
            "context_size": len(self.message_history) if self.use_context else 0
        }

    def clear_context(self) -> None:
        """Clear the translation context history."""
        if self.use_context and hasattr(self, 'message_history'):
            self.message_history.clear()
            logger.info(f"ðŸ§¹ Cleared translation context for {self.lang.value}")

    def __repr__(self) -> str:
        """String representation of the Translator."""
        return (
            f"Translator(lang={self.lang.value}, "
            f"context={self.use_context}, "
            f"translations={self.translation_count}, "
            f"errors={self.error_count})"
        )


================================================
FILE: update_from_server_dev.sh
================================================
#!/bin/bash

# Server_Dev to Production Update Script for Bayaan Server
# This script intelligently syncs changes from server_dev while preserving production optimizations
# Author: Bayaan DevOps Team
# Date: $(date +%Y-%m-%d)

set -euo pipefail  # Exit on error, undefined variables, pipe failures

# Configuration
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
DEV_DIR="${SCRIPT_DIR}/server_dev"
PROD_DIR="${SCRIPT_DIR}"
BACKUP_DIR="${PROD_DIR}/backup_$(date +%Y%m%d_%H%M%S)"
LOG_FILE="${PROD_DIR}/update_server_dev_$(date +%Y%m%d_%H%M%S).log"
MERGE_REPORT="${PROD_DIR}/merge_report_$(date +%Y%m%d_%H%M%S).md"

# Color codes for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color

# Logging function
log() {
    echo -e "$1" | tee -a "$LOG_FILE"
}

# Error handling
error_exit() {
    log "${RED}ERROR: $1${NC}"
    log "${YELLOW}Rolling back changes...${NC}"
    rollback
    exit 1
}

# Initialize merge report
init_merge_report() {
    cat > "$MERGE_REPORT" << EOF
# Server_Dev â†’ Production Update Report
**Date:** $(date)
**Source:** $DEV_DIR
**Target:** $PROD_DIR

## Update Summary

EOF
}

# Add to merge report
report() {
    echo "$1" >> "$MERGE_REPORT"
}

# Rollback function
rollback() {
    if [ -d "$BACKUP_DIR" ]; then
        log "${YELLOW}Restoring from backup: $BACKUP_DIR${NC}"
        
        # Restore all backed up files
        for file in "$BACKUP_DIR"/*; do
            if [ -f "$file" ]; then
                filename=$(basename "$file")
                cp -f "$file" "$PROD_DIR/$filename" 2>/dev/null || true
                log "Restored: $filename"
            fi
        done
        
        log "${GREEN}Rollback completed${NC}"
    else
        log "${RED}No backup found for rollback${NC}"
    fi
}

# Pre-flight checks
preflight_checks() {
    log "${CYAN}=== Starting Pre-flight Checks ===${NC}"
    
    # Check if server_dev directory exists
    if [ ! -d "$DEV_DIR" ]; then
        error_exit "Server_dev directory not found: $DEV_DIR"
    fi
    
    # Check if we're in the production directory
    if [ "$(pwd)" != "$PROD_DIR" ]; then
        log "${YELLOW}Changing to production directory${NC}"
        cd "$PROD_DIR" || error_exit "Cannot change to production directory"
    fi
    
    # Check for critical production files
    local critical_files=("main_production.py" "render.yaml" "requirements.txt" "Dockerfile")
    for file in "${critical_files[@]}"; do
        if [ ! -f "$file" ]; then
            log "${RED}WARNING: Critical production file missing: $file${NC}"
        fi
    done
    
    # Verify Python is available
    if ! command -v python3 &> /dev/null; then
        error_exit "Python3 is required but not installed"
    fi
    
    log "${GREEN}Pre-flight checks passed${NC}"
}

# Create backup
create_backup() {
    log "${CYAN}=== Creating Backup ===${NC}"
    
    # Create backup directory
    mkdir -p "$BACKUP_DIR" || error_exit "Cannot create backup directory"
    
    # Backup all Python files and critical configs
    local files_to_backup=(
        "*.py"
        ".env"
        "requirements.txt"
        "Dockerfile"
        "render.yaml"
        ".gitignore"
        "*.sh"
    )
    
    for pattern in "${files_to_backup[@]}"; do
        for file in $pattern; do
            if [ -f "$file" ]; then
                cp -p "$file" "$BACKUP_DIR/" 2>/dev/null || true
                log "Backed up: $file"
            fi
        done
    done
    
    log "${GREEN}Backup created at: $BACKUP_DIR${NC}"
    report "### Backup Location\n\`$BACKUP_DIR\`\n"
}

# Compare files and determine update strategy
compare_and_update() {
    local file=$1
    local src_file="$DEV_DIR/$file"
    local dst_file="$PROD_DIR/$file"
    
    if [ ! -f "$src_file" ]; then
        log "${YELLOW}Source file not found, skipping: $file${NC}"
        return 1
    fi
    
    # If destination doesn't exist, it's a simple copy
    if [ ! -f "$dst_file" ]; then
        cp -f "$src_file" "$dst_file"
        log "${GREEN}Added new file: $file${NC}"
        report "- **Added:** \`$file\` (new file from server_dev)"
        return 0
    fi
    
    # Check if files are different
    if ! diff -q "$src_file" "$dst_file" > /dev/null 2>&1; then
        return 0  # Files are different, needs update
    else
        return 1  # Files are identical
    fi
}

# Update simple files (direct replacement)
update_simple_files() {
    log "${CYAN}=== Updating Simple Files ===${NC}"
    report "\n### Simple File Updates\n"
    
    # Files that can be safely replaced without merging
    local simple_files=(
        "config.py"
        "prompt_builder.py"
        "text_processing.py"
        "translation_helpers.py"
        "translator.py"
        "webhook_handler.py"
    )
    
    for file in "${simple_files[@]}"; do
        if compare_and_update "$file"; then
            cp -f "$DEV_DIR/$file" "$PROD_DIR/$file"
            log "${GREEN}Updated: $file${NC}"
            report "- **Updated:** \`$file\`"
        else
            log "${BLUE}No changes needed: $file${NC}"
        fi
    done
}

# Handle complex files with merge logic
update_complex_files() {
    log "${CYAN}=== Handling Complex Files ===${NC}"
    report "\n### Complex File Handling\n"
    
    # main.py - Preserve production optimizations
    if [ -f "$DEV_DIR/main.py" ]; then
        log "${YELLOW}Analyzing main.py differences...${NC}"
        
        # Create a comparison report
        if diff -u "$PROD_DIR/main.py" "$DEV_DIR/main.py" > /tmp/main_diff.txt 2>&1; then
            log "${BLUE}main.py is identical in both versions${NC}"
        else
            log "${YELLOW}main.py has differences - preserving production optimizations${NC}"
            report "- **main.py:** Differences detected - production optimizations preserved"
            report "  - Kept production's interim transcript handling"
            report "  - Kept production's simplified cleanup approach"
            report "  - Review \`/tmp/main_diff.txt\` for detailed differences"
            
            # Don't update main.py automatically - requires manual review
            log "${YELLOW}âš ï¸  main.py requires manual review due to production-specific optimizations${NC}"
        fi
    fi
    
    # database.py - Check for schema changes
    if [ -f "$DEV_DIR/database.py" ]; then
        if compare_and_update "database.py"; then
            log "${YELLOW}database.py has changes - reviewing for compatibility...${NC}"
            
            # Check if new functions are added that don't exist in production
            if grep -q "close_room_session\|update_session_heartbeat" "$DEV_DIR/database.py"; then
                log "${YELLOW}New database functions detected (heartbeat/session management)${NC}"
                report "- **database.py:** New session management functions detected"
                report "  - Contains heartbeat monitoring functions not used in production"
                report "  - Manual review recommended"
            fi
            
            # For now, skip automatic update of database.py
            log "${YELLOW}âš ï¸  database.py update skipped - manual review required${NC}"
        fi
    fi
    
    # broadcasting.py
    if compare_and_update "broadcasting.py"; then
        cp -f "$DEV_DIR/broadcasting.py" "$PROD_DIR/broadcasting.py"
        log "${GREEN}Updated: broadcasting.py${NC}"
        report "- **Updated:** \`broadcasting.py\`"
    fi
    
    # resource_management.py
    if compare_and_update "resource_management.py"; then
        cp -f "$DEV_DIR/resource_management.py" "$PROD_DIR/resource_management.py"
        log "${GREEN}Updated: resource_management.py${NC}"
        report "- **Updated:** \`resource_management.py\`"
    fi
}

# Handle new files from server_dev
handle_new_files() {
    log "${CYAN}=== Checking for New Files ===${NC}"
    report "\n### New Files Analysis\n"
    
    # Files to explicitly exclude
    local exclude_patterns=(
        "*_cleanup*.py"
        "*_fix.py"
        "*.sql"
        "start_server.sh"
        "production_deployment.md"
        "DEPLOYMENT.md"
        "CLEANUP_SUMMARY.md"
    )
    
    # Check for new Python files
    for file in "$DEV_DIR"/*.py; do
        if [ -f "$file" ]; then
            filename=$(basename "$file")
            
            # Skip if file exists in production
            if [ -f "$PROD_DIR/$filename" ]; then
                continue
            fi
            
            # Check exclusion patterns
            local skip=false
            for pattern in "${exclude_patterns[@]}"; do
                if [[ "$filename" == $pattern ]]; then
                    skip=true
                    break
                fi
            done
            
            if [ "$skip" = true ]; then
                log "${YELLOW}Excluded new file: $filename${NC}"
                report "- **Excluded:** \`$filename\` (development/cleanup file)"
            else
                log "${CYAN}Found new file: $filename - requires review${NC}"
                report "- **New file found:** \`$filename\` - manual review required"
            fi
        fi
    done
}

# Update requirements.txt intelligently
update_requirements() {
    log "${CYAN}=== Checking requirements.txt ===${NC}"
    report "\n### Dependencies Update\n"
    
    if [ -f "$DEV_DIR/requirements.txt" ] && [ -f "$PROD_DIR/requirements.txt" ]; then
        # Create sorted unique lists
        sort "$DEV_DIR/requirements.txt" | grep -v "^#" | grep -v "^$" > /tmp/dev_reqs.txt
        sort "$PROD_DIR/requirements.txt" | grep -v "^#" | grep -v "^$" > /tmp/prod_reqs.txt
        
        # Find new requirements in dev
        comm -23 /tmp/dev_reqs.txt /tmp/prod_reqs.txt > /tmp/new_reqs.txt
        
        if [ -s /tmp/new_reqs.txt ]; then
            log "${YELLOW}New dependencies found in server_dev:${NC}"
            cat /tmp/new_reqs.txt | while read -r req; do
                log "  + $req"
                report "- New dependency: \`$req\`"
            done
            report "\nâš ï¸  **Action Required:** Review and add new dependencies to production requirements.txt"
        else
            log "${GREEN}No new dependencies found${NC}"
            report "- No new dependencies detected"
        fi
        
        # Cleanup temp files
        rm -f /tmp/dev_reqs.txt /tmp/prod_reqs.txt /tmp/new_reqs.txt
    fi
}

# Verify Python syntax
verify_python_syntax() {
    log "${CYAN}=== Verifying Python Syntax ===${NC}"
    
    local all_good=true
    
    for file in *.py; do
        if [ -f "$file" ]; then
            if python3 -m py_compile "$file" 2>/dev/null; then
                log "${GREEN}âœ“ Syntax OK: $file${NC}"
            else
                log "${RED}âœ— Syntax error in: $file${NC}"
                all_good=false
            fi
        fi
    done
    
    # Clean up __pycache__
    rm -rf __pycache__ 2>/dev/null || true
    
    if [ "$all_good" = false ]; then
        error_exit "Python syntax verification failed"
    fi
    
    log "${GREEN}All Python files passed syntax check${NC}"
}

# Generate final recommendations
generate_recommendations() {
    log "${CYAN}=== Generating Recommendations ===${NC}"
    
    report "\n## Post-Update Recommendations\n"
    report "### Manual Review Required:"
    report "1. **main.py** - Review differences between dev and production versions"
    report "2. **database.py** - Check if new session management functions are needed"
    report "3. **New files** - Evaluate any new files from server_dev for inclusion"
    report "4. **Dependencies** - Review and update requirements.txt if needed"
    report ""
    report "### Testing Checklist:"
    report "- [ ] Run local tests with updated code"
    report "- [ ] Verify WebSocket connections work correctly"
    report "- [ ] Test transcript handling (both final and interim)"
    report "- [ ] Confirm database operations function properly"
    report "- [ ] Check resource cleanup on disconnection"
    report ""
    report "### Deployment Steps:"
    report "1. Review this report and the update log"
    report "2. Manually review complex files if needed"
    report "3. Run \`git status\` to see all changes"
    report "4. Test locally if possible"
    report "5. Commit changes with descriptive message"
    report "6. Deploy to Render following standard procedure"
    report ""
    report "### Rollback Instructions:"
    report "If issues occur, run: \`bash $0 --rollback\`"
    report ""
    report "**Backup Location:** \`$BACKUP_DIR\`"
    report "**Log File:** \`$LOG_FILE\`"
}

# Post-update summary
post_update_summary() {
    log ""
    log "${GREEN}=== Update Completed Successfully ===${NC}"
    log ""
    log "${CYAN}Important Files:${NC}"
    log "  ðŸ“„ Merge Report: ${YELLOW}$MERGE_REPORT${NC}"
    log "  ðŸ“‹ Log File: ${YELLOW}$LOG_FILE${NC}"
    log "  ðŸ’¾ Backup: ${YELLOW}$BACKUP_DIR${NC}"
    log ""
    log "${YELLOW}Next Steps:${NC}"
    log "  1. Review the merge report for detailed changes"
    log "  2. Manually review files marked for attention"
    log "  3. Run tests before deploying"
    log ""
    log "To rollback if needed: ${CYAN}bash $0 --rollback${NC}"
}

# Rollback functionality
if [ "${1:-}" == "--rollback" ]; then
    # Initialize log for rollback
    LOG_FILE="${PROD_DIR}/rollback_$(date +%Y%m%d_%H%M%S).log"
    
    # Find the most recent backup
    LATEST_BACKUP=$(ls -td ${PROD_DIR}/backup_* 2>/dev/null | head -1)
    if [ -n "$LATEST_BACKUP" ]; then
        BACKUP_DIR="$LATEST_BACKUP"
        log "${YELLOW}Rolling back to: $BACKUP_DIR${NC}"
        rollback
        log "${GREEN}Rollback completed successfully${NC}"
        exit 0
    else
        log "${RED}No backup found for rollback${NC}"
        exit 1
    fi
fi

# Main execution
main() {
    log "${GREEN}=== Bayaan Server_Dev â†’ Production Update Script ===${NC}"
    log "Started at: $(date)"
    log "Source: $DEV_DIR"
    log "Target: $PROD_DIR"
    log ""
    
    # Initialize merge report
    init_merge_report
    
    # Execute update steps
    preflight_checks
    create_backup
    update_simple_files
    update_complex_files
    handle_new_files
    update_requirements
    verify_python_syntax
    generate_recommendations
    post_update_summary
    
    log "Completed at: $(date)"
}

# Run main function
main "$@"


================================================
FILE: update_prod.sh
================================================
#!/bin/bash

# Production Update Script for Bayaan Server
# This script safely updates production files from development while preserving production-specific configurations
# Author: Senior DevOps Engineer
# Date: $(date +%Y-%m-%d)

set -euo pipefail  # Exit on error, undefined variables, pipe failures

# Configuration
DEV_DIR="/mnt/c/Users/hassa/OneDrive/Desktop/0.2 Bayan/bayan-platform-admin-login/Backend/LiveKit-ai-translation/server"
PROD_DIR="/mnt/c/Users/hassa/OneDrive/Desktop/0.2 Bayan/bayaan-server-production"
BACKUP_DIR="${PROD_DIR}/backup_$(date +%Y%m%d_%H%M%S)"
LOG_FILE="${PROD_DIR}/update_$(date +%Y%m%d_%H%M%S).log"

# Color codes for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

# Logging function
log() {
    echo -e "$1" | tee -a "$LOG_FILE"
}

# Error handling
error_exit() {
    log "${RED}ERROR: $1${NC}"
    log "${YELLOW}Rolling back changes...${NC}"
    rollback
    exit 1
}

# Rollback function
rollback() {
    if [ -d "$BACKUP_DIR" ]; then
        log "${YELLOW}Restoring from backup: $BACKUP_DIR${NC}"
        
        # Restore Python files
        for file in "$BACKUP_DIR"/*.py; do
            if [ -f "$file" ]; then
                filename=$(basename "$file")
                cp -f "$file" "$PROD_DIR/$filename" 2>/dev/null || true
                log "Restored: $filename"
            fi
        done
        
        log "${GREEN}Rollback completed${NC}"
    else
        log "${RED}No backup found for rollback${NC}"
    fi
}

# Pre-flight checks
preflight_checks() {
    log "${YELLOW}=== Starting Pre-flight Checks ===${NC}"
    
    # Check if dev directory exists
    if [ ! -d "$DEV_DIR" ]; then
        error_exit "Development directory not found: $DEV_DIR"
    fi
    
    # Check if we're in the production directory
    if [ "$(pwd)" != "$PROD_DIR" ]; then
        log "${YELLOW}Changing to production directory${NC}"
        cd "$PROD_DIR" || error_exit "Cannot change to production directory"
    fi
    
    # Check for critical production files
    local critical_files=("main_production.py" "render.yaml" "requirements.txt" "Dockerfile" ".env")
    for file in "${critical_files[@]}"; do
        if [ ! -f "$file" ]; then
            log "${RED}WARNING: Critical production file missing: $file${NC}"
        fi
    done
    
    log "${GREEN}Pre-flight checks passed${NC}"
}

# Create backup
create_backup() {
    log "${YELLOW}=== Creating Backup ===${NC}"
    
    # Create backup directory
    mkdir -p "$BACKUP_DIR" || error_exit "Cannot create backup directory"
    
    # Backup all Python files and critical configs
    local files_to_backup=(
        "*.py"
        ".env"
        "requirements.txt"
        "Dockerfile"
        "render.yaml"
        ".gitignore"
    )
    
    for pattern in "${files_to_backup[@]}"; do
        for file in $pattern; do
            if [ -f "$file" ]; then
                cp -p "$file" "$BACKUP_DIR/" 2>/dev/null || true
                log "Backed up: $file"
            fi
        done
    done
    
    log "${GREEN}Backup created at: $BACKUP_DIR${NC}"
}

# Update files from development
update_files() {
    log "${YELLOW}=== Updating Files from Development ===${NC}"
    
    # Core Python modules to update (excluding dev-specific files)
    local python_files=(
        "main.py"
        "prompt_builder.py"
        "broadcasting.py"
        "config.py"
        "database.py"
        "resource_management.py"
        "text_processing.py"
        "translation_helpers.py"
        "translator.py"
        "webhook_handler.py"
    )
    
    # Files to explicitly exclude
    local exclude_patterns=(
        "*_backup.py"
        "*_fixed.py"
        "*_cleanup*.py"
        "production_deployment.md"
    )
    
    # Update each Python file
    for file in "${python_files[@]}"; do
        local src_file="$DEV_DIR/$file"
        
        if [ -f "$src_file" ]; then
            # Check if file exists in exclude patterns
            local skip=false
            for pattern in "${exclude_patterns[@]}"; do
                if [[ "$file" == $pattern ]]; then
                    skip=true
                    break
                fi
            done
            
            if [ "$skip" = false ]; then
                cp -f "$src_file" "$PROD_DIR/$file" || error_exit "Failed to copy $file"
                log "${GREEN}Updated: $file${NC}"
            else
                log "${YELLOW}Skipped (excluded): $file${NC}"
            fi
        else
            log "${YELLOW}Not found in dev (skipping): $file${NC}"
        fi
    done
    
    # Handle .env.example if it exists and production doesn't have it
    if [ -f "$DEV_DIR/.env.example" ] && [ ! -f "$PROD_DIR/.env.example" ]; then
        cp -f "$DEV_DIR/.env.example" "$PROD_DIR/.env.example"
        log "${GREEN}Added: .env.example${NC}"
    fi
    
    # Update .gitignore if needed
    if [ -f "$DEV_DIR/.gitignore" ]; then
        cp -f "$DEV_DIR/.gitignore" "$PROD_DIR/.gitignore"
        log "${GREEN}Updated: .gitignore${NC}"
    fi
}

# Verify production integrity
verify_production() {
    log "${YELLOW}=== Verifying Production Integrity ===${NC}"
    
    # Check that production-specific files are still present
    local prod_files=("main_production.py" "render.yaml" "requirements.txt" "Dockerfile" ".env")
    local all_good=true
    
    for file in "${prod_files[@]}"; do
        if [ -f "$file" ]; then
            log "${GREEN}âœ“ Production file intact: $file${NC}"
        else
            log "${RED}âœ— Production file missing: $file${NC}"
            all_good=false
        fi
    done
    
    # Check Python syntax for all .py files
    log "${YELLOW}Checking Python syntax...${NC}"
    for file in *.py; do
        if [ -f "$file" ]; then
            if python3 -m py_compile "$file" 2>/dev/null; then
                log "${GREEN}âœ“ Syntax OK: $file${NC}"
                rm -f "__pycache__/${file%.py}.cpython-*.pyc" 2>/dev/null
            else
                log "${RED}âœ— Syntax error in: $file${NC}"
                all_good=false
            fi
        fi
    done
    
    # Clean up __pycache__
    rmdir __pycache__ 2>/dev/null || true
    
    if [ "$all_good" = false ]; then
        error_exit "Production integrity check failed"
    fi
    
    log "${GREEN}Production integrity verified${NC}"
}

# Post-update recommendations
post_update_recommendations() {
    log "${YELLOW}=== Post-Update Recommendations ===${NC}"
    log ""
    log "1. ${YELLOW}Test locally:${NC} Test the updated code in a staging environment if available"
    log "2. ${YELLOW}Review logs:${NC} Check $LOG_FILE for any warnings"
    log "3. ${YELLOW}Git status:${NC} Run 'git status' to review changes before committing"
    log "4. ${YELLOW}Deploy:${NC} Follow your standard Render deployment process"
    log ""
    log "${GREEN}Update completed successfully!${NC}"
    log ""
    log "Backup location: $BACKUP_DIR"
    log "To rollback if needed, run: ${YELLOW}bash $0 --rollback${NC}"
}

# Standalone rollback option
if [ "${1:-}" == "--rollback" ]; then
    # Find the most recent backup
    LATEST_BACKUP=$(ls -td ${PROD_DIR}/backup_* 2>/dev/null | head -1)
    if [ -n "$LATEST_BACKUP" ]; then
        BACKUP_DIR="$LATEST_BACKUP"
        log "${YELLOW}Rolling back to: $BACKUP_DIR${NC}"
        rollback
        exit 0
    else
        log "${RED}No backup found for rollback${NC}"
        exit 1
    fi
fi

# Main execution
main() {
    log "${GREEN}=== Bayaan Production Update Script ===${NC}"
    log "Started at: $(date)"
    log "Dev source: $DEV_DIR"
    log "Production: $PROD_DIR"
    log ""
    
    # Execute update steps
    preflight_checks
    create_backup
    update_files
    verify_production
    post_update_recommendations
    
    log "Completed at: $(date)"
}

# Run main function
main "$@"


================================================
FILE: update_server.sh
================================================
#!/bin/bash

# Bayaan Server Update Script
# Updates production server with development files

set -e  # Exit on error

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

# Paths
PROD_DIR="/mnt/c/Users/hassa/OneDrive/Desktop/0.2 Bayan/bayaan-server-production"
DEV_DIR="/mnt/c/Users/hassa/OneDrive/Desktop/0.2 Bayan/Dev/bayan-platform-admin-login/Backend/LiveKit-ai-translation/server"
BACKUP_DIR="$PROD_DIR/backup_$(date +%Y%m%d_%H%M%S)"

echo -e "${GREEN}=== Bayaan Server Update Script ===${NC}"
echo "Production Dir: $PROD_DIR"
echo "Development Dir: $DEV_DIR"
echo ""

# Check if DEV directory exists
if [ ! -d "$DEV_DIR" ]; then
    echo -e "${RED}Error: Development directory not found!${NC}"
    exit 1
fi

# Step 1: Create backup
echo -e "${YELLOW}Step 1: Creating backup...${NC}"
mkdir -p "$BACKUP_DIR"

# Backup files that will be updated
cp "$PROD_DIR/main.py" "$BACKUP_DIR/" 2>/dev/null || true
cp "$PROD_DIR/resource_management.py" "$BACKUP_DIR/" 2>/dev/null || true
cp "$PROD_DIR/translation_helpers.py" "$BACKUP_DIR/" 2>/dev/null || true
cp "$PROD_DIR/broadcasting.py" "$BACKUP_DIR/" 2>/dev/null || true

echo -e "${GREEN}âœ“ Backup created at: $BACKUP_DIR${NC}"

# Step 2: Copy development files to production
echo -e "${YELLOW}Step 2: Copying development files...${NC}"

# Copy the main files
cp "$DEV_DIR/main.py" "$PROD_DIR/"
echo "  âœ“ Copied main.py"

cp "$DEV_DIR/resource_management.py" "$PROD_DIR/"
echo "  âœ“ Copied resource_management.py"

cp "$DEV_DIR/translation_helpers.py" "$PROD_DIR/"
echo "  âœ“ Copied translation_helpers.py"

cp "$DEV_DIR/broadcasting.py" "$PROD_DIR/"
echo "  âœ“ Copied broadcasting.py"

# Check if other files need updating
echo -e "${YELLOW}Step 3: Checking other files...${NC}"

# List of other files that might need updating
OTHER_FILES=("config.py" "database.py" "text_processing.py" "translator.py" "webhook_handler.py" "prompt_builder.py")

for file in "${OTHER_FILES[@]}"; do
    if [ -f "$DEV_DIR/$file" ]; then
        # Check if files are different
        if ! cmp -s "$PROD_DIR/$file" "$DEV_DIR/$file"; then
            echo -e "  ${YELLOW}! $file differs between DEV and PROD${NC}"
            cp "$PROD_DIR/$file" "$BACKUP_DIR/" 2>/dev/null || true
            cp "$DEV_DIR/$file" "$PROD_DIR/"
            echo "    âœ“ Updated $file"
        else
            echo "  - $file is identical (no update needed)"
        fi
    fi
done

# Step 4: Log the update
echo -e "${YELLOW}Step 4: Creating update log...${NC}"
cat > "$PROD_DIR/update_$(date +%Y%m%d_%H%M%S).log" << EOF
Update performed at: $(date)
Files updated:
- main.py (added heartbeat monitoring and sentence tracking)
- resource_management.py (added HeartbeatMonitor class)
- translation_helpers.py (added sentence_id parameter)
- broadcasting.py (added sentence context support)

Key improvements:
1. Heartbeat monitoring - Detects stuck participant sessions (45s timeout)
2. Sentence tracking - Unique IDs for better UI synchronization
3. Fragment handling - Improved real-time display
4. Resource management - Better cleanup and monitoring

Backup location: $BACKUP_DIR
EOF

echo -e "${GREEN}âœ“ Update log created${NC}"

# Step 5: Verify installation
echo -e "${YELLOW}Step 5: Verifying installation...${NC}"

# Check if key imports work
python3 -c "
import sys
sys.path.insert(0, '$PROD_DIR')
try:
    from resource_management import HeartbeatMonitor
    print('  âœ“ HeartbeatMonitor class imported successfully')
except ImportError as e:
    print('  âœ— Failed to import HeartbeatMonitor:', e)
    sys.exit(1)
"

if [ $? -eq 0 ]; then
    echo -e "${GREEN}âœ“ Verification passed${NC}"
else
    echo -e "${RED}âœ— Verification failed${NC}"
    echo -e "${YELLOW}Rolling back...${NC}"
    # Rollback
    for file in "$BACKUP_DIR"/*.py; do
        if [ -f "$file" ]; then
            filename=$(basename "$file")
            cp "$file" "$PROD_DIR/$filename"
        fi
    done
    echo -e "${GREEN}âœ“ Rollback completed${NC}"
    exit 1
fi

echo ""
echo -e "${GREEN}=== Update Complete ===${NC}"
echo ""
echo "Next steps:"
echo "1. Review the changes in your version control system"
echo "2. Restart the Bayaan server:"
echo "   - If using systemd: sudo systemctl restart bayaan"
echo "   - If using PM2: pm2 restart bayaan"
echo "   - If running directly: restart the Python process"
echo "3. Monitor logs for any errors"
echo "4. Test the heartbeat monitoring feature"
echo ""
echo "To rollback if needed:"
echo "  cp $BACKUP_DIR/*.py $PROD_DIR/"
echo ""


================================================
FILE: UPDATE_TO_126_GUIDE.md
================================================
# Updating from LiveKit Agents 1.2.1 to 1.2.6+

## Current Issue
Agents register successfully but don't receive job requests in 1.2.6+.

## Potential Solutions to Try

### 1. Update Job Request Function
The job acceptance might need additional parameters in newer versions:

```python
async def request_fnc(req: JobRequest):
    logger.info(f"ðŸŽ¯ Received job request for room: {req.room.name if req.room else 'unknown'}")
    
    # Try adding more explicit parameters for 1.2.6+
    await req.accept(
        name="agent",
        identity="agent",
        # New parameters that might be required:
        auto_subscribe=AutoSubscribe.AUDIO_ONLY,  # Explicitly set subscription
        auto_disconnect=True,  # Disconnect when room empties
    )
```

### 2. Check Worker Options
Newer versions might require different WorkerOptions configuration:

```python
if __name__ == "__main__":
    cli.run_app(
        WorkerOptions(
            entrypoint_fnc=entrypoint,
            prewarm_fnc=prewarm,
            request_fnc=request_fnc,
            # Try adding:
            agent_name="bayaan-transcriber",  # Explicit agent name
            worker_type="room",  # Specify worker type
            max_idle_time=60.0,  # Maximum idle time
        )
    )
```

### 3. Frontend Room Creation
The frontend might need to explicitly request an agent when creating rooms:

```javascript
// In your Supabase edge function or frontend:
const room = await livekitClient.createRoom({
    name: roomName,
    metadata: {
        // Add explicit agent request
        agent_request: {
            agents: ["agent"],  // Request specific agent
            dispatch_required: true
        }
    }
});
```

### 4. Environment Variables
Check if new environment variables are needed:
```bash
# Might be required in 1.2.6+
LIVEKIT_AGENT_NAMESPACE=default
LIVEKIT_AGENT_DISPATCH_ENABLED=true
```

### 5. Debug Job Dispatch
Add more logging to understand why jobs aren't arriving:

```python
async def request_fnc(req: JobRequest):
    # Add detailed logging
    logger.info(f"ðŸ“¦ Job request received: {req.__dict__}")
    logger.info(f"ðŸ·ï¸ Dispatch ID: {req.dispatch_id if hasattr(req, 'dispatch_id') else 'N/A'}")
    logger.info(f"ðŸŽ¯ Agent name: {req.agent_name if hasattr(req, 'agent_name') else 'N/A'}")
    
    # Accept the job
    result = await req.accept(name="agent", identity="agent")
    logger.info(f"âœ… Accept result: {result}")
    return result
```

## Testing Approach

1. **Create a minimal test**:
   ```python
   # test_agent.py
   from livekit.agents import cli, WorkerOptions, JobContext, JobRequest
   import logging
   
   logging.basicConfig(level=logging.DEBUG)
   
   async def test_entrypoint(ctx: JobContext):
       print(f"Connected to room: {ctx.room.name}")
   
   async def test_request(req: JobRequest):
       print(f"Got request for: {req.room.name}")
       await req.accept(name="test", identity="test")
   
   if __name__ == "__main__":
       cli.run_app(WorkerOptions(
           entrypoint_fnc=test_entrypoint,
           request_fnc=test_request
       ))
   ```

2. **Test with different LiveKit versions**:
   ```bash
   pip install livekit-agents==1.2.1  # Test working version
   python test_agent.py dev
   
   pip install livekit-agents==1.2.6  # Test newer version
   python test_agent.py dev
   ```

3. **Compare the debug output** to see what's different

## My Recommendation

**For Production**: Stay on 1.2.1 with the July 28 code. It works perfectly and is stable.

**For Development**: Create a separate branch to experiment with 1.2.6+ and figure out the new dispatch mechanism without affecting production.

**Long-term**: Once you understand the changes, you can plan a controlled migration to the newer version.

## The Real Problem
The issue isn't with your code - it's that LiveKit changed how agents are dispatched between these versions. Your frontend is still using the old dispatch method which works with 1.2.1 but not with 1.2.6+.


================================================
FILE: VERSION_STRATEGY.md
================================================
# LiveKit Version Strategy - Decision Guide

## Current Situation
- **Working Version**: LiveKit Agents 1.2.1 (July 28 deployment)
- **Broken Version**: LiveKit Agents 1.2.6+ (new deployments)
- **Core Issue**: Job dispatch mechanism changed between versions

## Your Options

### Option 1: Stay on 1.2.1 (RECOMMENDED) âœ…
**Why this is the best choice:**
- Your system works perfectly with 1.2.1
- No code changes needed
- Domain patch works as intended
- Immediate production stability

**How to implement:**
1. Use `requirements-pinned.txt` for all deployments
2. Keep the July 28 code exactly as is
3. Don't worry about deprecation warnings

### Option 2: Update to 1.2.6+ (Not Recommended) âŒ
**Why this is problematic:**
- Requires debugging the new job dispatch mechanism
- Frontend changes needed for room creation
- Extensive testing required
- Risk of breaking production

**What would need to change:**
- Frontend room creation must explicitly request agents
- Backend job acceptance might need new parameters
- Potential LiveKit Cloud configuration changes

## The Real Problem Explained

The issue isn't your code - it's that LiveKit changed how agents connect to rooms between these versions:

**1.2.1 Behavior:**
```
Frontend creates room â†’ LiveKit automatically dispatches to any registered agent
```

**1.2.6+ Behavior:**
```
Frontend creates room â†’ Must explicitly request specific agent â†’ Agent must match request criteria
```

## Immediate Action Plan

1. **For Production (Today):**
   ```bash
   # Deploy using pinned versions
   pip install -r requirements-pinned.txt
   ```

2. **Keep Your Original Code:**
   - The July 28 version with domain patch
   - TranscriptionConfig wrapper style
   - No changes needed

3. **Why This Works:**
   - LiveKit 1.2.1 is stable and battle-tested
   - Your domain patch functions correctly
   - Job dispatch works as expected

## Long-term Considerations

**If you eventually need to upgrade:**
1. Create a development branch
2. Test the new dispatch mechanism thoroughly
3. Update frontend room creation logic
4. Only deploy after extensive testing

**But for now:**
- You have a working, stable system
- No immediate need to upgrade
- Focus on your business logic, not SDK issues

## Summary

**Your July 28 code is perfect** - the only issue is version mismatch. By pinning to LiveKit 1.2.1, you get:
- âœ… Immediate working deployment
- âœ… Domain patch functionality
- âœ… Proper job dispatch
- âœ… Real-time transcription
- âœ… No code changes needed

The "newer" versions aren't necessarily "better" for your use case. Stick with what works!


================================================
FILE: webhook_handler.py
================================================
#!/usr/bin/env python3
"""
Webhook handler for Supabase integration
Receives notifications about room creation and management from the dashboard
"""

import asyncio
import json
import logging
import os
from aiohttp import web
from typing import Dict, Any

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Webhook secret for validation (should match Supabase webhook secret)
WEBHOOK_SECRET = os.environ.get("SUPABASE_WEBHOOK_SECRET", "")

class WebhookHandler:
    def __init__(self):
        self.active_sessions: Dict[str, Dict[str, Any]] = {}
        
    async def handle_room_created(self, payload: dict):
        """Handle room creation webhook from Supabase"""
        try:
            # Extract room information (aligning with your Supabase schema)
            room_data = payload.get("record", {})
            room_name = room_data.get("Livekit_room_name")  # Note: Capital L in your schema
            mosque_id = room_data.get("mosque_id")
            room_id = room_data.get("id")
            room_title = room_data.get("Title")
            transcription_language = room_data.get("transcription_language")
            translation_language = room_data.get("translation__language")  # Note: double underscore
            
            if not room_name or not mosque_id:
                logger.error(f"Missing required fields in room creation webhook: {payload}")
                return {"status": "error", "message": "Missing Livekit_room_name or mosque_id"}
            
            # Store session information
            self.active_sessions[room_name] = {
                "room_id": room_id,
                "mosque_id": mosque_id,
                "room_title": room_title,
                "transcription_language": transcription_language or "ar",  # Default to Arabic
                "translation_language": translation_language or "nl",     # Default to Dutch
                "created_at": room_data.get("created_at"),
                "status": "active"
            }
            
            logger.info(f"ðŸ›ï¸ Room created for mosque {mosque_id}: {room_name} (ID: {room_id})")
            logger.info(f"ðŸ—£ï¸ Transcription: {transcription_language}, Translation: {translation_language}")
            logger.info(f"ðŸ“Š Active sessions: {len(self.active_sessions)}")
            
            return {"status": "success", "room_name": room_name, "room_id": room_id}
            
        except Exception as e:
            logger.error(f"Error handling room creation webhook: {e}")
            return {"status": "error", "message": str(e)}
    
    async def handle_room_deleted(self, payload: dict):
        """Handle room deletion webhook from Supabase"""
        try:
            # Extract room information
            room_data = payload.get("old_record", {})
            room_name = room_data.get("livekit_room_name")
            
            if room_name and room_name in self.active_sessions:
                del self.active_sessions[room_name]
                logger.info(f"ðŸ—‘ï¸ Room deleted: {room_name}")
                logger.info(f"ðŸ“Š Active sessions: {len(self.active_sessions)}")
            
            return {"status": "success", "room_name": room_name}
            
        except Exception as e:
            logger.error(f"Error handling room deletion webhook: {e}")
            return {"status": "error", "message": str(e)}
    
    async def handle_session_started(self, payload: dict):
        """Handle session start webhook from Supabase"""
        try:
            session_data = payload.get("record", {})
            room_id = session_data.get("room_id")
            session_id = session_data.get("id")
            mosque_id = session_data.get("mosque_id")
            logging_enabled = session_data.get("logging_enabled", False)
            
            logger.info(f"ðŸŽ¤ Session started: {session_id} for room {room_id}, mosque {mosque_id}")
            logger.info(f"ðŸ“ Logging enabled: {logging_enabled}")
            
            # Find matching room by room_id and update with session info
            room_found = False
            for room_name, room_info in self.active_sessions.items():
                if room_info.get("room_id") == room_id:
                    room_info["session_id"] = session_id
                    room_info["session_started_at"] = session_data.get("started_at")
                    room_info["logging_enabled"] = logging_enabled
                    room_info["status"] = "recording" if logging_enabled else "active"
                    logger.info(f"ðŸ›ï¸ Updated room {room_name} with session {session_id}")
                    room_found = True
                    break
            
            if not room_found:
                # Create temporary session entry if room not found
                logger.warning(f"âš ï¸ Room not found for session {session_id}, creating temporary entry")
                temp_room_name = f"session_{session_id[:8]}"
                self.active_sessions[temp_room_name] = {
                    "room_id": room_id,
                    "mosque_id": mosque_id,
                    "session_id": session_id,
                    "session_started_at": session_data.get("started_at"),
                    "logging_enabled": logging_enabled,
                    "status": "recording" if logging_enabled else "active",
                    "transcription_language": "ar",  # Default
                    "translation_language": "nl"    # Default
                }
                    
            return {"status": "success", "session_id": session_id, "logging_enabled": logging_enabled}
            
        except Exception as e:
            logger.error(f"Error handling session start webhook: {e}")
            return {"status": "error", "message": str(e)}
    
    async def handle_session_ended(self, payload: dict):
        """Handle session end webhook from Supabase"""
        try:
            session_data = payload.get("record", {})
            session_id = session_data.get("id")
            
            # Update session status
            for room_name, room_info in self.active_sessions.items():
                if room_info.get("session_id") == session_id:
                    room_info["session_ended_at"] = session_data.get("ended_at")
                    room_info["status"] = "ended"
                    logger.info(f"ðŸ›‘ Session ended for room {room_name}: {session_id}")
                    break
                    
            return {"status": "success", "session_id": session_id}
            
        except Exception as e:
            logger.error(f"Error handling session end webhook: {e}")
            return {"status": "error", "message": str(e)}
    
    def get_room_context(self, room_name: str) -> Dict[str, Any]:
        """Get tenant context for a specific room"""
        return self.active_sessions.get(room_name, {})

# Global webhook handler instance
webhook_handler = WebhookHandler()

async def handle_webhook(request):
    """Main webhook endpoint handler"""
    try:
        # Validate webhook secret if configured
        if WEBHOOK_SECRET:
            webhook_signature = request.headers.get("X-Supabase-Signature", "")
            # TODO: Implement proper signature validation
            
        # Parse webhook payload
        payload = await request.json()
        webhook_type = payload.get("type")
        table = payload.get("table")
        
        logger.info(f"ðŸ“¨ Received webhook: type={webhook_type}, table={table}")
        
        # Route to appropriate handler
        result = {"status": "error", "message": "Unknown webhook type"}
        
        if table == "rooms":
            if webhook_type == "INSERT":
                result = await webhook_handler.handle_room_created(payload)
            elif webhook_type == "DELETE":
                result = await webhook_handler.handle_room_deleted(payload)
                
        elif table == "room_sessions":
            if webhook_type == "INSERT":
                result = await webhook_handler.handle_session_started(payload)
            elif webhook_type == "UPDATE":
                # Check if session is ending
                if payload.get("record", {}).get("ended_at"):
                    result = await webhook_handler.handle_session_ended(payload)
                    
        return web.json_response(result)
        
    except json.JSONDecodeError:
        return web.json_response({"status": "error", "message": "Invalid JSON"}, status=400)
    except Exception as e:
        logger.error(f"Error processing webhook: {e}")
        return web.json_response({"status": "error", "message": str(e)}, status=500)

async def handle_status(request):
    """Status endpoint to check webhook handler health"""
    return web.json_response({
        "status": "healthy",
        "active_sessions": len(webhook_handler.active_sessions),
        "sessions": list(webhook_handler.active_sessions.keys())
    })

async def start_webhook_server():
    """Start the webhook server"""
    app = web.Application()
    
    # Add routes
    app.router.add_post('/webhook', handle_webhook)
    app.router.add_get('/status', handle_status)
    
    # Add CORS middleware
    @web.middleware
    async def cors_middleware(request, handler):
        if request.method == 'OPTIONS':
            return web.Response(headers={
                'Access-Control-Allow-Origin': '*',
                'Access-Control-Allow-Methods': 'POST, GET, OPTIONS',
                'Access-Control-Allow-Headers': 'Content-Type, X-Supabase-Signature',
            })
        response = await handler(request)
        response.headers['Access-Control-Allow-Origin'] = '*'
        return response
    
    app.middlewares.append(cors_middleware)
    
    # Start server
    runner = web.AppRunner(app)
    await runner.setup()
    site = web.TCPSite(runner, '0.0.0.0', 8767)
    await site.start()
    
    logger.info("ðŸš€ Webhook server started on http://0.0.0.0:8767")
    logger.info("ðŸ“¨ Webhook endpoint: POST http://0.0.0.0:8767/webhook")
    logger.info("ðŸ“Š Status endpoint: GET http://0.0.0.0:8767/status")
    
    try:
        await asyncio.Future()  # Run forever
    except KeyboardInterrupt:
        logger.info("Shutting down webhook server...")
        await runner.cleanup()

# Export the handler for use in main.py
def get_room_context(room_name: str) -> Dict[str, Any]:
    """Get tenant context for a room from webhook handler"""
    return webhook_handler.get_room_context(room_name)

if __name__ == "__main__":
    try:
        asyncio.run(start_webhook_server())
    except KeyboardInterrupt:
        logger.info("Webhook server stopped by user")
    except Exception as e:
        logger.error(f"Webhook server error: {e}")


================================================
FILE: .env.example
================================================
# LiveKit Configuration
LIVEKIT_URL=wss://your-livekit-server.livekit.cloud
LIVEKIT_API_KEY=your_livekit_api_key
LIVEKIT_API_SECRET=your_livekit_api_secret

# OpenAI Configuration
OPENAI_API_KEY=your_openai_api_key

# Speechmatics Configuration (Optional)
SPEECHMATICS_API_KEY=your_speechmatics_api_key

# Supabase Configuration
SUPABASE_URL=https://bpsahvbdlkzemwjdgxmq.supabase.co
SUPABASE_SERVICE_ROLE_KEY=your_supabase_service_role_key
SUPABASE_WEBHOOK_SECRET=your_webhook_secret_for_validation

# WebSocket Configuration
WEBSOCKET_BRIDGE_URL=wss://bpsahvbdlkzemwjdgxmq.functions.supabase.co/functions/v1/websocket-bridge
WEBSOCKET_SERVER_PORT=8765

# Logging Level
LOG_LEVEL=INFO


================================================
FILE: backup_20250723_021521/broadcasting.py
================================================
"""
Broadcasting module for LiveKit AI Translation Server.
Handles real-time broadcasting of transcriptions and translations to displays.
"""
import asyncio
import logging
import hashlib
from typing import Optional, Dict, Any
from datetime import datetime

from config import get_config
from database import broadcast_to_channel, store_transcript_in_database

logger = logging.getLogger("transcriber.broadcasting")
config = get_config()


class BroadcastError(Exception):
    """Custom exception for broadcasting-related errors."""
    pass


async def broadcast_to_displays(
    message_type: str, 
    language: str, 
    text: str, 
    tenant_context: Optional[Dict[str, Any]] = None
) -> bool:
    """
    Send transcription/translation to frontend via Supabase Broadcast and store in database.
    
    This function handles both real-time broadcasting and database storage of
    transcriptions and translations. It uses Supabase's broadcast feature for
    real-time updates and stores the data for persistence.
    
    Args:
        message_type: Type of message ("transcription" or "translation")
        language: Language code (e.g., "ar", "nl")
        text: The text content to broadcast
        tenant_context: Optional context containing room_id, mosque_id, etc.
        
    Returns:
        bool: True if broadcast was successful, False otherwise
    """
    if not text or not text.strip():
        logger.debug("Empty text provided, skipping broadcast")
        return False
    
    success = False
    
    # Phase 1: Immediate broadcast via Supabase for real-time display
    if tenant_context and tenant_context.get("room_id") and tenant_context.get("mosque_id"):
        try:
            channel_name = f"live-transcription-{tenant_context['room_id']}-{tenant_context['mosque_id']}"
            
            # Generate unique message ID based on timestamp and content hash
            timestamp = datetime.utcnow().isoformat() + "Z"
            text_hash = hashlib.md5(text.encode('utf-8')).hexdigest()[:8]
            msg_id = f"{timestamp}_{text_hash}"
            
            payload = {
                "type": message_type,
                "room_id": tenant_context["room_id"],
                "mosque_id": tenant_context["mosque_id"],
                "data": {
                    "text": text,
                    "language": language,
                    "timestamp": timestamp,
                    "msg_id": msg_id
                }
            }
            
            # Use the broadcast_to_channel function from database module
            success = await broadcast_to_channel(channel_name, message_type, payload)
            
            if success:
                logger.info(
                    f"ðŸ“¡ LIVE: Sent {message_type} ({language}) via Supabase broadcast: "
                    f"{text[:50]}{'...' if len(text) > 50 else ''}"
                )
            else:
                logger.warning(f"âš ï¸ Failed to broadcast {message_type} to Supabase")
                
        except Exception as e:
            logger.error(f"âŒ Broadcast error: {e}")
            success = False
    else:
        logger.warning("âš ï¸ Missing tenant context for Supabase broadcast")
    
    # Phase 2: Direct database storage (no batching)
    if tenant_context and tenant_context.get("room_id") and tenant_context.get("mosque_id"):
        try:
            # Store directly in database using existing function
            # Use create_task to avoid blocking the broadcast with proper error handling
            task = asyncio.create_task(
                _store_with_error_handling(message_type, language, text, tenant_context)
            )
            task.add_done_callback(lambda t: None if not t.exception() else logger.error(f"Storage task failed: {t.exception()}"))
            logger.debug(
                f"ðŸ’¾ DIRECT: Storing {message_type} directly to database "
                f"for room {tenant_context['room_id']}"
            )
        except Exception as e:
            logger.error(f"âŒ Failed to initiate database storage: {e}")
    else:
        logger.warning("âš ï¸ Missing tenant context for database storage")
    
    return success


async def _store_with_error_handling(
    message_type: str, 
    language: str, 
    text: str, 
    tenant_context: Dict[str, Any]
) -> None:
    """
    Store transcript with proper error handling.
    
    This is a wrapper around store_transcript_in_database that ensures
    errors don't propagate and crash the application.
    
    Args:
        message_type: Type of message ("transcription" or "translation")
        language: Language code
        text: The text content to store
        tenant_context: Context containing room_id, mosque_id, etc.
    """
    try:
        success = await store_transcript_in_database(
            message_type, language, text, tenant_context
        )
        if not success:
            logger.warning(
                f"âš ï¸ Failed to store {message_type} in database for "
                f"room {tenant_context.get('room_id')}"
            )
    except Exception as e:
        logger.error(
            f"âŒ Database storage error for {message_type}: {e}\n"
            f"Room: {tenant_context.get('room_id')}, "
            f"Language: {language}"
        )


async def broadcast_batch(
    messages: list[tuple[str, str, str, Dict[str, Any]]]
) -> Dict[str, int]:
    """
    Broadcast multiple messages in batch for efficiency.
    
    Args:
        messages: List of tuples (message_type, language, text, tenant_context)
        
    Returns:
        Dictionary with counts of successful and failed broadcasts
    """
    results = {"success": 0, "failed": 0}
    
    # Process all broadcasts concurrently
    tasks = []
    for message_type, language, text, tenant_context in messages:
        task = broadcast_to_displays(message_type, language, text, tenant_context)
        tasks.append(task)
    
    # Wait for all broadcasts to complete
    broadcast_results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # Count results
    for result in broadcast_results:
        if isinstance(result, Exception):
            results["failed"] += 1
            logger.error(f"Batch broadcast error: {result}")
        elif result:
            results["success"] += 1
        else:
            results["failed"] += 1
    
    logger.info(
        f"ðŸ“Š Batch broadcast complete: "
        f"{results['success']} successful, {results['failed']} failed"
    )
    
    return results


def create_broadcast_payload(
    message_type: str,
    language: str,
    text: str,
    room_id: int,
    mosque_id: int,
    additional_data: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """
    Create a standardized broadcast payload.
    
    Args:
        message_type: Type of message
        language: Language code
        text: The text content
        room_id: Room ID
        mosque_id: Mosque ID
        additional_data: Optional additional data to include
        
    Returns:
        Formatted payload dictionary
    """
    # Generate unique message ID
    timestamp = datetime.utcnow().isoformat() + "Z"
    text_hash = hashlib.md5(text.encode('utf-8')).hexdigest()[:8]
    msg_id = f"{timestamp}_{text_hash}"
    
    payload = {
        "type": message_type,
        "room_id": room_id,
        "mosque_id": mosque_id,
        "data": {
            "text": text,
            "language": language,
            "timestamp": timestamp,
            "msg_id": msg_id
        }
    }
    
    if additional_data:
        payload["data"].update(additional_data)
    
    return payload


def get_channel_name(room_id: int, mosque_id: int) -> str:
    """
    Generate the channel name for a room.
    
    Args:
        room_id: Room ID
        mosque_id: Mosque ID
        
    Returns:
        Channel name string
    """
    return f"live-transcription-{room_id}-{mosque_id}"


================================================
FILE: backup_20250723_021521/config.py
================================================
"""
Configuration management for LiveKit AI Translation Server.
Centralizes all configuration values and environment variables.
"""
import os
from dataclasses import dataclass
from typing import Optional, Dict
# Try to load dotenv if available
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    # dotenv not available, will use system environment variables
    pass


@dataclass
class SupabaseConfig:
    """Supabase database configuration."""
    url: str
    service_role_key: str
    anon_key: Optional[str] = None
    
    # Timeouts
    http_timeout: float = 5.0  # General HTTP request timeout
    broadcast_timeout: float = 2.0  # Broadcast API timeout
    
    @classmethod
    def from_env(cls) -> 'SupabaseConfig':
        """Load configuration from environment variables."""
        url = os.getenv('SUPABASE_URL')
        service_key = os.getenv('SUPABASE_SERVICE_ROLE_KEY')
        anon_key = os.getenv('SUPABASE_ANON_KEY')
        
        if not url:
            raise ValueError("SUPABASE_URL environment variable is required")
        if not service_key:
            raise ValueError("SUPABASE_SERVICE_ROLE_KEY environment variable is required")
            
        return cls(
            url=url,
            service_role_key=service_key,
            anon_key=anon_key
        )


@dataclass
class TranslationConfig:
    """Translation-related configuration."""
    # Language settings
    default_source_language: str = "ar"  # Arabic
    default_target_language: str = "nl"  # Dutch
    
    # Context window settings
    use_context: bool = True
    max_context_pairs: int = 6  # Number of translation pairs to keep in memory
    
    # Timing settings
    translation_delay: float = 10.0  # Delay before translating incomplete sentences
    
    # Supported languages
    supported_languages: Dict[str, Dict[str, str]] = None
    
    def __post_init__(self):
        if self.supported_languages is None:
            self.supported_languages = {
                "ar": {"name": "Arabic", "flag": "ðŸ‡¸ðŸ‡¦"},
                "en": {"name": "English", "flag": "ðŸ‡ºðŸ‡¸"},
                "es": {"name": "Spanish", "flag": "ðŸ‡ªðŸ‡¸"},
                "fr": {"name": "French", "flag": "ðŸ‡«ðŸ‡·"},
                "de": {"name": "German", "flag": "ðŸ‡©ðŸ‡ª"},
                "ja": {"name": "Japanese", "flag": "ðŸ‡¯ðŸ‡µ"},
                "nl": {"name": "Dutch", "flag": "ðŸ‡³ðŸ‡±"},
            }
    
    def get_target_language(self, room_config: Optional[Dict[str, any]] = None) -> str:
        """Get target language from room config or use default."""
        if room_config:
            # Check both possible field names (translation_language and translation__language)
            if 'translation_language' in room_config and room_config['translation_language']:
                return room_config['translation_language']
            elif 'translation__language' in room_config and room_config['translation__language']:
                return room_config['translation__language']
        return self.default_target_language
    
    def get_source_language(self, room_config: Optional[Dict[str, any]] = None) -> str:
        """Get source language from room config or use default."""
        if room_config and 'transcription_language' in room_config and room_config['transcription_language']:
            return room_config['transcription_language']
        return self.default_source_language


@dataclass
class SpeechmaticsConfig:
    """Speechmatics STT configuration."""
    language: str = "ar"
    operating_point: str = "enhanced"
    enable_partials: bool = True
    max_delay: float = 2.0
    punctuation_sensitivity: float = 0.5
    diarization: str = "speaker"
    
    def with_room_settings(self, room_config: Optional[Dict[str, any]] = None) -> 'SpeechmaticsConfig':
        """Create a new config with room-specific overrides."""
        if not room_config:
            return self
            
        # Create a copy with room-specific overrides
        import copy
        new_config = copy.deepcopy(self)
        
        # Override with room settings if available
        if 'transcription_language' in room_config and room_config['transcription_language']:
            new_config.language = room_config['transcription_language']
        if 'max_delay' in room_config and room_config['max_delay'] is not None:
            new_config.max_delay = float(room_config['max_delay'])
        if 'punctuation_sensitivity' in room_config and room_config['punctuation_sensitivity'] is not None:
            new_config.punctuation_sensitivity = float(room_config['punctuation_sensitivity'])
            
        return new_config


@dataclass
class ApplicationConfig:
    """Main application configuration."""
    # Component configurations
    supabase: SupabaseConfig
    translation: TranslationConfig
    speechmatics: SpeechmaticsConfig
    
    # Logging
    log_level: str = "INFO"
    
    # Testing/Development
    default_mosque_id: int = 1
    test_mosque_id: int = 546012  # Hardcoded test mosque
    test_room_id: int = 192577    # Hardcoded test room
    
    @classmethod
    def load(cls) -> 'ApplicationConfig':
        """Load complete configuration from environment and defaults."""
        return cls(
            supabase=SupabaseConfig.from_env(),
            translation=TranslationConfig(),
            speechmatics=SpeechmaticsConfig()
        )
    
    def validate(self) -> None:
        """Validate configuration at startup."""
        # Print configuration status
        print("ðŸ”§ Configuration loaded:")
        print(f"   SUPABASE_URL: {self.supabase.url[:50]}...")
        print(f"   SERVICE_KEY: {'âœ… SET' if self.supabase.service_role_key else 'âŒ NOT SET'}")
        print(f"   Default Languages: {self.translation.default_source_language} â†’ {self.translation.default_target_language}")
        print(f"   Context Window: {'âœ… ENABLED' if self.translation.use_context else 'âŒ DISABLED'} ({self.translation.max_context_pairs} pairs)")
        print(f"   STT Defaults: delay={self.speechmatics.max_delay}s, punctuation={self.speechmatics.punctuation_sensitivity}")


# Global configuration instance
_config: Optional[ApplicationConfig] = None


def get_config() -> ApplicationConfig:
    """Get or create the global configuration instance."""
    global _config
    if _config is None:
        _config = ApplicationConfig.load()
        _config.validate()
    return _config


def reset_config() -> None:
    """Reset configuration (mainly for testing)."""
    global _config
    _config = None


================================================
FILE: backup_20250723_021521/database.py
================================================
"""
Database operations for LiveKit AI Translation Server.
Handles all Supabase database interactions with connection pooling and async support.
FIXED: Thread-safe connection pool that works with LiveKit's multi-process architecture.
"""
import asyncio
import logging
import uuid
from typing import Optional, Dict, Any
from datetime import datetime
import aiohttp
from contextlib import asynccontextmanager
import threading

from config import get_config

logger = logging.getLogger("transcriber.database")
config = get_config()


class ThreadSafeDatabasePool:
    """Thread-safe database connection pool that creates separate pools per thread/process."""
    
    def __init__(self, max_connections: int = 10):
        self.max_connections = max_connections
        self._local = threading.local()
        self._lock = threading.Lock()
        
    async def get_session(self) -> aiohttp.ClientSession:
        """Get or create a session for the current thread."""
        # Check if current thread has a session
        if not hasattr(self._local, 'session') or self._local.session is None or self._local.session.closed:
            # Create new session for this thread
            connector = aiohttp.TCPConnector(
                limit=self.max_connections,
                limit_per_host=self.max_connections,
                force_close=True  # Force close to avoid connection issues
            )
            self._local.session = aiohttp.ClientSession(
                connector=connector,
                trust_env=True  # Trust environment proxy settings
            )
            logger.debug(f"Created new connection pool for thread {threading.current_thread().ident}")
        
        return self._local.session
    
    async def close(self):
        """Close the session for current thread."""
        if hasattr(self._local, 'session') and self._local.session and not self._local.session.closed:
            await self._local.session.close()
            self._local.session = None
            logger.debug(f"Closed connection pool for thread {threading.current_thread().ident}")


# Use thread-safe pool
_pool = ThreadSafeDatabasePool()


@asynccontextmanager
async def get_db_headers():
    """Get headers for Supabase API requests."""
    if not config.supabase.service_role_key:
        raise ValueError("SUPABASE_SERVICE_ROLE_KEY not configured")
    
    yield {
        'apikey': config.supabase.service_role_key,
        'Authorization': f'Bearer {config.supabase.service_role_key}',
        'Content-Type': 'application/json'
    }


async def ensure_active_session(room_id: int, mosque_id: int) -> Optional[str]:
    """
    Ensure there's an active session for the room and return session_id.
    
    This function:
    1. Checks for existing active sessions
    2. Creates a new session if none exists
    3. Returns the session ID or None on failure
    """
    try:
        # Get session from thread-safe pool
        session = await _pool.get_session()
        
        async with get_db_headers() as headers:
            # Check for existing active session
            url = f"{config.supabase.url}/rest/v1/room_sessions"
            params = {
                "room_id": f"eq.{room_id}",
                "status": "eq.active",
                "select": "id,started_at",
                "order": "started_at.desc",
                "limit": "1"
            }
            
            timeout = aiohttp.ClientTimeout(total=config.supabase.http_timeout)
            
            try:
                async with session.get(url, headers=headers, params=params, timeout=timeout) as response:
                    if response.status == 200:
                        sessions = await response.json()
                        if sessions and len(sessions) > 0:
                            session_id = sessions[0]["id"]
                            logger.debug(f"ðŸ“ Using existing active session: {session_id}")
                            return session_id
                    else:
                        error_text = await response.text()
                        logger.warning(f"Failed to check existing sessions: {response.status} - {error_text}")
            except asyncio.TimeoutError:
                logger.warning("Timeout checking for existing sessions")
            except Exception as e:
                logger.error(f"Error checking sessions: {e}")
            
            # Create new session if none exists
            new_session_id = str(uuid.uuid4())
            session_data = {
                "id": new_session_id,
                "room_id": room_id,
                "mosque_id": mosque_id,
                "status": "active",
                "started_at": datetime.utcnow().isoformat() + "Z",
                "logging_enabled": True
            }
            
            try:
                async with session.post(
                    url,
                    json=session_data,
                    headers={**headers, 'Prefer': 'return=minimal'},
                    timeout=timeout
                ) as response:
                    if response.status in [200, 201]:
                        logger.info(f"ðŸ“ Created new session: {new_session_id}")
                        return new_session_id
                    else:
                        error_text = await response.text()
                        logger.error(f"âŒ Failed to create session: {response.status} - {error_text}")
                        return None
            except asyncio.TimeoutError:
                logger.error("Timeout creating new session")
                return None
            except Exception as e:
                logger.error(f"Error creating session: {e}")
                return None
                    
    except Exception as e:
        logger.error(f"âŒ Session management failed: {e}")
        return None


async def store_transcript_in_database(
    message_type: str, 
    language: str, 
    text: str, 
    tenant_context: Dict[str, Any]
) -> bool:
    """
    Store transcription/translation in Supabase database.
    
    Args:
        message_type: Either "transcription" or "translation"
        language: Language code (e.g., "ar", "nl")
        text: The text to store
        tenant_context: Context containing room_id, mosque_id, session_id
        
    Returns:
        bool: True if successful, False otherwise
    """
    try:
        if not config.supabase.service_role_key:
            logger.error("âŒ SUPABASE_SERVICE_ROLE_KEY not found - cannot store transcripts")
            return False
            
        room_id = tenant_context.get("room_id")
        mosque_id = tenant_context.get("mosque_id")
        session_id = tenant_context.get("session_id")
        
        if not room_id or not mosque_id:
            logger.warning(f"âš ï¸ Missing room context: room_id={room_id}, mosque_id={mosque_id}")
            return False
            
        # Ensure we have an active session
        if not session_id:
            session_id = await ensure_active_session(room_id, mosque_id)
            if session_id:
                tenant_context["session_id"] = session_id
            else:
                logger.error("âŒ Could not establish session - skipping database storage")
                return False
        
        # Prepare transcript data
        transcript_data = {
            "room_id": room_id,
            "session_id": session_id,
            "timestamp": datetime.utcnow().isoformat() + "Z",
        }
        
        # Set appropriate field based on message type
        if message_type == "transcription":
            transcript_data["transcription_segment"] = text
        else:  # translation
            transcript_data["translation_segment"] = text
            
        # Store in database
        session = await _pool.get_session()
        async with get_db_headers() as headers:
            timeout = aiohttp.ClientTimeout(total=config.supabase.http_timeout)
            
            try:
                async with session.post(
                    f"{config.supabase.url}/rest/v1/transcripts",
                    json=transcript_data,
                    headers={**headers, 'Prefer': 'return=minimal'},
                    timeout=timeout
                ) as response:
                    if response.status in [200, 201]:
                        logger.debug(f"âœ… Stored {message_type} in database: room_id={room_id}, session_id={session_id[:8]}")
                        return True
                    else:
                        error_text = await response.text()
                        logger.warning(f"âš ï¸ Database storage failed with status {response.status}: {error_text}")
                        return False
            except asyncio.TimeoutError:
                logger.warning("Timeout storing transcript")
                return False
            except Exception as e:
                logger.error(f"Error storing transcript: {e}")
                return False
                    
    except Exception as e:
        logger.error(f"âŒ Database storage error: {e}")
        return False


async def query_room_by_name(room_name: str) -> Optional[Dict[str, Any]]:
    """
    Query room information by LiveKit room name.
    
    Args:
        room_name: The LiveKit room name
        
    Returns:
        Room data dictionary or None if not found
    """
    try:
        session = await _pool.get_session()
        async with get_db_headers() as headers:
            url = f"{config.supabase.url}/rest/v1/rooms"
            params = {"Livekit_room_name": f"eq.{room_name}"}
            
            timeout = aiohttp.ClientTimeout(total=config.supabase.http_timeout)
            
            try:
                async with session.get(url, headers=headers, params=params, timeout=timeout) as response:
                    if response.status == 200:
                        rooms = await response.json()
                        if rooms and len(rooms) > 0:
                            return rooms[0]
                    else:
                        error_text = await response.text()
                        logger.warning(f"Failed to query room: {response.status} - {error_text}")
            except asyncio.TimeoutError:
                logger.warning("Timeout querying room")
            except Exception as e:
                logger.error(f"Error querying room: {e}")
        
        return None
        
    except Exception as e:
        logger.error(f"âŒ Room query failed: {e}")
        return None


async def get_active_session_for_room(room_id: int) -> Optional[str]:
    """
    Get the active session ID for a room if one exists.
    
    Args:
        room_id: The room ID
        
    Returns:
        Session ID or None if no active session
    """
    try:
        session = await _pool.get_session()
        async with get_db_headers() as headers:
            url = f"{config.supabase.url}/rest/v1/room_sessions"
            params = {
                "room_id": f"eq.{room_id}",
                "status": "eq.active",
                "select": "id",
                "order": "started_at.desc",
                "limit": "1"
            }
            
            timeout = aiohttp.ClientTimeout(total=config.supabase.http_timeout)
            
            try:
                async with session.get(url, headers=headers, params=params, timeout=timeout) as response:
                    if response.status == 200:
                        sessions = await response.json()
                        if sessions and len(sessions) > 0:
                            return sessions[0].get("id")
            except asyncio.TimeoutError:
                logger.warning("Timeout getting active session")
            except Exception as e:
                logger.error(f"Error getting active session: {e}")
        
        return None
        
    except Exception as e:
        logger.error(f"âŒ Active session query failed: {e}")
        return None


async def broadcast_to_channel(
    channel_name: str,
    event_type: str,
    payload: Dict[str, Any]
) -> bool:
    """
    Broadcast a message to a Supabase channel.
    
    Args:
        channel_name: The channel to broadcast to
        event_type: The event type (e.g., "transcription", "translation")
        payload: The data to broadcast
        
    Returns:
        bool: True if successful, False otherwise
    """
    try:
        if not config.supabase.service_role_key:
            logger.warning("âš ï¸ SUPABASE_SERVICE_ROLE_KEY not found - skipping broadcast")
            return False
            
        session = await _pool.get_session()
        
        async with get_db_headers() as headers:
            # Use broadcast-specific timeout
            broadcast_timeout = aiohttp.ClientTimeout(total=config.supabase.broadcast_timeout)
            
            try:
                async with session.post(
                    f"{config.supabase.url}/functions/v1/broadcast",
                    json={
                        "channel": channel_name,
                        "event": event_type,
                        "payload": payload
                    },
                    headers=headers,
                    timeout=broadcast_timeout
                ) as response:
                    if response.status == 200:
                        return True
                    else:
                        error_text = await response.text()
                        logger.warning(f"âš ï¸ Broadcast failed: {response.status} - {error_text}")
                        return False
            except asyncio.TimeoutError:
                logger.warning(f"âš ï¸ Broadcast timeout for channel {channel_name}")
                return False
            except Exception as e:
                logger.error(f"Error broadcasting: {e}")
                return False
                    
    except Exception as e:
        logger.error(f"âŒ Broadcast error: {e}")
        return False


async def query_prompt_template_for_room(room_id: int) -> Optional[Dict[str, Any]]:
    """
    Query the prompt template for a specific room.
    
    Args:
        room_id: The room ID
        
    Returns:
        Template data dictionary or None if not found
    """
    try:
        session = await _pool.get_session()
        async with get_db_headers() as headers:
            # Use the database function to get the appropriate template
            url = f"{config.supabase.url}/rest/v1/rpc/get_room_prompt_template"
            data = {"room_id": room_id}
            
            timeout = aiohttp.ClientTimeout(total=config.supabase.http_timeout)
            
            try:
                async with session.post(url, headers=headers, json=data, timeout=timeout) as response:
                    if response.status == 200:
                        result = await response.json()
                        if result and len(result) > 0:
                            template = result[0]
                            # Parse template_variables if it's a string
                            if isinstance(template.get('template_variables'), str):
                                try:
                                    import json
                                    template['template_variables'] = json.loads(template['template_variables'])
                                except:
                                    template['template_variables'] = {}
                            return template
                    else:
                        error_text = await response.text()
                        logger.warning(f"Failed to query prompt template: {response.status} - {error_text}")
            except asyncio.TimeoutError:
                logger.warning("Timeout querying prompt template")
            except Exception as e:
                logger.error(f"Error querying prompt template: {e}")
        
        return None
        
    except Exception as e:
        logger.error(f"âŒ Prompt template query failed: {e}")
        return None


async def close_database_connections():
    """Close all database connections. Call this on shutdown."""
    await _pool.close()
    logger.info("âœ… Database connections closed")


================================================
FILE: backup_20250723_021521/Dockerfile
================================================
# Use Python 3.10 as base image
FROM python:3.10-slim

# Set working directory
WORKDIR /app

# Install system dependencies for audio processing and LiveKit
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    portaudio19-dev \
    python3-dev \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements and install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir --root-user-action=ignore -r requirements.txt

# Copy application code
COPY . .

# Create non-root user for security
RUN useradd -m -u 1000 agent && chown -R agent:agent /app
USER agent

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
  CMD python -c "import asyncio; import sys; print('Agent is running')" || exit 1

# Default command to run the agent in production mode
CMD ["python", "main_production.py", "start"] 


================================================
FILE: backup_20250723_021521/main.py
================================================
import asyncio
import logging
import json
import time
import re
import os
from typing import Set, Any, Dict, Optional
from collections import defaultdict, deque
from datetime import datetime, timedelta

from enum import Enum
from dataclasses import dataclass, asdict

from livekit import rtc
from livekit.agents import (
    AutoSubscribe,
    JobContext,
    JobProcess,
    JobRequest,
    WorkerOptions,
    cli,
    stt,
    utils,
)
from livekit.plugins import silero, speechmatics
from livekit.plugins.speechmatics.types import TranscriptionConfig

# Import configuration
from config import get_config, ApplicationConfig

# Import database operations
from database import (
    ensure_active_session,
    store_transcript_in_database,
    query_room_by_name,
    get_active_session_for_room,
    broadcast_to_channel,
    close_database_connections
)

# Import text processing and translation helpers
from text_processing import extract_complete_sentences
from translation_helpers import translate_sentences

# Import Translator class
from translator import Translator

# Import broadcasting function
from broadcasting import broadcast_to_displays

# Import resource management
from resource_management import ResourceManager, TaskManager, STTStreamManager

# Import webhook handler for room context
try:
    from webhook_handler import get_room_context as get_webhook_room_context
except ImportError:
    # Webhook handler not available, use empty context
    def get_webhook_room_context(room_name: str):
        return {}


# Load configuration
config = get_config()

logger = logging.getLogger("transcriber")


@dataclass
class Language:
    code: str
    name: str
    flag: str


# Build languages dictionary from config
languages = {}
for code, lang_info in config.translation.supported_languages.items():
    languages[code] = Language(
        code=code,
        name=lang_info["name"],
        flag=lang_info["flag"]
    )

LanguageCode = Enum(
    "LanguageCode",  # Name of the Enum
    {lang.name: code for code, lang in languages.items()},  # Enum entries: name -> code mapping
)


# Translator class has been moved to translator.py


def prewarm(proc: JobProcess):
    proc.userdata["vad"] = silero.VAD.load()


async def entrypoint(job: JobContext):
    # Configure source language - ARABIC as default
    # This will be the language that users are actually speaking (host/speaker language)
    source_language = config.translation.default_source_language
    
    # Initialize resource manager
    resource_manager = ResourceManager()
    
    # Extract tenant context from room metadata or webhook handler
    tenant_context = {}
    try:
        # Try to query Supabase directly for room information
        if job.room and job.room.name:
            logger.info(f"ðŸ” Looking up room context for: {job.room.name}")
            
            # Check if database is configured
            logger.info(f"ðŸ”‘ Supabase URL: {config.supabase.url}")
            logger.info(f"ðŸ”‘ Supabase key available: {'Yes' if config.supabase.service_role_key else 'No'}")
            
            if config.supabase.service_role_key:
                try:
                    # Query room by LiveKit room name using the new database module
                    logger.info(f"ðŸ” Querying database for room: {job.room.name}")
                    # Query room directly without task wrapper
                    room_data = await query_room_by_name(job.room.name)
                    
                    if room_data:
                        tenant_context = {
                            "room_id": room_data.get("id"),
                            "mosque_id": room_data.get("mosque_id"),
                            "room_title": room_data.get("Title"),
                            "transcription_language": room_data.get("transcription_language", "ar"),
                            "translation_language": room_data.get("translation__language", "nl"),
                            "created_at": room_data.get("created_at")
                        }
                        # Also store the double underscore version for compatibility
                        if room_data.get("translation__language"):
                            tenant_context["translation__language"] = room_data.get("translation__language")
                        
                        logger.info(f"âœ… Found room in database: room_id={tenant_context.get('room_id')}, mosque_id={tenant_context.get('mosque_id')}")
                        logger.info(f"ðŸ—£ï¸ Languages: transcription={tenant_context.get('transcription_language')}, translation={tenant_context.get('translation_language')} (or {tenant_context.get('translation__language')})")
                        
                        # Try to get active session for this room
                        session_id = await get_active_session_for_room(tenant_context['room_id'])
                        if session_id:
                            tenant_context["session_id"] = session_id
                            logger.info(f"ðŸ“ Found active session: {tenant_context['session_id']}")
                except Exception as e:
                    logger.warning(f"âš ï¸ Could not query Supabase: {e}")
        
        # Fallback to webhook handler if available
        if not tenant_context:
            webhook_context = get_webhook_room_context(job.room.name if job.room else "")
            if webhook_context:
                tenant_context = {
                    "room_id": webhook_context.get("room_id"),
                    "mosque_id": webhook_context.get("mosque_id"),
                    "session_id": webhook_context.get("session_id"),
                    "room_title": webhook_context.get("room_title"),
                    "transcription_language": webhook_context.get("transcription_language", "ar"),
                    "translation_language": webhook_context.get("translation_language", "nl"),
                    "created_at": webhook_context.get("created_at")
                }
                logger.info(f"ðŸ¢ Tenant context from webhook handler: mosque_id={tenant_context.get('mosque_id')}, room_id={tenant_context.get('room_id')}")
        
        # Fallback to room metadata if available
        if not tenant_context and job.room and job.room.metadata:
            try:
                metadata = json.loads(job.room.metadata)
                tenant_context = {
                    "room_id": metadata.get("room_id"),
                    "mosque_id": metadata.get("mosque_id"),
                    "session_id": metadata.get("session_id"),
                    "room_title": metadata.get("room_title"),
                    "transcription_language": metadata.get("transcription_language", "ar"),
                    "translation_language": metadata.get("translation_language", "nl"),
                    "created_at": metadata.get("created_at")
                }
                logger.info(f"ðŸ¢ Tenant context from room metadata: mosque_id={tenant_context.get('mosque_id')}, room_id={tenant_context.get('room_id')}")
            except:
                pass
        
        # Final fallback to default context with hardcoded values for testing
        if not tenant_context:
            logger.warning(f"âš ï¸ No tenant context available for room: {job.room.name if job.room else 'unknown'}")
            # TEMPORARY: Use hardcoded values for mosque_546012 rooms
            if job.room and f"mosque_{config.test_mosque_id}" in job.room.name:
                tenant_context = {
                    "room_id": config.test_room_id,
                    "mosque_id": config.test_mosque_id,
                    "session_id": None,
                    "transcription_language": "ar",
                    "translation_language": "nl"
                }
                logger.info(f"ðŸ”§ Using hardcoded tenant context for testing: mosque_id={tenant_context['mosque_id']}, room_id={tenant_context['room_id']}")
            else:
                tenant_context = {
                    "room_id": None,
                    "mosque_id": int(os.getenv('DEFAULT_MOSQUE_ID', str(config.default_mosque_id))),
                    "session_id": None,
                    "transcription_language": "ar",
                    "translation_language": "nl"
                }
    except Exception as e:
        logger.warning(f"âš ï¸ Could not extract tenant context: {e}")
    
    # Configure Speechmatics STT with room-specific settings
    # Use tenant_context which already has room configuration
    room_config = None
    if tenant_context and tenant_context.get('room_id'):
        # We already have the room data in tenant_context from earlier query
        room_config = tenant_context
        logger.info(f"ðŸ“‹ Using room-specific configuration from context: "
                  f"lang={room_config.get('transcription_language', 'ar')}, "
                  f"target={room_config.get('translation_language', 'nl')}, "
                  f"delay={room_config.get('max_delay', 2.0)}, "
                  f"punct={room_config.get('punctuation_sensitivity', 0.5)}")
        
        # If we need full room data and it's not in context, query it
        if not room_config.get('max_delay'):
            try:
                full_room_data = await query_room_by_name(job.room.name if job.room else None)
                if full_room_data:
                    # Merge the full room data with tenant context
                    room_config.update({
                        'max_delay': full_room_data.get('max_delay'),
                        'punctuation_sensitivity': full_room_data.get('punctuation_sensitivity'),
                        'translation__language': full_room_data.get('translation__language')
                    })
                    logger.info(f"ðŸ“‹ Fetched additional room config: delay={room_config.get('max_delay')}, punct={room_config.get('punctuation_sensitivity')}")
            except Exception as e:
                logger.warning(f"Failed to fetch additional room config: {e}")
    
    # Create STT configuration with room-specific overrides
    stt_config = config.speechmatics.with_room_settings(room_config)
    
    # Initialize STT provider with configured settings
    stt_provider = speechmatics.STT(
        transcription_config=TranscriptionConfig(
            language=stt_config.language,
            operating_point=stt_config.operating_point,
            enable_partials=stt_config.enable_partials,
            max_delay=stt_config.max_delay,
            punctuation_overrides={"sensitivity": stt_config.punctuation_sensitivity},
            diarization=stt_config.diarization
        )
    )
    
    # Update source language based on room config
    source_language = config.translation.get_source_language(room_config)
    logger.info(f"ðŸ—£ï¸ STT configured for {languages[source_language].name} speech recognition")
    
    translators = {}
    
    # Get target language from room config or use default
    target_language = config.translation.get_target_language(room_config)
    logger.info(f"ðŸŽ¯ Target language resolved to: '{target_language}' (from room_config: {room_config.get('translation_language') if room_config else 'None'} or {room_config.get('translation__language') if room_config else 'None'})")
    
    # Create translator for the configured target language
    if target_language in languages:
        # Get language enum dynamically
        lang_info = languages[target_language]
        lang_enum = getattr(LanguageCode, lang_info.name)
        translators[target_language] = Translator(job.room, lang_enum, tenant_context, broadcast_to_displays)
        logger.info(f"ðŸ“ Initialized {lang_info.name} translator ({target_language})")
    else:
        logger.warning(f"âš ï¸ Target language '{target_language}' not supported, falling back to Dutch")
        dutch_enum = getattr(LanguageCode, 'Dutch')
        translators["nl"] = Translator(job.room, dutch_enum, tenant_context, broadcast_to_displays)
    
    # Sentence accumulation for proper sentence-by-sentence translation
    accumulated_text = ""  # Accumulates text until we get a complete sentence
    last_final_transcript = ""  # Keep track of the last final transcript to avoid duplicates
    
    logger.info(f"ðŸš€ Starting entrypoint for room: {job.room.name if job.room else 'unknown'}")
    logger.info(f"ðŸ” Translators dict ID: {id(translators)}")
    logger.info(f"ðŸŽ¯ Configuration: {languages[source_language].name} â†’ {languages.get(target_language, languages['nl']).name}")
    logger.info(f"âš™ï¸ STT Settings: delay={stt_config.max_delay}s, punctuation={stt_config.punctuation_sensitivity}")

    async def _forward_transcription(
        stt_stream: stt.SpeechStream,
        track: rtc.Track,
    ):
        """Forward the transcription and log the transcript in the console"""
        nonlocal accumulated_text, last_final_transcript
        
        try:
            async for ev in stt_stream:
                # Log to console for interim (word-by-word)
                if ev.type == stt.SpeechEventType.INTERIM_TRANSCRIPT:
                    print(ev.alternatives[0].text, end="", flush=True)
                    
                    # Publish interim transcription for real-time word-by-word display
                    interim_text = ev.alternatives[0].text.strip()
                    if interim_text:
                        try:
                            interim_segment = rtc.TranscriptionSegment(
                                id=utils.misc.shortuuid("SG_"),
                                text=interim_text,
                                start_time=0,
                                end_time=0,
                                language=source_language,  # Arabic
                                final=False,  # This is interim, not final
                            )
                            interim_transcription = rtc.Transcription(
                                job.room.local_participant.identity, "", [interim_segment]
                            )
                            await job.room.local_participant.publish_transcription(interim_transcription)
                        except Exception as e:
                            logger.debug(f"Failed to publish interim transcription: {str(e)}")
                    
                elif ev.type == stt.SpeechEventType.FINAL_TRANSCRIPT:
                    print("\n")
                    final_text = ev.alternatives[0].text.strip()
                    print(" -> ", final_text)
                    logger.info(f"Final Arabic transcript: {final_text}")

                    if final_text and final_text != last_final_transcript:
                        last_final_transcript = final_text
                        
                        # Publish final transcription for the original language (Arabic)
                        try:
                            final_segment = rtc.TranscriptionSegment(
                                id=utils.misc.shortuuid("SG_"),
                                text=final_text,
                                start_time=0,
                                end_time=0,
                                language=source_language,  # Arabic
                                final=True,
                            )
                            final_transcription = rtc.Transcription(
                                job.room.local_participant.identity, "", [final_segment]
                            )
                            await job.room.local_participant.publish_transcription(final_transcription)
                            
                            logger.info(f"âœ… Published final {languages[source_language].name} transcription: '{final_text}'")
                        except Exception as e:
                            logger.error(f"âŒ Failed to publish final transcription: {str(e)}")
                        
                        # Broadcast final transcription text for real-time display
                        print(f"ðŸ“¡ Broadcasting Arabic text to frontend: '{final_text}'")
                        # Broadcast directly without task wrapper
                        await broadcast_to_displays("transcription", source_language, final_text, tenant_context)
                        
                        # Handle translation logic
                        if translators:
                            # SIMPLE ACCUMULATION LOGIC - ONLY APPEND, NEVER REPLACE
                            if accumulated_text:
                                # ALWAYS append new final transcript to existing accumulated text
                                accumulated_text = accumulated_text.strip() + " " + final_text
                            else:
                                # First transcript - start accumulation
                                accumulated_text = final_text
                            
                            logger.info(f"ðŸ“ Updated accumulated Arabic text: '{accumulated_text}'")
                            
                            # Extract complete sentences from accumulated text
                            complete_sentences, remaining_text = extract_complete_sentences(accumulated_text)
                            
                            # Handle special punctuation completion signal
                            if complete_sentences and complete_sentences[0] == "PUNCTUATION_COMPLETE":
                                if accumulated_text.strip():
                                    # Complete the accumulated sentence with this punctuation
                                    print(f"ðŸ“ PUNCTUATION SIGNAL: Completing accumulated text: '{accumulated_text}'")
                                    
                                    # Translate the completed sentence (don't include the punctuation marker)
                                    await translate_sentences([accumulated_text], translators, source_language)
                                    
                                    # Clear accumulated text as sentence is now complete
                                    accumulated_text = ""
                                    print(f"ðŸ“ Cleared accumulated text after punctuation completion")
                                else:
                                    print(f"âš ï¸ Received punctuation completion signal but no accumulated text")
                            elif complete_sentences:
                                # We have complete sentences - translate them immediately
                                print(f"ðŸŽ¯ Found {len(complete_sentences)} complete Arabic sentences: {complete_sentences}")
                                
                                # Translate complete sentences
                                await translate_sentences(complete_sentences, translators, source_language)
                                
                                # Update accumulated text to only remaining incomplete text
                                accumulated_text = remaining_text
                                print(f"ðŸ“ Updated accumulated Arabic text after sentence extraction: '{accumulated_text}'")
                            
                            # Log remaining incomplete text (no delayed translation)
                            if accumulated_text.strip():
                                logger.info(f"ðŸ“ Incomplete Arabic text remaining: '{accumulated_text}'")
                                # Note: Incomplete text will be translated when the next sentence completes
                        else:
                            logger.warning(f"âš ï¸ No translators available in room {job.room.name}, only {languages[source_language].name} transcription published")
                    else:
                        logger.debug("Empty or duplicate transcription, skipping")
        except Exception as e:
            logger.error(f"STT transcription error: {str(e)}")
            raise

    async def transcribe_track(participant: rtc.RemoteParticipant, track: rtc.Track):
        logger.info(f"ðŸŽ¤ Starting Arabic transcription for participant {participant.identity}, track {track.sid}")
        
        try:
            audio_stream = rtc.AudioStream(track)
            
            # Use context manager for STT stream
            async with resource_manager.stt_manager.create_stream(stt_provider, participant.identity) as stt_stream:
                # Create transcription task with tracking
                stt_task = resource_manager.task_manager.create_task(
                    _forward_transcription(stt_stream, track),
                    name=f"transcribe-{participant.identity}",
                    metadata={"participant": participant.identity, "track": track.sid}
                )
                
                frame_count = 0
                async for ev in audio_stream:
                    frame_count += 1
                    if frame_count % 100 == 0:  # Log every 100 frames to avoid spam
                        logger.debug(f"ðŸ”Š Received audio frame #{frame_count} from {participant.identity}")
                    stt_stream.push_frame(ev.frame)
                    
                logger.warning(f"ðŸ”‡ Audio stream ended for {participant.identity}")
                
                # Cancel the transcription task if still running
                if not stt_task.done():
                    stt_task.cancel()
                    try:
                        await stt_task
                    except asyncio.CancelledError:
                        logger.debug(f"STT task cancelled for {participant.identity}")
                        
        except Exception as e:
            logger.error(f"âŒ Transcription track error for {participant.identity}: {str(e)}")
        
        logger.info(f"ðŸ§¹ Transcription cleanup completed for {participant.identity}")

    @job.room.on("track_subscribed")
    def on_track_subscribed(
        track: rtc.Track,
        publication: rtc.TrackPublication,
        participant: rtc.RemoteParticipant,
    ):
        logger.info(f"ðŸŽµ Track subscribed: {track.kind} from {participant.identity} (track: {track.sid})")
        logger.info(f"Track details - muted: {publication.muted}")
        if track.kind == rtc.TrackKind.KIND_AUDIO:
            logger.info(f"âœ… Adding Arabic transcriber for participant: {participant.identity}")
            resource_manager.task_manager.create_task(
                transcribe_track(participant, track),
                name=f"track-handler-{participant.identity}",
                metadata={"participant": participant.identity, "track": track.sid}
            )
        else:
            logger.info(f"âŒ Ignoring non-audio track: {track.kind}")

    @job.room.on("track_published")
    def on_track_published(publication: rtc.TrackPublication, participant: rtc.RemoteParticipant):
        logger.info(f"ðŸ“¡ Track published: {publication.kind} from {participant.identity} (track: {publication.sid})")
        logger.info(f"Publication details - muted: {publication.muted}")

    @job.room.on("track_unpublished") 
    def on_track_unpublished(publication: rtc.TrackPublication, participant: rtc.RemoteParticipant):
        logger.info(f"ðŸ“¡ Track unpublished: {publication.kind} from {participant.identity}")

    @job.room.on("participant_connected")
    def on_participant_connected(participant: rtc.RemoteParticipant):
        logger.info(f"ðŸ‘¥ Participant connected: {participant.identity}")
        
        # Try to extract metadata from participant if available
        if hasattr(participant, 'metadata') and participant.metadata:
            try:
                participant_metadata = json.loads(participant.metadata)
                if participant_metadata:
                    # Update tenant context with participant metadata
                    tenant_context.update({
                        "room_id": participant_metadata.get("room_id", tenant_context.get("room_id")),
                        "mosque_id": participant_metadata.get("mosque_id", tenant_context.get("mosque_id")),
                        "session_id": participant_metadata.get("session_id", tenant_context.get("session_id")),
                        "room_title": participant_metadata.get("room_title", tenant_context.get("room_title"))
                    })
                    logger.info(f"ðŸ“‹ Updated tenant context from participant metadata: {tenant_context}")
                    
                    # Update all translators with new context
                    for translator in translators.values():
                        translator.tenant_context = tenant_context
            except Exception as e:
                logger.debug(f"Could not parse participant metadata: {e}")

    @job.room.on("participant_disconnected")
    def on_participant_disconnected(participant: rtc.RemoteParticipant):
        logger.info(f"ðŸ‘¥ Participant disconnected: {participant.identity}")
        
        # Resource cleanup is now handled by ResourceManager
        # Log current resource statistics
        resource_manager.log_stats()
        logger.info(f"ðŸ§¹ Participant cleanup completed for {participant.identity}")

    @job.room.on("participant_attributes_changed")
    def on_attributes_changed(
        changed_attributes: dict[str, str], participant: rtc.Participant
    ):
        """
        When participant attributes change, handle new translation requests.
        """
        logger.info(f"ðŸŒ Participant {participant.identity} attributes changed: {changed_attributes}")
        lang = changed_attributes.get("captions_language", None)
        if lang:
            if lang == source_language:
                logger.info(f"âœ… Participant {participant.identity} requested {languages[source_language].name} (source language - Arabic)")
            elif lang in translators:
                logger.info(f"âœ… Participant {participant.identity} requested existing language: {lang}")
                logger.info(f"ðŸ“Š Current translators for this room: {list(translators.keys())}")
            else:
                # Check if the language is supported and different from source language
                if lang in languages:
                    try:
                        # Create a translator for the requested language using the language enum
                        language_obj = languages[lang]
                        language_enum = getattr(LanguageCode, language_obj.name)
                        translators[lang] = Translator(job.room, language_enum, tenant_context, broadcast_to_displays)
                        logger.info(f"ðŸ†• Added translator for ROOM {job.room.name} (requested by {participant.identity}), language: {language_obj.name}")
                        logger.info(f"ðŸ¢ Translator created with tenant context: mosque_id={tenant_context.get('mosque_id')}")
                        logger.info(f"ðŸ“Š Total translators for room {job.room.name}: {len(translators)} -> {list(translators.keys())}")
                        logger.info(f"ðŸ” Translators dict ID: {id(translators)}")
                        
                        # Debug: Verify the translator was actually added
                        if lang in translators:
                            logger.info(f"âœ… Translator verification: {lang} successfully added to room translators")
                        else:
                            logger.error(f"âŒ Translator verification FAILED: {lang} not found in translators dict")
                            
                    except Exception as e:
                        logger.error(f"âŒ Error creating translator for {lang}: {str(e)}")
                else:
                    logger.warning(f"âŒ Unsupported language requested by {participant.identity}: {lang}")
                    logger.info(f"ðŸ’¡ Supported languages: {list(languages.keys())}")
        else:
            logger.debug(f"No caption language change for participant {participant.identity}")

    logger.info("Connecting to room...")
    await job.connect(auto_subscribe=AutoSubscribe.AUDIO_ONLY)
    logger.info(f"Successfully connected to room: {job.room.name}")
    logger.info(f"ðŸ“¡ Real-time transcription data will be sent via Supabase Broadcast")
    
    # Debug room state after connection
    logger.info(f"Room participants: {len(job.room.remote_participants)}")
    for participant in job.room.remote_participants.values():
        logger.info(f"Participant: {participant.identity}")
        logger.info(f"  Audio tracks: {len(participant.track_publications)}")
        for sid, pub in participant.track_publications.items():
            logger.info(f"    Track {sid}: {pub.kind}, muted: {pub.muted}")

    # Also check local participant
    logger.info(f"Local participant: {job.room.local_participant.identity}")
    logger.info(f"Local participant tracks: {len(job.room.local_participant.track_publications)}")

    @job.room.local_participant.register_rpc_method("get/languages")
    async def get_languages(data: rtc.RpcInvocationData):
        languages_list = [asdict(lang) for lang in languages.values()]
        return json.dumps(languages_list)

    @job.room.on("disconnected")
    def on_room_disconnected():
        """Handle room disconnection - cleanup all resources"""
        logger.info("ðŸšª Room disconnected, starting cleanup...")
        
        # Create async task for cleanup
        async def cleanup():
            # Log final resource statistics
            resource_manager.log_stats()
            
            # Shutdown resource manager (cancels all tasks, closes all streams)
            await resource_manager.shutdown()
            
            # Close database connections
            try:
                await close_database_connections()
                logger.info("âœ… Database connections closed")
                
                # Force cleanup of any remaining sessions
                import gc
                gc.collect()  # Force garbage collection
                await asyncio.sleep(0.1)  # Give time for cleanup
            except Exception as e:
                logger.debug(f"Database cleanup error: {e}")
            
            logger.info("âœ… Room cleanup completed")
        
        # Run cleanup in the event loop
        asyncio.create_task(cleanup())


async def request_fnc(req: JobRequest):
    logger.info(f"ðŸŽ¯ Received job request for room: {req.room.name if req.room else 'unknown'}")
    logger.info(f"ðŸ“‹ Request details: job_id={req.id}, room_name={req.room.name if req.room else 'unknown'}")
    await req.accept(
        name="agent",
        identity="agent",
    )
    logger.info(f"âœ… Accepted job request for room: {req.room.name if req.room else 'unknown'}")


if __name__ == "__main__":
    cli.run_app(
        WorkerOptions(
            entrypoint_fnc=entrypoint, prewarm_fnc=prewarm, request_fnc=request_fnc
        )
    )


================================================
FILE: backup_20250723_021521/main_production.py
================================================
#!/usr/bin/env python3
"""
Production-ready LiveKit Agent for Bayaan Translation Service
Optimized for Render deployment as a background worker
"""

import asyncio
import logging
import os
import sys
import signal
from datetime import datetime

# Setup production logging
def setup_production_logging():
    """Configure logging for production."""
    log_level = os.getenv('LOG_LEVEL', 'INFO').upper()
    
    logging.basicConfig(
        level=getattr(logging, log_level),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(sys.stdout)
        ]
    )

def health_check() -> bool:
    """Health check endpoint for Render."""
    try:
        # Basic health check - verify environment variables are set
        required_vars = [
            'LIVEKIT_URL', 'LIVEKIT_API_KEY', 'LIVEKIT_API_SECRET',
            'OPENAI_API_KEY', 'SUPABASE_URL', 'SUPABASE_SERVICE_ROLE_KEY'
        ]
        
        for var in required_vars:
            if not os.getenv(var):
                print(f"Missing required environment variable: {var}")
                return False
        
        print("Health check passed - all required environment variables are set")
        return True
        
    except Exception as e:
        print(f"Health check failed: {e}")
        return False

def main():
    """Main production entry point."""
    # Setup production logging
    setup_production_logging()
    
    logger = logging.getLogger(__name__)
    logger.info("Starting Bayaan LiveKit Agent for production deployment")
    
    # Set production environment
    os.environ['ENVIRONMENT'] = 'production'
    
    try:
        # Import and run the agent directly
        from livekit.agents import WorkerOptions, cli
        from main import entrypoint, prewarm, request_fnc
        
        # Production worker configuration
        worker_opts = WorkerOptions(
            entrypoint_fnc=entrypoint,
            prewarm_fnc=prewarm,
            request_fnc=request_fnc
        )
        
        logger.info("Starting LiveKit CLI with production configuration")
        
        # Run the agent - this handles its own event loop
        cli.run_app(worker_opts)
        
    except Exception as e:
        logger.error(f"Fatal error in production agent: {e}")
        sys.exit(1)

if __name__ == "__main__":
    # Handle different command line arguments
    if len(sys.argv) > 1:
        if sys.argv[1] == "health":
            # Health check endpoint
            is_healthy = health_check()
            sys.exit(0 if is_healthy else 1)
        elif sys.argv[1] == "start":
            # Start the agent
            main()
        else:
            # Default to original main.py behavior
            from main import *
            # Run with original CLI
            from livekit.agents import cli, WorkerOptions
            cli.run_app(WorkerOptions(
                entrypoint_fnc=entrypoint,
                prewarm_fnc=prewarm,
                request_fnc=request_fnc
            ))
    else:
        # Run the main agent
        main() 


================================================
FILE: backup_20250723_021521/prompt_builder.py
================================================
"""
Prompt Builder for customizable translation prompts.
Handles loading and formatting of prompt templates with variable substitution.
"""
import logging
from typing import Dict, Optional, Any
import json

from config import get_config
from database import query_prompt_template_for_room

logger = logging.getLogger("transcriber.prompt_builder")
config = get_config()


class PromptBuilder:
    """
    Builds customized translation prompts based on templates and room configuration.
    """
    
    # Default fallback prompt if no template is found
    DEFAULT_PROMPT = (
        "You are an expert simultaneous interpreter. Your task is to translate from {source_lang} to {target_lang}. "
        "Provide a direct and accurate translation of the user's input. Be concise and use natural-sounding language. "
        "Do not add any additional commentary, explanations, or introductory phrases."
    )
    
    def __init__(self):
        """Initialize the prompt builder."""
        self.cached_templates = {}
        logger.info("ðŸŽ¨ PromptBuilder initialized")
    
    async def get_prompt_for_room(
        self, 
        room_id: Optional[int],
        source_lang: str,
        target_lang: str,
        room_config: Optional[Dict[str, Any]] = None
    ) -> str:
        """
        Get the appropriate prompt for a room with variable substitution.
        
        Args:
            room_id: The room ID to get prompt for
            source_lang: Source language name (e.g., "Arabic")
            target_lang: Target language name (e.g., "Dutch")
            room_config: Optional room configuration with additional context
            
        Returns:
            Formatted prompt string ready for use
        """
        try:
            # Try to get template from database if room_id provided
            template = None
            if room_id:
                template = await self._fetch_template_for_room(room_id)
            
            if template:
                prompt = template['prompt_template']
                variables = template.get('template_variables', {})
                logger.info(f"ðŸ“‹ Using prompt template: {template.get('name', 'Unknown')}")
            else:
                # Use default prompt
                prompt = self.DEFAULT_PROMPT
                variables = {}
                logger.info("ðŸ“‹ Using default prompt template")
            
            # Prepare substitution variables
            substitutions = {
                'source_lang': source_lang,
                'target_lang': target_lang,
                **variables  # Include template-specific variables
            }
            
            # Add room-specific context if available
            if room_config:
                if room_config.get('mosque_name'):
                    substitutions['mosque_name'] = room_config['mosque_name']
                if room_config.get('speaker_role'):
                    substitutions['speaker_role'] = room_config['speaker_role']
            
            # Format the prompt with variables
            formatted_prompt = prompt.format(**substitutions)
            
            # Log the generated prompt for debugging
            logger.debug(f"Generated prompt: {formatted_prompt[:100]}...")
            
            return formatted_prompt
            
        except Exception as e:
            logger.error(f"âŒ Error building prompt: {e}")
            # Fallback to basic default
            return self.DEFAULT_PROMPT.format(
                source_lang=source_lang,
                target_lang=target_lang
            )
    
    async def _fetch_template_for_room(self, room_id: int) -> Optional[Dict[str, Any]]:
        """
        Fetch prompt template for a specific room.
        
        Args:
            room_id: The room ID
            
        Returns:
            Template dictionary or None
        """
        try:
            # Check cache first
            cache_key = f"room_{room_id}"
            if cache_key in self.cached_templates:
                return self.cached_templates[cache_key]
            
            # Fetch from database
            template = await query_prompt_template_for_room(room_id)
            
            if template:
                # Cache for future use (5 minute cache)
                self.cached_templates[cache_key] = template
                
            return template
            
        except Exception as e:
            logger.warning(f"Failed to fetch template for room {room_id}: {e}")
            return None
    
    def build_prompt_with_context(
        self,
        base_prompt: str,
        context_type: str,
        additional_context: Optional[Dict[str, str]] = None
    ) -> str:
        """
        Enhance a prompt with additional context based on content type.
        
        Args:
            base_prompt: The base prompt template
            context_type: Type of content (sermon, announcement, etc.)
            additional_context: Additional context variables
            
        Returns:
            Enhanced prompt with context
        """
        context_additions = {
            'sermon': (
                " Remember this is a religious sermon, so maintain appropriate "
                "reverence and formality. Preserve the spiritual tone."
            ),
            'announcement': (
                " This is a community announcement, so prioritize clarity and "
                "practical information over stylistic concerns."
            ),
            'dua': (
                " This is a prayer or supplication. Maintain the devotional "
                "atmosphere and emotional depth of the original."
            ),
            'lecture': (
                " This is an educational lecture. You may add brief clarifications "
                "in parentheses for complex religious terms if needed."
            )
        }
        
        # Add context-specific guidance
        addition = context_additions.get(context_type, "")
        
        # Add any additional context
        if additional_context:
            for key, value in additional_context.items():
                if value:
                    addition += f" {key}: {value}."
        
        return base_prompt + addition
    
    def get_preserved_terms_for_template(self, template_variables: Dict) -> list:
        """
        Extract list of terms to preserve from template variables.
        
        Args:
            template_variables: Template variables dictionary
            
        Returns:
            List of terms to preserve in original language
        """
        return template_variables.get('preserve_terms', [])


# Global instance
_prompt_builder: Optional[PromptBuilder] = None


def get_prompt_builder() -> PromptBuilder:
    """Get or create the global prompt builder instance."""
    global _prompt_builder
    if _prompt_builder is None:
        _prompt_builder = PromptBuilder()
    return _prompt_builder


================================================
FILE: backup_20250723_021521/render.yaml
================================================
services:
  - type: worker
    name: bayaan-livekit-agent
    env: docker
    dockerfilePath: ./Dockerfile
    plan: starter
    region: oregon
    scaling:
      minInstances: 1
      maxInstances: 3
      targetMemoryPercent: 80
      targetCPUPercent: 80
    envVars:
      - key: ENVIRONMENT
        value: production
      - key: LIVEKIT_URL
        fromSecret: livekit_url
      - key: LIVEKIT_API_KEY
        fromSecret: livekit_api_key
      - key: LIVEKIT_API_SECRET
        fromSecret: livekit_api_secret
      - key: OPENAI_API_KEY
        fromSecret: openai_api_key
      - key: SPEECHMATICS_API_KEY
        fromSecret: speechmatics_api_key
      - key: SUPABASE_URL
        fromSecret: supabase_url
      - key: SUPABASE_SERVICE_ROLE_KEY
        fromSecret: supabase_service_role_key
      - key: SUPABASE_ANON_KEY
        fromSecret: supabase_anon_key
      - key: PYTHONPATH
        value: /app
      - key: AGENT_NAME
        value: bayaan-transcriber
      - key: WORKER_TYPE
        value: background
      - key: LOG_LEVEL
        value: INFO
      - key: PERSISTENT_MODE
        value: "true"
      - key: MAX_WORKERS
        value: "3"
      - key: IDLE_TIMEOUT
        value: "300" 


================================================
FILE: backup_20250723_021521/requirements.txt
================================================
# LiveKit Core Dependencies
livekit-agents>=1.0.0
livekit-plugins-openai>=0.8.0
livekit-plugins-speechmatics>=0.6.0
livekit-plugins-silero>=0.6.0

# AI/ML Dependencies
openai>=1.0.0

# Web Framework (for health checks)
fastapi>=0.104.0
uvicorn[standard]>=0.24.0

# HTTP Client
aiohttp>=3.8.0

# Database
asyncpg>=0.29.0

# Environment & Configuration
python-dotenv>=1.0.0

# Logging & Monitoring
# structlog>=23.0.0  # Removed - using standard logging

# Production Dependencies
gunicorn>=21.0.0
psutil>=5.9.0

# Audio Processing
pyaudio>=0.2.11

# Async utilities
asyncio-throttle>=1.0.0


================================================
FILE: backup_20250723_021521/resource_management.py
================================================
"""
Resource management module for LiveKit AI Translation Server.
Handles tracking and cleanup of async tasks, STT streams, and other resources.
"""
import asyncio
import logging
from typing import Set, List, Dict, Any, Optional, Callable
from contextlib import asynccontextmanager
from dataclasses import dataclass, field
from datetime import datetime
import weakref

logger = logging.getLogger("transcriber.resources")


@dataclass
class ResourceStats:
    """Statistics about managed resources."""
    tasks_created: int = 0
    tasks_completed: int = 0
    tasks_failed: int = 0
    tasks_cancelled: int = 0
    streams_opened: int = 0
    streams_closed: int = 0
    active_tasks: int = 0
    active_streams: int = 0


class TaskManager:
    """
    Manages async tasks with proper tracking and cleanup.
    
    Features:
    - Automatic task tracking
    - Graceful cancellation
    - Resource leak prevention
    - Statistics tracking
    """
    
    def __init__(self, name: str = "default"):
        self.name = name
        self._tasks: Set[asyncio.Task] = set()
        self._task_metadata: Dict[asyncio.Task, Dict[str, Any]] = {}
        self._stats = ResourceStats()
        self._cleanup_interval = 30.0  # seconds
        self._cleanup_task: Optional[asyncio.Task] = None
        self._shutdown = False
        logger.info(f"ðŸ“‹ TaskManager '{self.name}' initialized")
    
    async def __aenter__(self):
        """Context manager entry."""
        self._cleanup_task = asyncio.create_task(self._periodic_cleanup())
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit with cleanup."""
        await self.shutdown()
    
    def create_task(
        self, 
        coro, 
        name: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None
    ) -> asyncio.Task:
        """
        Create and track an async task.
        
        Args:
            coro: Coroutine to run
            name: Optional task name
            metadata: Optional metadata for the task
            
        Returns:
            Created task
        """
        if self._shutdown:
            raise RuntimeError("TaskManager is shutting down")
        
        task = asyncio.create_task(coro, name=name)
        self._tasks.add(task)
        
        if metadata:
            self._task_metadata[task] = metadata
        
        # Add callback to clean up when done
        task.add_done_callback(self._task_done_callback)
        
        self._stats.tasks_created += 1
        self._stats.active_tasks = len(self._tasks)
        
        logger.debug(f"ðŸ“Œ Created task: {name or task.get_name()} (total: {len(self._tasks)})")
        return task
    
    def _task_done_callback(self, task: asyncio.Task):
        """Callback when a task completes."""
        self._tasks.discard(task)
        self._task_metadata.pop(task, None)
        
        try:
            if task.cancelled():
                self._stats.tasks_cancelled += 1
                logger.debug(f"ðŸš« Task cancelled: {task.get_name()}")
            elif task.exception():
                self._stats.tasks_failed += 1
                logger.error(f"âŒ Task failed: {task.get_name()}", exc_info=task.exception())
            else:
                self._stats.tasks_completed += 1
                logger.debug(f"âœ… Task completed: {task.get_name()}")
        except Exception as e:
            logger.debug(f"Error in task callback: {e}")
        
        self._stats.active_tasks = len(self._tasks)
    
    async def _periodic_cleanup(self):
        """Periodically clean up completed tasks."""
        while not self._shutdown:
            try:
                await asyncio.sleep(self._cleanup_interval)
                
                # Clean up any references to completed tasks
                completed = [t for t in self._tasks if t.done()]
                for task in completed:
                    self._tasks.discard(task)
                    self._task_metadata.pop(task, None)
                
                if completed:
                    logger.debug(f"ðŸ§¹ Cleaned up {len(completed)} completed tasks")
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"Error in periodic cleanup: {e}")
    
    async def cancel_all(self, timeout: float = 5.0) -> int:
        """
        Cancel all active tasks.
        
        Args:
            timeout: Maximum time to wait for cancellation
            
        Returns:
            Number of tasks cancelled
        """
        if not self._tasks:
            return 0
        
        tasks_to_cancel = list(self._tasks)
        cancelled_count = 0
        
        logger.info(f"ðŸš« Cancelling {len(tasks_to_cancel)} tasks...")
        
        # Cancel all tasks
        for task in tasks_to_cancel:
            if not task.done():
                task.cancel()
                cancelled_count += 1
        
        # Wait for cancellation with timeout
        if tasks_to_cancel:
            try:
                await asyncio.wait_for(
                    asyncio.gather(*tasks_to_cancel, return_exceptions=True),
                    timeout=timeout
                )
            except asyncio.TimeoutError:
                logger.warning(f"â° Timeout waiting for {len(tasks_to_cancel)} tasks to cancel")
        
        logger.info(f"âœ… Cancelled {cancelled_count} tasks")
        return cancelled_count
    
    async def shutdown(self):
        """Shutdown the task manager and cleanup all resources."""
        if self._shutdown:
            return
        
        self._shutdown = True
        logger.info(f"ðŸ›‘ Shutting down TaskManager '{self.name}'...")
        
        # Cancel cleanup task
        if self._cleanup_task and not self._cleanup_task.done():
            self._cleanup_task.cancel()
            try:
                await self._cleanup_task
            except asyncio.CancelledError:
                pass
        
        # Cancel all managed tasks
        await self.cancel_all()
        
        logger.info(f"âœ… TaskManager '{self.name}' shutdown complete")
    
    def get_stats(self) -> ResourceStats:
        """Get current statistics."""
        return self._stats
    
    def get_active_tasks(self) -> List[asyncio.Task]:
        """Get list of active tasks."""
        return list(self._tasks)


class STTStreamManager:
    """
    Manages STT (Speech-to-Text) streams with proper cleanup.
    """
    
    def __init__(self):
        self._streams: Set[Any] = set()
        self._stream_metadata: Dict[Any, Dict[str, Any]] = {}
        self._stats = ResourceStats()
        logger.info("ðŸŽ¤ STTStreamManager initialized")
    
    @asynccontextmanager
    async def create_stream(self, stt_provider, participant_id: str):
        """
        Create and manage an STT stream.
        
        Args:
            stt_provider: STT provider instance
            participant_id: ID of the participant
            
        Yields:
            STT stream instance
        """
        stream = None
        try:
            # Create stream
            stream = stt_provider.stream()
            self._streams.add(stream)
            self._stream_metadata[stream] = {
                "participant_id": participant_id,
                "created_at": datetime.utcnow()
            }
            self._stats.streams_opened += 1
            self._stats.active_streams = len(self._streams)
            
            logger.info(f"ðŸŽ¤ Created STT stream for {participant_id}")
            yield stream
            
        finally:
            # Cleanup stream
            if stream:
                try:
                    await stream.aclose()
                    logger.info(f"âœ… STT stream closed for {participant_id}")
                except Exception as e:
                    logger.error(f"Error closing STT stream: {e}")
                finally:
                    self._streams.discard(stream)
                    self._stream_metadata.pop(stream, None)
                    self._stats.streams_closed += 1
                    self._stats.active_streams = len(self._streams)
    
    async def close_all(self):
        """Close all active streams."""
        if not self._streams:
            return
        
        logger.info(f"ðŸš« Closing {len(self._streams)} STT streams...")
        
        streams_to_close = list(self._streams)
        for stream in streams_to_close:
            try:
                await stream.aclose()
            except Exception as e:
                logger.error(f"Error closing stream: {e}")
            finally:
                self._streams.discard(stream)
                self._stream_metadata.pop(stream, None)
        
        self._stats.active_streams = 0
        logger.info("âœ… All STT streams closed")
    
    def get_stats(self) -> ResourceStats:
        """Get current statistics."""
        return self._stats


class ResourceManager:
    """
    Central resource manager for the application.
    Coordinates TaskManager and STTStreamManager.
    """
    
    def __init__(self):
        self.task_manager = TaskManager("main")
        self.stt_manager = STTStreamManager()
        self._shutdown_handlers: List[Callable] = []
        logger.info("ðŸ—ï¸ ResourceManager initialized")
    
    async def __aenter__(self):
        """Context manager entry."""
        await self.task_manager.__aenter__()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit with cleanup."""
        await self.shutdown()
    
    def add_shutdown_handler(self, handler: Callable):
        """Add a handler to be called on shutdown."""
        self._shutdown_handlers.append(handler)
    
    async def shutdown(self):
        """Shutdown all managed resources."""
        logger.info("ðŸ›‘ Starting ResourceManager shutdown...")
        
        # Run shutdown handlers
        for handler in self._shutdown_handlers:
            try:
                if asyncio.iscoroutinefunction(handler):
                    await handler()
                else:
                    handler()
            except Exception as e:
                logger.error(f"Error in shutdown handler: {e}")
        
        # Shutdown managers
        await self.task_manager.shutdown()
        await self.stt_manager.close_all()
        
        logger.info("âœ… ResourceManager shutdown complete")
    
    def get_all_stats(self) -> Dict[str, ResourceStats]:
        """Get statistics from all managers."""
        return {
            "tasks": self.task_manager.get_stats(),
            "stt_streams": self.stt_manager.get_stats()
        }
    
    def log_stats(self):
        """Log current resource statistics."""
        stats = self.get_all_stats()
        logger.info(
            f"ðŸ“Š Resource Stats - "
            f"Tasks: {stats['tasks'].active_tasks} active "
            f"({stats['tasks'].tasks_completed} completed, "
            f"{stats['tasks'].tasks_failed} failed, "
            f"{stats['tasks'].tasks_cancelled} cancelled), "
            f"STT Streams: {stats['stt_streams'].active_streams} active"
        )


================================================
FILE: backup_20250723_021521/text_processing.py
================================================
"""
Text processing utilities for LiveKit AI Translation Server.
Handles sentence extraction and text manipulation for Arabic and other languages.
"""
import re
import logging
from typing import Tuple, List

logger = logging.getLogger("transcriber.text_processing")


def extract_complete_sentences(text: str) -> Tuple[List[str], str]:
    """
    Extract complete sentences from text and return them along with remaining incomplete text.
    
    This function is designed to work with Arabic text and handles Arabic punctuation marks.
    It identifies complete sentences based on punctuation and returns both the complete
    sentences and any remaining incomplete text.
    
    Args:
        text: The input text to process
        
    Returns:
        A tuple containing:
        - List of complete sentences
        - Remaining incomplete text
    """
    if not text.strip():
        return [], ""
    
    # Arabic sentence ending punctuation marks
    sentence_endings = ['.', '!', '?', 'ØŸ']  # Including Arabic question mark
    
    complete_sentences = []
    remaining_text = ""
    
    logger.debug(f"ðŸ” Processing text for sentence extraction: '{text}'")
    
    # Check if this is standalone punctuation
    if text.strip() in sentence_endings:
        logger.debug(f"ðŸ“ Detected standalone punctuation: '{text.strip()}'")
        # This is standalone punctuation - signal to complete any accumulated sentence
        return ["PUNCTUATION_COMPLETE"], ""
    
    # Split text into parts ending with punctuation
    # This regex splits on punctuation but keeps the punctuation in the result
    parts = re.split(r'([.!?ØŸ])', text)
    
    current_building = ""
    i = 0
    while i < len(parts):
        part = parts[i].strip()
        if not part:
            i += 1
            continue
            
        if part in sentence_endings:
            # Found punctuation - complete the current sentence
            if current_building.strip():
                complete_sentence = current_building.strip() + part
                complete_sentences.append(complete_sentence)
                logger.debug(f"âœ… Complete sentence found: '{complete_sentence}'")
                current_building = ""
            i += 1
        else:
            # Regular text - add to current building
            current_building += (" " + part if current_building else part)
            i += 1
    
    # Any remaining text becomes the incomplete part
    if current_building.strip():
        remaining_text = current_building.strip()
        logger.debug(f"ðŸ”„ Remaining incomplete text: '{remaining_text}'")
    
    logger.debug(f"ðŸ“Š Extracted {len(complete_sentences)} complete sentences, remaining: '{remaining_text}'")
    return complete_sentences, remaining_text


def is_sentence_ending(text: str) -> bool:
    """
    Check if the text ends with a sentence-ending punctuation mark.
    
    Args:
        text: The text to check
        
    Returns:
        True if the text ends with sentence-ending punctuation
    """
    if not text.strip():
        return False
    
    sentence_endings = ['.', '!', '?', 'ØŸ']
    return text.strip()[-1] in sentence_endings


def clean_text(text: str) -> str:
    """
    Clean and normalize text for processing.
    
    Args:
        text: The text to clean
        
    Returns:
        Cleaned text
    """
    # Remove extra whitespace
    text = ' '.join(text.split())
    # Remove leading/trailing whitespace
    text = text.strip()
    return text


def split_into_chunks(text: str, max_length: int = 500) -> List[str]:
    """
    Split text into chunks of maximum length, preferring to split at sentence boundaries.
    
    Args:
        text: The text to split
        max_length: Maximum length of each chunk
        
    Returns:
        List of text chunks
    """
    if len(text) <= max_length:
        return [text]
    
    chunks = []
    sentences, _ = extract_complete_sentences(text)
    
    current_chunk = ""
    for sentence in sentences:
        if len(current_chunk) + len(sentence) + 1 <= max_length:
            current_chunk += (" " + sentence if current_chunk else sentence)
        else:
            if current_chunk:
                chunks.append(current_chunk)
            current_chunk = sentence
    
    if current_chunk:
        chunks.append(current_chunk)
    
    return chunks


================================================
FILE: backup_20250723_021521/translation_helpers.py
================================================
"""
Translation helper functions for LiveKit AI Translation Server.
Handles translation orchestration and related utilities.
"""
import asyncio
import logging
from typing import List, Dict, Any, Optional

logger = logging.getLogger("transcriber.translation")


async def translate_sentences(
    sentences: List[str], 
    translators: Dict[str, Any],
    source_language: str = "ar"
) -> None:
    """
    Translate complete sentences to all target languages.
    
    This function takes a list of sentences and sends them to all available
    translators concurrently for better performance.
    
    Args:
        sentences: List of sentences to translate
        translators: Dictionary of language code to translator instances
        source_language: Source language code (default: "ar" for Arabic)
    """
    if not sentences or not translators:
        return
        
    for sentence in sentences:
        if sentence.strip():
            logger.info(f"ðŸŽ¯ TRANSLATING COMPLETE {source_language.upper()} SENTENCE: '{sentence}'")
            logger.info(f"ðŸ“Š Processing sentence for {len(translators)} translators")
            
            # Send to all translators concurrently for better performance
            translation_tasks = []
            for lang, translator in translators.items():
                logger.info(f"ðŸ“¤ Sending complete {source_language.upper()} sentence '{sentence}' to {lang} translator")
                translation_tasks.append(translator.translate(sentence, None))
            
            # Execute all translations concurrently
            if translation_tasks:
                results = await asyncio.gather(*translation_tasks, return_exceptions=True)
                # Check for any exceptions
                for i, result in enumerate(results):
                    if isinstance(result, Exception):
                        logger.error(f"âŒ Translation failed: {result}")


async def translate_single_sentence(
    sentence: str,
    translator: Any,
    target_language: str
) -> Optional[str]:
    """
    Translate a single sentence to a specific target language.
    
    Args:
        sentence: The sentence to translate
        translator: The translator instance to use
        target_language: Target language code
        
    Returns:
        Translated text or None if translation failed
    """
    try:
        if not sentence.strip():
            return None
            
        logger.debug(f"Translating to {target_language}: '{sentence}'")
        result = await translator.translate(sentence, None)
        return result
    except Exception as e:
        logger.error(f"Translation to {target_language} failed: {e}")
        return None


def should_translate_text(text: str, min_length: int = 3) -> bool:
    """
    Determine if text should be translated based on various criteria.
    
    Args:
        text: The text to evaluate
        min_length: Minimum length for translation (default: 3 characters)
        
    Returns:
        True if text should be translated
    """
    if not text or not text.strip():
        return False
    
    # Don't translate very short text
    if len(text.strip()) < min_length:
        return False
    
    # Don't translate if it's only punctuation
    if all(c in '.!?ØŸ,ØŒ;Ø›:' for c in text.strip()):
        return False
    
    return True


def format_translation_output(
    original_text: str,
    translated_text: str,
    source_lang: str,
    target_lang: str
) -> Dict[str, str]:
    """
    Format translation output for consistent structure.
    
    Args:
        original_text: Original text
        translated_text: Translated text
        source_lang: Source language code
        target_lang: Target language code
        
    Returns:
        Formatted translation dictionary
    """
    return {
        "original": original_text,
        "translated": translated_text,
        "source_language": source_lang,
        "target_language": target_lang,
        "type": "translation"
    }


async def batch_translate(
    texts: List[str],
    translators: Dict[str, Any],
    batch_size: int = 5
) -> Dict[str, List[str]]:
    """
    Translate multiple texts in batches for efficiency.
    
    Args:
        texts: List of texts to translate
        translators: Dictionary of language code to translator instances
        batch_size: Number of texts to process in each batch
        
    Returns:
        Dictionary mapping language codes to lists of translations
    """
    results = {lang: [] for lang in translators.keys()}
    
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]
        
        # Process each batch
        for text in batch:
            if should_translate_text(text):
                await translate_sentences([text], translators)
    
    return results


================================================
FILE: backup_20250723_021521/translator.py
================================================
"""
Translator module for LiveKit AI Translation Server.
Handles real-time translation with context management and error handling.
"""
import asyncio
import logging
from typing import Optional, Dict, Any, List, Callable
from collections import deque
from enum import Enum

from livekit import rtc
from livekit.agents import llm, utils
from livekit.plugins import openai

from config import get_config
from prompt_builder import get_prompt_builder

logger = logging.getLogger("transcriber.translator")
config = get_config()
prompt_builder = get_prompt_builder()


class TranslationError(Exception):
    """Custom exception for translation-related errors."""
    pass


class Translator:
    """
    Handles translation from source language to target language with context management.
    
    Features:
    - Sliding window context for better translation coherence
    - Automatic retry on failures
    - Comprehensive error handling
    - Real-time broadcasting to displays
    """
    
    # Class-level configuration from config module
    use_context = config.translation.use_context
    max_context_pairs = config.translation.max_context_pairs
    
    def __init__(self, room: rtc.Room, lang: Enum, tenant_context: Optional[Dict[str, Any]] = None, broadcast_callback: Optional[Callable] = None):
        """
        Initialize the Translator.
        
        Args:
            room: LiveKit room instance
            lang: Target language enum
            tenant_context: Optional context containing room_id, mosque_id, etc.
            broadcast_callback: Optional callback function for broadcasting translations
        """
        self.room = room
        self.lang = lang
        self.tenant_context = tenant_context or {}
        self.broadcast_callback = broadcast_callback
        self.llm = openai.LLM()
        
        # Initialize system prompt as None - will be built dynamically
        self.system_prompt = None
        self._prompt_template = None
        
        # Use deque for automatic sliding window (old messages auto-removed)
        if self.use_context:
            self.message_history: deque = deque(maxlen=(self.max_context_pairs * 2))
        
        # Track translation statistics
        self.translation_count = 0
        self.error_count = 0
        
        # Log the context mode being used
        context_mode = f"TRUE SLIDING WINDOW ({self.max_context_pairs}-pair memory)" if self.use_context else "FRESH CONTEXT (no memory)"
        logger.info(f"ðŸ§  Translator initialized for {lang.value} with {context_mode} mode")
        
        # Initialize prompt asynchronously on first use
        self._prompt_initialized = False

    async def translate(self, message: str, track: Optional[rtc.Track] = None, max_retries: int = 2) -> str:
        """
        Translate a message from source to target language.
        
        Args:
            message: Text to translate
            track: Optional audio track reference
            max_retries: Maximum number of retry attempts on failure
            
        Returns:
            Translated text (empty string on failure)
            
        Raises:
            TranslationError: If translation fails after all retries
        """
        if not message or not message.strip():
            logger.debug("Empty message, skipping translation")
            return ""
        
        retry_count = 0
        last_error = None
        
        while retry_count <= max_retries:
            try:
                translated_message = await self._perform_translation(message)
                
                if translated_message:
                    # Publish transcription to LiveKit room
                    await self._publish_transcription(translated_message, track)
                    
                    # Broadcast to displays
                    await self._broadcast_translation(translated_message)
                    
                    # Update statistics
                    self.translation_count += 1
                    
                    # Log successful translation
                    logger.info(f"âœ… Translated to {self.lang.value}: '{message}' â†’ '{translated_message}'")
                    
                    return translated_message
                else:
                    logger.warning(f"Empty translation result for: '{message}'")
                    return ""
                    
            except Exception as e:
                last_error = e
                retry_count += 1
                self.error_count += 1
                
                if retry_count <= max_retries:
                    wait_time = retry_count * 0.5  # Exponential backoff
                    logger.warning(
                        f"Translation attempt {retry_count} failed: {e}. "
                        f"Retrying in {wait_time}s..."
                    )
                    await asyncio.sleep(wait_time)
                else:
                    logger.error(
                        f"âŒ Translation failed after {max_retries} retries: {e}\n"
                        f"Message: '{message}'"
                    )
                    
        # If we get here, all retries failed
        error_msg = f"Translation failed for '{message}' after {max_retries} retries"
        if last_error:
            error_msg += f": {last_error}"
        
        # Don't raise exception - return empty string to keep stream going
        logger.error(error_msg)
        return ""

    async def _initialize_prompt(self):
        """Initialize the system prompt using the prompt builder."""
        if self._prompt_initialized:
            return
            
        try:
            # Get room ID from tenant context
            room_id = self.tenant_context.get('room_id')
            
            # Get source language from room config
            source_lang_code = self.tenant_context.get('transcription_language', 'ar')
            source_lang_name = config.translation.supported_languages.get(
                source_lang_code, 
                {"name": "Arabic"}
            )["name"]
            
            # Build the prompt using prompt builder
            self.system_prompt = await prompt_builder.get_prompt_for_room(
                room_id=room_id,
                source_lang=source_lang_name,
                target_lang=self.lang.value,
                room_config=self.tenant_context
            )
            
            logger.info(f"ðŸ“ Initialized translation prompt for room {room_id}: {source_lang_name} â†’ {self.lang.value}")
            self._prompt_initialized = True
            
        except Exception as e:
            logger.error(f"Failed to initialize prompt: {e}")
            # Fallback to default prompt with dynamic source language
            source_lang_code = self.tenant_context.get('transcription_language', 'ar')
            source_lang_name = config.translation.supported_languages.get(
                source_lang_code, 
                {"name": "Arabic"}
            )["name"]
            
            self.system_prompt = (
                f"You are an expert simultaneous interpreter. Your task is to translate from {source_lang_name} to {self.lang.value}. "
                f"Provide a direct and accurate translation of the user's input. Be concise and use natural-sounding language. "
                f"Do not add any additional commentary, explanations, or introductory phrases."
            )
            self._prompt_initialized = True
    
    async def _perform_translation(self, message: str) -> str:
        """
        Perform the actual translation using the LLM.
        
        Args:
            message: Text to translate
            
        Returns:
            Translated text
        """
        # Ensure prompt is initialized
        await self._initialize_prompt()
        # Build a fresh context for every translation (rebuild method)
        temp_context = llm.ChatContext()
        temp_context.add_message(role="system", content=self.system_prompt)
        
        # If using context, add the message history from our deque
        if self.use_context and hasattr(self, 'message_history'):
            logger.debug(f"ðŸ”„ Building context with {len(self.message_history)} historical messages")
            for msg in self.message_history:
                temp_context.add_message(role=msg['role'], content=msg['content'])
        
        # Add the current message to translate
        temp_context.add_message(content=message, role="user")
        
        # Get translation from LLM with the freshly built context
        stream = self.llm.chat(chat_ctx=temp_context)
        
        translated_message = ""
        async for chunk in stream:
            if chunk.delta is None:
                continue
            content = chunk.delta.content
            if content is None:
                break
            translated_message += content
        
        # If using context, update our history (deque will auto-remove old messages)
        if self.use_context and translated_message:
            self.message_history.append({"role": "user", "content": message})
            self.message_history.append({"role": "assistant", "content": translated_message})
            logger.debug(f"ðŸ’¾ History updated. Current size: {len(self.message_history)} messages")
        
        return translated_message

    async def _publish_transcription(self, translated_text: str, track: Optional[rtc.Track]) -> None:
        """
        Publish the translation as a transcription to the LiveKit room.
        
        Args:
            translated_text: The translated text to publish
            track: Optional track reference
        """
        try:
            segment = rtc.TranscriptionSegment(
                id=utils.misc.shortuuid("SG_"),
                text=translated_text,
                start_time=0,
                end_time=0,
                language=self.lang.value,
                final=True,
            )
            transcription = rtc.Transcription(
                self.room.local_participant.identity, 
                track.sid if track else "", 
                [segment]
            )
            await self.room.local_participant.publish_transcription(transcription)
            logger.debug(f"ðŸ“¤ Published {self.lang.value} transcription to LiveKit room")
        except Exception as e:
            logger.error(f"Failed to publish transcription: {e}")
            # Don't re-raise - translation was successful even if publishing failed

    async def _broadcast_translation(self, translated_text: str) -> None:
        """
        Broadcast the translation to WebSocket displays.
        
        Args:
            translated_text: The translated text to broadcast
        """
        if self.broadcast_callback:
            try:
                # Use asyncio.create_task to avoid blocking
                asyncio.create_task(
                    self.broadcast_callback(
                        "translation", 
                        self.lang.value, 
                        translated_text, 
                        self.tenant_context
                    )
                )
                logger.debug(f"ðŸ“¡ Broadcasted {self.lang.value} translation to displays")
            except Exception as e:
                logger.error(f"Failed to broadcast translation: {e}")
                # Don't re-raise - translation was successful even if broadcasting failed
        else:
            logger.debug("No broadcast callback provided, skipping broadcast")

    def get_statistics(self) -> Dict[str, Any]:
        """
        Get translation statistics.
        
        Returns:
            Dictionary containing translation stats
        """
        return {
            "language": self.lang.value,
            "translation_count": self.translation_count,
            "error_count": self.error_count,
            "error_rate": self.error_count / max(1, self.translation_count),
            "context_enabled": self.use_context,
            "context_size": len(self.message_history) if self.use_context else 0
        }

    def clear_context(self) -> None:
        """Clear the translation context history."""
        if self.use_context and hasattr(self, 'message_history'):
            self.message_history.clear()
            logger.info(f"ðŸ§¹ Cleared translation context for {self.lang.value}")

    def __repr__(self) -> str:
        """String representation of the Translator."""
        return (
            f"Translator(lang={self.lang.value}, "
            f"context={self.use_context}, "
            f"translations={self.translation_count}, "
            f"errors={self.error_count})"
        )


================================================
FILE: backup_20250723_021521/webhook_handler.py
================================================
#!/usr/bin/env python3
"""
Webhook handler for Supabase integration
Receives notifications about room creation and management from the dashboard
"""

import asyncio
import json
import logging
import os
from aiohttp import web
from typing import Dict, Any

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Webhook secret for validation (should match Supabase webhook secret)
WEBHOOK_SECRET = os.environ.get("SUPABASE_WEBHOOK_SECRET", "")

class WebhookHandler:
    def __init__(self):
        self.active_sessions: Dict[str, Dict[str, Any]] = {}
        
    async def handle_room_created(self, payload: dict):
        """Handle room creation webhook from Supabase"""
        try:
            # Extract room information (aligning with your Supabase schema)
            room_data = payload.get("record", {})
            room_name = room_data.get("Livekit_room_name")  # Note: Capital L in your schema
            mosque_id = room_data.get("mosque_id")
            room_id = room_data.get("id")
            room_title = room_data.get("Title")
            transcription_language = room_data.get("transcription_language")
            translation_language = room_data.get("translation__language")  # Note: double underscore
            
            if not room_name or not mosque_id:
                logger.error(f"Missing required fields in room creation webhook: {payload}")
                return {"status": "error", "message": "Missing Livekit_room_name or mosque_id"}
            
            # Store session information
            self.active_sessions[room_name] = {
                "room_id": room_id,
                "mosque_id": mosque_id,
                "room_title": room_title,
                "transcription_language": transcription_language or "ar",  # Default to Arabic
                "translation_language": translation_language or "nl",     # Default to Dutch
                "created_at": room_data.get("created_at"),
                "status": "active"
            }
            
            logger.info(f"ðŸ›ï¸ Room created for mosque {mosque_id}: {room_name} (ID: {room_id})")
            logger.info(f"ðŸ—£ï¸ Transcription: {transcription_language}, Translation: {translation_language}")
            logger.info(f"ðŸ“Š Active sessions: {len(self.active_sessions)}")
            
            return {"status": "success", "room_name": room_name, "room_id": room_id}
            
        except Exception as e:
            logger.error(f"Error handling room creation webhook: {e}")
            return {"status": "error", "message": str(e)}
    
    async def handle_room_deleted(self, payload: dict):
        """Handle room deletion webhook from Supabase"""
        try:
            # Extract room information
            room_data = payload.get("old_record", {})
            room_name = room_data.get("livekit_room_name")
            
            if room_name and room_name in self.active_sessions:
                del self.active_sessions[room_name]
                logger.info(f"ðŸ—‘ï¸ Room deleted: {room_name}")
                logger.info(f"ðŸ“Š Active sessions: {len(self.active_sessions)}")
            
            return {"status": "success", "room_name": room_name}
            
        except Exception as e:
            logger.error(f"Error handling room deletion webhook: {e}")
            return {"status": "error", "message": str(e)}
    
    async def handle_session_started(self, payload: dict):
        """Handle session start webhook from Supabase"""
        try:
            session_data = payload.get("record", {})
            room_id = session_data.get("room_id")
            session_id = session_data.get("id")
            mosque_id = session_data.get("mosque_id")
            logging_enabled = session_data.get("logging_enabled", False)
            
            logger.info(f"ðŸŽ¤ Session started: {session_id} for room {room_id}, mosque {mosque_id}")
            logger.info(f"ðŸ“ Logging enabled: {logging_enabled}")
            
            # Find matching room by room_id and update with session info
            room_found = False
            for room_name, room_info in self.active_sessions.items():
                if room_info.get("room_id") == room_id:
                    room_info["session_id"] = session_id
                    room_info["session_started_at"] = session_data.get("started_at")
                    room_info["logging_enabled"] = logging_enabled
                    room_info["status"] = "recording" if logging_enabled else "active"
                    logger.info(f"ðŸ›ï¸ Updated room {room_name} with session {session_id}")
                    room_found = True
                    break
            
            if not room_found:
                # Create temporary session entry if room not found
                logger.warning(f"âš ï¸ Room not found for session {session_id}, creating temporary entry")
                temp_room_name = f"session_{session_id[:8]}"
                self.active_sessions[temp_room_name] = {
                    "room_id": room_id,
                    "mosque_id": mosque_id,
                    "session_id": session_id,
                    "session_started_at": session_data.get("started_at"),
                    "logging_enabled": logging_enabled,
                    "status": "recording" if logging_enabled else "active",
                    "transcription_language": "ar",  # Default
                    "translation_language": "nl"    # Default
                }
                    
            return {"status": "success", "session_id": session_id, "logging_enabled": logging_enabled}
            
        except Exception as e:
            logger.error(f"Error handling session start webhook: {e}")
            return {"status": "error", "message": str(e)}
    
    async def handle_session_ended(self, payload: dict):
        """Handle session end webhook from Supabase"""
        try:
            session_data = payload.get("record", {})
            session_id = session_data.get("id")
            
            # Update session status
            for room_name, room_info in self.active_sessions.items():
                if room_info.get("session_id") == session_id:
                    room_info["session_ended_at"] = session_data.get("ended_at")
                    room_info["status"] = "ended"
                    logger.info(f"ðŸ›‘ Session ended for room {room_name}: {session_id}")
                    break
                    
            return {"status": "success", "session_id": session_id}
            
        except Exception as e:
            logger.error(f"Error handling session end webhook: {e}")
            return {"status": "error", "message": str(e)}
    
    def get_room_context(self, room_name: str) -> Dict[str, Any]:
        """Get tenant context for a specific room"""
        return self.active_sessions.get(room_name, {})

# Global webhook handler instance
webhook_handler = WebhookHandler()

async def handle_webhook(request):
    """Main webhook endpoint handler"""
    try:
        # Validate webhook secret if configured
        if WEBHOOK_SECRET:
            webhook_signature = request.headers.get("X-Supabase-Signature", "")
            # TODO: Implement proper signature validation
            
        # Parse webhook payload
        payload = await request.json()
        webhook_type = payload.get("type")
        table = payload.get("table")
        
        logger.info(f"ðŸ“¨ Received webhook: type={webhook_type}, table={table}")
        
        # Route to appropriate handler
        result = {"status": "error", "message": "Unknown webhook type"}
        
        if table == "rooms":
            if webhook_type == "INSERT":
                result = await webhook_handler.handle_room_created(payload)
            elif webhook_type == "DELETE":
                result = await webhook_handler.handle_room_deleted(payload)
                
        elif table == "room_sessions":
            if webhook_type == "INSERT":
                result = await webhook_handler.handle_session_started(payload)
            elif webhook_type == "UPDATE":
                # Check if session is ending
                if payload.get("record", {}).get("ended_at"):
                    result = await webhook_handler.handle_session_ended(payload)
                    
        return web.json_response(result)
        
    except json.JSONDecodeError:
        return web.json_response({"status": "error", "message": "Invalid JSON"}, status=400)
    except Exception as e:
        logger.error(f"Error processing webhook: {e}")
        return web.json_response({"status": "error", "message": str(e)}, status=500)

async def handle_status(request):
    """Status endpoint to check webhook handler health"""
    return web.json_response({
        "status": "healthy",
        "active_sessions": len(webhook_handler.active_sessions),
        "sessions": list(webhook_handler.active_sessions.keys())
    })

async def start_webhook_server():
    """Start the webhook server"""
    app = web.Application()
    
    # Add routes
    app.router.add_post('/webhook', handle_webhook)
    app.router.add_get('/status', handle_status)
    
    # Add CORS middleware
    @web.middleware
    async def cors_middleware(request, handler):
        if request.method == 'OPTIONS':
            return web.Response(headers={
                'Access-Control-Allow-Origin': '*',
                'Access-Control-Allow-Methods': 'POST, GET, OPTIONS',
                'Access-Control-Allow-Headers': 'Content-Type, X-Supabase-Signature',
            })
        response = await handler(request)
        response.headers['Access-Control-Allow-Origin'] = '*'
        return response
    
    app.middlewares.append(cors_middleware)
    
    # Start server
    runner = web.AppRunner(app)
    await runner.setup()
    site = web.TCPSite(runner, '0.0.0.0', 8767)
    await site.start()
    
    logger.info("ðŸš€ Webhook server started on http://0.0.0.0:8767")
    logger.info("ðŸ“¨ Webhook endpoint: POST http://0.0.0.0:8767/webhook")
    logger.info("ðŸ“Š Status endpoint: GET http://0.0.0.0:8767/status")
    
    try:
        await asyncio.Future()  # Run forever
    except KeyboardInterrupt:
        logger.info("Shutting down webhook server...")
        await runner.cleanup()

# Export the handler for use in main.py
def get_room_context(room_name: str) -> Dict[str, Any]:
    """Get tenant context for a room from webhook handler"""
    return webhook_handler.get_room_context(room_name)

if __name__ == "__main__":
    try:
        asyncio.run(start_webhook_server())
    except KeyboardInterrupt:
        logger.info("Webhook server stopped by user")
    except Exception as e:
        logger.error(f"Webhook server error: {e}")


================================================
FILE: backup_20250723_173804/broadcasting.py
================================================
"""
Broadcasting module for LiveKit AI Translation Server.
Handles real-time broadcasting of transcriptions and translations to displays.
"""
import asyncio
import logging
import hashlib
from typing import Optional, Dict, Any
from datetime import datetime

from config import get_config
from database import broadcast_to_channel, store_transcript_in_database

logger = logging.getLogger("transcriber.broadcasting")
config = get_config()


class BroadcastError(Exception):
    """Custom exception for broadcasting-related errors."""
    pass


async def broadcast_to_displays(
    message_type: str, 
    language: str, 
    text: str, 
    tenant_context: Optional[Dict[str, Any]] = None
) -> bool:
    """
    Send transcription/translation to frontend via Supabase Broadcast and store in database.
    
    This function handles both real-time broadcasting and database storage of
    transcriptions and translations. It uses Supabase's broadcast feature for
    real-time updates and stores the data for persistence.
    
    Args:
        message_type: Type of message ("transcription" or "translation")
        language: Language code (e.g., "ar", "nl")
        text: The text content to broadcast
        tenant_context: Optional context containing room_id, mosque_id, etc.
        
    Returns:
        bool: True if broadcast was successful, False otherwise
    """
    if not text or not text.strip():
        logger.debug("Empty text provided, skipping broadcast")
        return False
    
    success = False
    
    # Phase 1: Immediate broadcast via Supabase for real-time display
    if tenant_context and tenant_context.get("room_id") and tenant_context.get("mosque_id"):
        try:
            channel_name = f"live-transcription-{tenant_context['room_id']}-{tenant_context['mosque_id']}"
            
            # Generate unique message ID based on timestamp and content hash
            timestamp = datetime.utcnow().isoformat() + "Z"
            text_hash = hashlib.md5(text.encode('utf-8')).hexdigest()[:8]
            msg_id = f"{timestamp}_{text_hash}"
            
            payload = {
                "type": message_type,
                "room_id": tenant_context["room_id"],
                "mosque_id": tenant_context["mosque_id"],
                "data": {
                    "text": text,
                    "language": language,
                    "timestamp": timestamp,
                    "msg_id": msg_id
                }
            }
            
            # Use the broadcast_to_channel function from database module
            success = await broadcast_to_channel(channel_name, message_type, payload)
            
            if success:
                logger.info(
                    f"ðŸ“¡ LIVE: Sent {message_type} ({language}) via Supabase broadcast: "
                    f"{text[:50]}{'...' if len(text) > 50 else ''}"
                )
            else:
                logger.warning(f"âš ï¸ Failed to broadcast {message_type} to Supabase")
                
        except Exception as e:
            logger.error(f"âŒ Broadcast error: {e}")
            success = False
    else:
        logger.warning("âš ï¸ Missing tenant context for Supabase broadcast")
    
    # Phase 2: Direct database storage (no batching)
    if tenant_context and tenant_context.get("room_id") and tenant_context.get("mosque_id"):
        try:
            # Store directly in database using existing function
            # Use create_task to avoid blocking the broadcast with proper error handling
            task = asyncio.create_task(
                _store_with_error_handling(message_type, language, text, tenant_context)
            )
            task.add_done_callback(lambda t: None if not t.exception() else logger.error(f"Storage task failed: {t.exception()}"))
            logger.debug(
                f"ðŸ’¾ DIRECT: Storing {message_type} directly to database "
                f"for room {tenant_context['room_id']}"
            )
        except Exception as e:
            logger.error(f"âŒ Failed to initiate database storage: {e}")
    else:
        logger.warning("âš ï¸ Missing tenant context for database storage")
    
    return success


async def _store_with_error_handling(
    message_type: str, 
    language: str, 
    text: str, 
    tenant_context: Dict[str, Any]
) -> None:
    """
    Store transcript with proper error handling.
    
    This is a wrapper around store_transcript_in_database that ensures
    errors don't propagate and crash the application.
    
    Args:
        message_type: Type of message ("transcription" or "translation")
        language: Language code
        text: The text content to store
        tenant_context: Context containing room_id, mosque_id, etc.
    """
    try:
        success = await store_transcript_in_database(
            message_type, language, text, tenant_context
        )
        if not success:
            logger.warning(
                f"âš ï¸ Failed to store {message_type} in database for "
                f"room {tenant_context.get('room_id')}"
            )
    except Exception as e:
        logger.error(
            f"âŒ Database storage error for {message_type}: {e}\n"
            f"Room: {tenant_context.get('room_id')}, "
            f"Language: {language}"
        )


async def broadcast_batch(
    messages: list[tuple[str, str, str, Dict[str, Any]]]
) -> Dict[str, int]:
    """
    Broadcast multiple messages in batch for efficiency.
    
    Args:
        messages: List of tuples (message_type, language, text, tenant_context)
        
    Returns:
        Dictionary with counts of successful and failed broadcasts
    """
    results = {"success": 0, "failed": 0}
    
    # Process all broadcasts concurrently
    tasks = []
    for message_type, language, text, tenant_context in messages:
        task = broadcast_to_displays(message_type, language, text, tenant_context)
        tasks.append(task)
    
    # Wait for all broadcasts to complete
    broadcast_results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # Count results
    for result in broadcast_results:
        if isinstance(result, Exception):
            results["failed"] += 1
            logger.error(f"Batch broadcast error: {result}")
        elif result:
            results["success"] += 1
        else:
            results["failed"] += 1
    
    logger.info(
        f"ðŸ“Š Batch broadcast complete: "
        f"{results['success']} successful, {results['failed']} failed"
    )
    
    return results


def create_broadcast_payload(
    message_type: str,
    language: str,
    text: str,
    room_id: int,
    mosque_id: int,
    additional_data: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """
    Create a standardized broadcast payload.
    
    Args:
        message_type: Type of message
        language: Language code
        text: The text content
        room_id: Room ID
        mosque_id: Mosque ID
        additional_data: Optional additional data to include
        
    Returns:
        Formatted payload dictionary
    """
    # Generate unique message ID
    timestamp = datetime.utcnow().isoformat() + "Z"
    text_hash = hashlib.md5(text.encode('utf-8')).hexdigest()[:8]
    msg_id = f"{timestamp}_{text_hash}"
    
    payload = {
        "type": message_type,
        "room_id": room_id,
        "mosque_id": mosque_id,
        "data": {
            "text": text,
            "language": language,
            "timestamp": timestamp,
            "msg_id": msg_id
        }
    }
    
    if additional_data:
        payload["data"].update(additional_data)
    
    return payload


def get_channel_name(room_id: int, mosque_id: int) -> str:
    """
    Generate the channel name for a room.
    
    Args:
        room_id: Room ID
        mosque_id: Mosque ID
        
    Returns:
        Channel name string
    """
    return f"live-transcription-{room_id}-{mosque_id}"


================================================
FILE: backup_20250723_173804/main.py
================================================
import asyncio
import logging
import json
import time
import re
import os
from typing import Set, Any, Dict, Optional
from collections import defaultdict, deque
from datetime import datetime, timedelta

from enum import Enum
from dataclasses import dataclass, asdict

from livekit import rtc
from livekit.agents import (
    AutoSubscribe,
    JobContext,
    JobProcess,
    JobRequest,
    WorkerOptions,
    cli,
    stt,
    utils,
)
from livekit.plugins import silero, speechmatics
from livekit.plugins.speechmatics.types import TranscriptionConfig

# Import configuration
from config import get_config, ApplicationConfig

# Import database operations
from database import (
    ensure_active_session,
    store_transcript_in_database,
    query_room_by_name,
    get_active_session_for_room,
    broadcast_to_channel,
    close_database_connections
)

# Import text processing and translation helpers
from text_processing import extract_complete_sentences
from translation_helpers import translate_sentences

# Import Translator class
from translator import Translator

# Import broadcasting function
from broadcasting import broadcast_to_displays

# Import resource management
from resource_management import ResourceManager, TaskManager, STTStreamManager

# Import webhook handler for room context
try:
    from webhook_handler import get_room_context as get_webhook_room_context
except ImportError:
    # Webhook handler not available, use empty context
    def get_webhook_room_context(room_name: str):
        return {}


# Load configuration
config = get_config()

logger = logging.getLogger("transcriber")


@dataclass
class Language:
    code: str
    name: str
    flag: str


# Build languages dictionary from config
languages = {}
for code, lang_info in config.translation.supported_languages.items():
    languages[code] = Language(
        code=code,
        name=lang_info["name"],
        flag=lang_info["flag"]
    )

LanguageCode = Enum(
    "LanguageCode",  # Name of the Enum
    {lang.name: code for code, lang in languages.items()},  # Enum entries: name -> code mapping
)


# Translator class has been moved to translator.py


def prewarm(proc: JobProcess):
    proc.userdata["vad"] = silero.VAD.load()


async def entrypoint(job: JobContext):
    # Configure source language - ARABIC as default
    # This will be the language that users are actually speaking (host/speaker language)
    source_language = config.translation.default_source_language
    
    # Initialize resource manager
    resource_manager = ResourceManager()
    
    # Extract tenant context from room metadata or webhook handler
    tenant_context = {}
    try:
        # Try to query Supabase directly for room information
        if job.room and job.room.name:
            logger.info(f"ðŸ” Looking up room context for: {job.room.name}")
            
            # Check if database is configured
            logger.info(f"ðŸ”‘ Supabase URL: {config.supabase.url}")
            logger.info(f"ðŸ”‘ Supabase key available: {'Yes' if config.supabase.service_role_key else 'No'}")
            
            if config.supabase.service_role_key:
                try:
                    # Query room by LiveKit room name using the new database module
                    logger.info(f"ðŸ” Querying database for room: {job.room.name}")
                    # Query room directly without task wrapper
                    room_data = await query_room_by_name(job.room.name)
                    
                    if room_data:
                        tenant_context = {
                            "room_id": room_data.get("id"),
                            "mosque_id": room_data.get("mosque_id"),
                            "room_title": room_data.get("Title"),
                            "transcription_language": room_data.get("transcription_language", "ar"),
                            "translation_language": room_data.get("translation__language", "nl"),
                            "created_at": room_data.get("created_at")
                        }
                        # Also store the double underscore version for compatibility
                        if room_data.get("translation__language"):
                            tenant_context["translation__language"] = room_data.get("translation__language")
                        
                        logger.info(f"âœ… Found room in database: room_id={tenant_context.get('room_id')}, mosque_id={tenant_context.get('mosque_id')}")
                        logger.info(f"ðŸ—£ï¸ Languages: transcription={tenant_context.get('transcription_language')}, translation={tenant_context.get('translation_language')} (or {tenant_context.get('translation__language')})")
                        
                        # Try to get active session for this room
                        session_id = await get_active_session_for_room(tenant_context['room_id'])
                        if session_id:
                            tenant_context["session_id"] = session_id
                            logger.info(f"ðŸ“ Found active session: {tenant_context['session_id']}")
                except Exception as e:
                    logger.warning(f"âš ï¸ Could not query Supabase: {e}")
        
        # Fallback to webhook handler if available
        if not tenant_context:
            webhook_context = get_webhook_room_context(job.room.name if job.room else "")
            if webhook_context:
                tenant_context = {
                    "room_id": webhook_context.get("room_id"),
                    "mosque_id": webhook_context.get("mosque_id"),
                    "session_id": webhook_context.get("session_id"),
                    "room_title": webhook_context.get("room_title"),
                    "transcription_language": webhook_context.get("transcription_language", "ar"),
                    "translation_language": webhook_context.get("translation_language", "nl"),
                    "created_at": webhook_context.get("created_at")
                }
                logger.info(f"ðŸ¢ Tenant context from webhook handler: mosque_id={tenant_context.get('mosque_id')}, room_id={tenant_context.get('room_id')}")
        
        # Fallback to room metadata if available
        if not tenant_context and job.room and job.room.metadata:
            try:
                metadata = json.loads(job.room.metadata)
                tenant_context = {
                    "room_id": metadata.get("room_id"),
                    "mosque_id": metadata.get("mosque_id"),
                    "session_id": metadata.get("session_id"),
                    "room_title": metadata.get("room_title"),
                    "transcription_language": metadata.get("transcription_language", "ar"),
                    "translation_language": metadata.get("translation_language", "nl"),
                    "created_at": metadata.get("created_at")
                }
                logger.info(f"ðŸ¢ Tenant context from room metadata: mosque_id={tenant_context.get('mosque_id')}, room_id={tenant_context.get('room_id')}")
            except:
                pass
        
        # Final fallback to default context with hardcoded values for testing
        if not tenant_context:
            logger.warning(f"âš ï¸ No tenant context available for room: {job.room.name if job.room else 'unknown'}")
            # TEMPORARY: Use hardcoded values for mosque_546012 rooms
            if job.room and f"mosque_{config.test_mosque_id}" in job.room.name:
                tenant_context = {
                    "room_id": config.test_room_id,
                    "mosque_id": config.test_mosque_id,
                    "session_id": None,
                    "transcription_language": "ar",
                    "translation_language": "nl"
                }
                logger.info(f"ðŸ”§ Using hardcoded tenant context for testing: mosque_id={tenant_context['mosque_id']}, room_id={tenant_context['room_id']}")
            else:
                tenant_context = {
                    "room_id": None,
                    "mosque_id": int(os.getenv('DEFAULT_MOSQUE_ID', str(config.default_mosque_id))),
                    "session_id": None,
                    "transcription_language": "ar",
                    "translation_language": "nl"
                }
    except Exception as e:
        logger.warning(f"âš ï¸ Could not extract tenant context: {e}")
    
    # Configure Speechmatics STT with room-specific settings
    # Use tenant_context which already has room configuration
    room_config = None
    if tenant_context and tenant_context.get('room_id'):
        # We already have the room data in tenant_context from earlier query
        room_config = tenant_context
        logger.info(f"ðŸ“‹ Using room-specific configuration from context: "
                  f"lang={room_config.get('transcription_language', 'ar')}, "
                  f"target={room_config.get('translation_language', 'nl')}, "
                  f"delay={room_config.get('max_delay', 2.0)}, "
                  f"punct={room_config.get('punctuation_sensitivity', 0.5)}")
        
        # If we need full room data and it's not in context, query it
        if not room_config.get('max_delay'):
            try:
                full_room_data = await query_room_by_name(job.room.name if job.room else None)
                if full_room_data:
                    # Merge the full room data with tenant context
                    room_config.update({
                        'max_delay': full_room_data.get('max_delay'),
                        'punctuation_sensitivity': full_room_data.get('punctuation_sensitivity'),
                        'translation__language': full_room_data.get('translation__language')
                    })
                    logger.info(f"ðŸ“‹ Fetched additional room config: delay={room_config.get('max_delay')}, punct={room_config.get('punctuation_sensitivity')}")
            except Exception as e:
                logger.warning(f"Failed to fetch additional room config: {e}")
    
    # Create STT configuration with room-specific overrides
    stt_config = config.speechmatics.with_room_settings(room_config)
    
    # Initialize STT provider with configured settings
    stt_provider = speechmatics.STT(
        transcription_config=TranscriptionConfig(
            language=stt_config.language,
            operating_point=stt_config.operating_point,
            enable_partials=stt_config.enable_partials,
            max_delay=stt_config.max_delay,
            punctuation_overrides={"sensitivity": stt_config.punctuation_sensitivity},
            diarization=stt_config.diarization
        )
    )
    
    # Update source language based on room config
    source_language = config.translation.get_source_language(room_config)
    logger.info(f"ðŸ—£ï¸ STT configured for {languages[source_language].name} speech recognition")
    
    translators = {}
    
    # Get target language from room config or use default
    target_language = config.translation.get_target_language(room_config)
    logger.info(f"ðŸŽ¯ Target language resolved to: '{target_language}' (from room_config: {room_config.get('translation_language') if room_config else 'None'} or {room_config.get('translation__language') if room_config else 'None'})")
    
    # Create translator for the configured target language
    if target_language in languages:
        # Get language enum dynamically
        lang_info = languages[target_language]
        lang_enum = getattr(LanguageCode, lang_info.name)
        translators[target_language] = Translator(job.room, lang_enum, tenant_context, broadcast_to_displays)
        logger.info(f"ðŸ“ Initialized {lang_info.name} translator ({target_language})")
    else:
        logger.warning(f"âš ï¸ Target language '{target_language}' not supported, falling back to Dutch")
        dutch_enum = getattr(LanguageCode, 'Dutch')
        translators["nl"] = Translator(job.room, dutch_enum, tenant_context, broadcast_to_displays)
    
    # Sentence accumulation for proper sentence-by-sentence translation
    accumulated_text = ""  # Accumulates text until we get a complete sentence
    last_final_transcript = ""  # Keep track of the last final transcript to avoid duplicates
    
    logger.info(f"ðŸš€ Starting entrypoint for room: {job.room.name if job.room else 'unknown'}")
    logger.info(f"ðŸ” Translators dict ID: {id(translators)}")
    logger.info(f"ðŸŽ¯ Configuration: {languages[source_language].name} â†’ {languages.get(target_language, languages['nl']).name}")
    logger.info(f"âš™ï¸ STT Settings: delay={stt_config.max_delay}s, punctuation={stt_config.punctuation_sensitivity}")

    async def _forward_transcription(
        stt_stream: stt.SpeechStream,
        track: rtc.Track,
    ):
        """Forward the transcription and log the transcript in the console"""
        nonlocal accumulated_text, last_final_transcript
        
        try:
            async for ev in stt_stream:
                # Log to console for interim (word-by-word)
                if ev.type == stt.SpeechEventType.INTERIM_TRANSCRIPT:
                    print(ev.alternatives[0].text, end="", flush=True)
                    
                    # Publish interim transcription for real-time word-by-word display
                    interim_text = ev.alternatives[0].text.strip()
                    if interim_text:
                        try:
                            interim_segment = rtc.TranscriptionSegment(
                                id=utils.misc.shortuuid("SG_"),
                                text=interim_text,
                                start_time=0,
                                end_time=0,
                                language=source_language,  # Arabic
                                final=False,  # This is interim, not final
                            )
                            interim_transcription = rtc.Transcription(
                                job.room.local_participant.identity, "", [interim_segment]
                            )
                            await job.room.local_participant.publish_transcription(interim_transcription)
                        except Exception as e:
                            logger.debug(f"Failed to publish interim transcription: {str(e)}")
                    
                elif ev.type == stt.SpeechEventType.FINAL_TRANSCRIPT:
                    print("\n")
                    final_text = ev.alternatives[0].text.strip()
                    print(" -> ", final_text)
                    logger.info(f"Final Arabic transcript: {final_text}")

                    if final_text and final_text != last_final_transcript:
                        last_final_transcript = final_text
                        
                        # Publish final transcription for the original language (Arabic)
                        try:
                            final_segment = rtc.TranscriptionSegment(
                                id=utils.misc.shortuuid("SG_"),
                                text=final_text,
                                start_time=0,
                                end_time=0,
                                language=source_language,  # Arabic
                                final=True,
                            )
                            final_transcription = rtc.Transcription(
                                job.room.local_participant.identity, "", [final_segment]
                            )
                            await job.room.local_participant.publish_transcription(final_transcription)
                            
                            logger.info(f"âœ… Published final {languages[source_language].name} transcription: '{final_text}'")
                        except Exception as e:
                            logger.error(f"âŒ Failed to publish final transcription: {str(e)}")
                        
                        # Broadcast final transcription text for real-time display
                        print(f"ðŸ“¡ Broadcasting Arabic text to frontend: '{final_text}'")
                        # Broadcast directly without task wrapper
                        await broadcast_to_displays("transcription", source_language, final_text, tenant_context)
                        
                        # Handle translation logic
                        if translators:
                            # SIMPLE ACCUMULATION LOGIC - ONLY APPEND, NEVER REPLACE
                            if accumulated_text:
                                # ALWAYS append new final transcript to existing accumulated text
                                accumulated_text = accumulated_text.strip() + " " + final_text
                            else:
                                # First transcript - start accumulation
                                accumulated_text = final_text
                            
                            logger.info(f"ðŸ“ Updated accumulated Arabic text: '{accumulated_text}'")
                            
                            # Extract complete sentences from accumulated text
                            complete_sentences, remaining_text = extract_complete_sentences(accumulated_text)
                            
                            # Handle special punctuation completion signal
                            if complete_sentences and complete_sentences[0] == "PUNCTUATION_COMPLETE":
                                if accumulated_text.strip():
                                    # Complete the accumulated sentence with this punctuation
                                    print(f"ðŸ“ PUNCTUATION SIGNAL: Completing accumulated text: '{accumulated_text}'")
                                    
                                    # Translate the completed sentence (don't include the punctuation marker)
                                    await translate_sentences([accumulated_text], translators, source_language)
                                    
                                    # Clear accumulated text as sentence is now complete
                                    accumulated_text = ""
                                    print(f"ðŸ“ Cleared accumulated text after punctuation completion")
                                else:
                                    print(f"âš ï¸ Received punctuation completion signal but no accumulated text")
                            elif complete_sentences:
                                # We have complete sentences - translate them immediately
                                print(f"ðŸŽ¯ Found {len(complete_sentences)} complete Arabic sentences: {complete_sentences}")
                                
                                # Translate complete sentences
                                await translate_sentences(complete_sentences, translators, source_language)
                                
                                # Update accumulated text to only remaining incomplete text
                                accumulated_text = remaining_text
                                print(f"ðŸ“ Updated accumulated Arabic text after sentence extraction: '{accumulated_text}'")
                            
                            # Log remaining incomplete text (no delayed translation)
                            if accumulated_text.strip():
                                logger.info(f"ðŸ“ Incomplete Arabic text remaining: '{accumulated_text}'")
                                # Note: Incomplete text will be translated when the next sentence completes
                        else:
                            logger.warning(f"âš ï¸ No translators available in room {job.room.name}, only {languages[source_language].name} transcription published")
                    else:
                        logger.debug("Empty or duplicate transcription, skipping")
        except Exception as e:
            logger.error(f"STT transcription error: {str(e)}")
            raise

    async def transcribe_track(participant: rtc.RemoteParticipant, track: rtc.Track):
        logger.info(f"ðŸŽ¤ Starting Arabic transcription for participant {participant.identity}, track {track.sid}")
        
        try:
            audio_stream = rtc.AudioStream(track)
            
            # Use context manager for STT stream
            async with resource_manager.stt_manager.create_stream(stt_provider, participant.identity) as stt_stream:
                # Create transcription task with tracking
                stt_task = resource_manager.task_manager.create_task(
                    _forward_transcription(stt_stream, track),
                    name=f"transcribe-{participant.identity}",
                    metadata={"participant": participant.identity, "track": track.sid}
                )
                
                frame_count = 0
                async for ev in audio_stream:
                    frame_count += 1
                    if frame_count % 100 == 0:  # Log every 100 frames to avoid spam
                        logger.debug(f"ðŸ”Š Received audio frame #{frame_count} from {participant.identity}")
                    stt_stream.push_frame(ev.frame)
                    
                logger.warning(f"ðŸ”‡ Audio stream ended for {participant.identity}")
                
                # Cancel the transcription task if still running
                if not stt_task.done():
                    stt_task.cancel()
                    try:
                        await stt_task
                    except asyncio.CancelledError:
                        logger.debug(f"STT task cancelled for {participant.identity}")
                        
        except Exception as e:
            logger.error(f"âŒ Transcription track error for {participant.identity}: {str(e)}")
        
        logger.info(f"ðŸ§¹ Transcription cleanup completed for {participant.identity}")

    @job.room.on("track_subscribed")
    def on_track_subscribed(
        track: rtc.Track,
        publication: rtc.TrackPublication,
        participant: rtc.RemoteParticipant,
    ):
        logger.info(f"ðŸŽµ Track subscribed: {track.kind} from {participant.identity} (track: {track.sid})")
        logger.info(f"Track details - muted: {publication.muted}")
        if track.kind == rtc.TrackKind.KIND_AUDIO:
            logger.info(f"âœ… Adding Arabic transcriber for participant: {participant.identity}")
            resource_manager.task_manager.create_task(
                transcribe_track(participant, track),
                name=f"track-handler-{participant.identity}",
                metadata={"participant": participant.identity, "track": track.sid}
            )
        else:
            logger.info(f"âŒ Ignoring non-audio track: {track.kind}")

    @job.room.on("track_published")
    def on_track_published(publication: rtc.TrackPublication, participant: rtc.RemoteParticipant):
        logger.info(f"ðŸ“¡ Track published: {publication.kind} from {participant.identity} (track: {publication.sid})")
        logger.info(f"Publication details - muted: {publication.muted}")

    @job.room.on("track_unpublished") 
    def on_track_unpublished(publication: rtc.TrackPublication, participant: rtc.RemoteParticipant):
        logger.info(f"ðŸ“¡ Track unpublished: {publication.kind} from {participant.identity}")

    @job.room.on("participant_connected")
    def on_participant_connected(participant: rtc.RemoteParticipant):
        logger.info(f"ðŸ‘¥ Participant connected: {participant.identity}")
        
        # Try to extract metadata from participant if available
        if hasattr(participant, 'metadata') and participant.metadata:
            try:
                participant_metadata = json.loads(participant.metadata)
                if participant_metadata:
                    # Update tenant context with participant metadata
                    tenant_context.update({
                        "room_id": participant_metadata.get("room_id", tenant_context.get("room_id")),
                        "mosque_id": participant_metadata.get("mosque_id", tenant_context.get("mosque_id")),
                        "session_id": participant_metadata.get("session_id", tenant_context.get("session_id")),
                        "room_title": participant_metadata.get("room_title", tenant_context.get("room_title"))
                    })
                    logger.info(f"ðŸ“‹ Updated tenant context from participant metadata: {tenant_context}")
                    
                    # Update all translators with new context
                    for translator in translators.values():
                        translator.tenant_context = tenant_context
            except Exception as e:
                logger.debug(f"Could not parse participant metadata: {e}")

    @job.room.on("participant_disconnected")
    def on_participant_disconnected(participant: rtc.RemoteParticipant):
        logger.info(f"ðŸ‘¥ Participant disconnected: {participant.identity}")
        
        # Resource cleanup is now handled by ResourceManager
        # Log current resource statistics
        resource_manager.log_stats()
        logger.info(f"ðŸ§¹ Participant cleanup completed for {participant.identity}")

    @job.room.on("participant_attributes_changed")
    def on_attributes_changed(
        changed_attributes: dict[str, str], participant: rtc.Participant
    ):
        """
        When participant attributes change, handle new translation requests.
        """
        logger.info(f"ðŸŒ Participant {participant.identity} attributes changed: {changed_attributes}")
        lang = changed_attributes.get("captions_language", None)
        if lang:
            if lang == source_language:
                logger.info(f"âœ… Participant {participant.identity} requested {languages[source_language].name} (source language - Arabic)")
            elif lang in translators:
                logger.info(f"âœ… Participant {participant.identity} requested existing language: {lang}")
                logger.info(f"ðŸ“Š Current translators for this room: {list(translators.keys())}")
            else:
                # Check if the language is supported and different from source language
                if lang in languages:
                    try:
                        # Create a translator for the requested language using the language enum
                        language_obj = languages[lang]
                        language_enum = getattr(LanguageCode, language_obj.name)
                        translators[lang] = Translator(job.room, language_enum, tenant_context, broadcast_to_displays)
                        logger.info(f"ðŸ†• Added translator for ROOM {job.room.name} (requested by {participant.identity}), language: {language_obj.name}")
                        logger.info(f"ðŸ¢ Translator created with tenant context: mosque_id={tenant_context.get('mosque_id')}")
                        logger.info(f"ðŸ“Š Total translators for room {job.room.name}: {len(translators)} -> {list(translators.keys())}")
                        logger.info(f"ðŸ” Translators dict ID: {id(translators)}")
                        
                        # Debug: Verify the translator was actually added
                        if lang in translators:
                            logger.info(f"âœ… Translator verification: {lang} successfully added to room translators")
                        else:
                            logger.error(f"âŒ Translator verification FAILED: {lang} not found in translators dict")
                            
                    except Exception as e:
                        logger.error(f"âŒ Error creating translator for {lang}: {str(e)}")
                else:
                    logger.warning(f"âŒ Unsupported language requested by {participant.identity}: {lang}")
                    logger.info(f"ðŸ’¡ Supported languages: {list(languages.keys())}")
        else:
            logger.debug(f"No caption language change for participant {participant.identity}")

    logger.info("Connecting to room...")
    await job.connect(auto_subscribe=AutoSubscribe.AUDIO_ONLY)
    logger.info(f"Successfully connected to room: {job.room.name}")
    logger.info(f"ðŸ“¡ Real-time transcription data will be sent via Supabase Broadcast")
    
    # Debug room state after connection
    logger.info(f"Room participants: {len(job.room.remote_participants)}")
    for participant in job.room.remote_participants.values():
        logger.info(f"Participant: {participant.identity}")
        logger.info(f"  Audio tracks: {len(participant.track_publications)}")
        for sid, pub in participant.track_publications.items():
            logger.info(f"    Track {sid}: {pub.kind}, muted: {pub.muted}")

    # Also check local participant
    logger.info(f"Local participant: {job.room.local_participant.identity}")
    logger.info(f"Local participant tracks: {len(job.room.local_participant.track_publications)}")

    @job.room.local_participant.register_rpc_method("get/languages")
    async def get_languages(data: rtc.RpcInvocationData):
        languages_list = [asdict(lang) for lang in languages.values()]
        return json.dumps(languages_list)

    @job.room.on("disconnected")
    def on_room_disconnected():
        """Handle room disconnection - cleanup all resources"""
        logger.info("ðŸšª Room disconnected, starting cleanup...")
        
        # Create async task for cleanup
        async def cleanup():
            # Log final resource statistics
            resource_manager.log_stats()
            
            # Shutdown resource manager (cancels all tasks, closes all streams)
            await resource_manager.shutdown()
            
            # Close database connections
            try:
                await close_database_connections()
                logger.info("âœ… Database connections closed")
                
                # Force cleanup of any remaining sessions
                import gc
                gc.collect()  # Force garbage collection
                await asyncio.sleep(0.1)  # Give time for cleanup
            except Exception as e:
                logger.debug(f"Database cleanup error: {e}")
            
            logger.info("âœ… Room cleanup completed")
        
        # Run cleanup in the event loop
        asyncio.create_task(cleanup())


async def request_fnc(req: JobRequest):
    logger.info(f"ðŸŽ¯ Received job request for room: {req.room.name if req.room else 'unknown'}")
    logger.info(f"ðŸ“‹ Request details: job_id={req.id}, room_name={req.room.name if req.room else 'unknown'}")
    await req.accept(
        name="agent",
        identity="agent",
    )
    logger.info(f"âœ… Accepted job request for room: {req.room.name if req.room else 'unknown'}")


if __name__ == "__main__":
    cli.run_app(
        WorkerOptions(
            entrypoint_fnc=entrypoint, prewarm_fnc=prewarm, request_fnc=request_fnc
        )
    )


================================================
FILE: backup_20250723_173804/resource_management.py
================================================
"""
Resource management module for LiveKit AI Translation Server.
Handles tracking and cleanup of async tasks, STT streams, and other resources.
"""
import asyncio
import logging
from typing import Set, List, Dict, Any, Optional, Callable
from contextlib import asynccontextmanager
from dataclasses import dataclass, field
from datetime import datetime
import weakref

logger = logging.getLogger("transcriber.resources")


@dataclass
class ResourceStats:
    """Statistics about managed resources."""
    tasks_created: int = 0
    tasks_completed: int = 0
    tasks_failed: int = 0
    tasks_cancelled: int = 0
    streams_opened: int = 0
    streams_closed: int = 0
    active_tasks: int = 0
    active_streams: int = 0


class TaskManager:
    """
    Manages async tasks with proper tracking and cleanup.
    
    Features:
    - Automatic task tracking
    - Graceful cancellation
    - Resource leak prevention
    - Statistics tracking
    """
    
    def __init__(self, name: str = "default"):
        self.name = name
        self._tasks: Set[asyncio.Task] = set()
        self._task_metadata: Dict[asyncio.Task, Dict[str, Any]] = {}
        self._stats = ResourceStats()
        self._cleanup_interval = 30.0  # seconds
        self._cleanup_task: Optional[asyncio.Task] = None
        self._shutdown = False
        logger.info(f"ðŸ“‹ TaskManager '{self.name}' initialized")
    
    async def __aenter__(self):
        """Context manager entry."""
        self._cleanup_task = asyncio.create_task(self._periodic_cleanup())
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit with cleanup."""
        await self.shutdown()
    
    def create_task(
        self, 
        coro, 
        name: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None
    ) -> asyncio.Task:
        """
        Create and track an async task.
        
        Args:
            coro: Coroutine to run
            name: Optional task name
            metadata: Optional metadata for the task
            
        Returns:
            Created task
        """
        if self._shutdown:
            raise RuntimeError("TaskManager is shutting down")
        
        task = asyncio.create_task(coro, name=name)
        self._tasks.add(task)
        
        if metadata:
            self._task_metadata[task] = metadata
        
        # Add callback to clean up when done
        task.add_done_callback(self._task_done_callback)
        
        self._stats.tasks_created += 1
        self._stats.active_tasks = len(self._tasks)
        
        logger.debug(f"ðŸ“Œ Created task: {name or task.get_name()} (total: {len(self._tasks)})")
        return task
    
    def _task_done_callback(self, task: asyncio.Task):
        """Callback when a task completes."""
        self._tasks.discard(task)
        self._task_metadata.pop(task, None)
        
        try:
            if task.cancelled():
                self._stats.tasks_cancelled += 1
                logger.debug(f"ðŸš« Task cancelled: {task.get_name()}")
            elif task.exception():
                self._stats.tasks_failed += 1
                logger.error(f"âŒ Task failed: {task.get_name()}", exc_info=task.exception())
            else:
                self._stats.tasks_completed += 1
                logger.debug(f"âœ… Task completed: {task.get_name()}")
        except Exception as e:
            logger.debug(f"Error in task callback: {e}")
        
        self._stats.active_tasks = len(self._tasks)
    
    async def _periodic_cleanup(self):
        """Periodically clean up completed tasks."""
        while not self._shutdown:
            try:
                await asyncio.sleep(self._cleanup_interval)
                
                # Clean up any references to completed tasks
                completed = [t for t in self._tasks if t.done()]
                for task in completed:
                    self._tasks.discard(task)
                    self._task_metadata.pop(task, None)
                
                if completed:
                    logger.debug(f"ðŸ§¹ Cleaned up {len(completed)} completed tasks")
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"Error in periodic cleanup: {e}")
    
    async def cancel_all(self, timeout: float = 5.0) -> int:
        """
        Cancel all active tasks.
        
        Args:
            timeout: Maximum time to wait for cancellation
            
        Returns:
            Number of tasks cancelled
        """
        if not self._tasks:
            return 0
        
        tasks_to_cancel = list(self._tasks)
        cancelled_count = 0
        
        logger.info(f"ðŸš« Cancelling {len(tasks_to_cancel)} tasks...")
        
        # Cancel all tasks
        for task in tasks_to_cancel:
            if not task.done():
                task.cancel()
                cancelled_count += 1
        
        # Wait for cancellation with timeout
        if tasks_to_cancel:
            try:
                await asyncio.wait_for(
                    asyncio.gather(*tasks_to_cancel, return_exceptions=True),
                    timeout=timeout
                )
            except asyncio.TimeoutError:
                logger.warning(f"â° Timeout waiting for {len(tasks_to_cancel)} tasks to cancel")
        
        logger.info(f"âœ… Cancelled {cancelled_count} tasks")
        return cancelled_count
    
    async def shutdown(self):
        """Shutdown the task manager and cleanup all resources."""
        if self._shutdown:
            return
        
        self._shutdown = True
        logger.info(f"ðŸ›‘ Shutting down TaskManager '{self.name}'...")
        
        # Cancel cleanup task
        if self._cleanup_task and not self._cleanup_task.done():
            self._cleanup_task.cancel()
            try:
                await self._cleanup_task
            except asyncio.CancelledError:
                pass
        
        # Cancel all managed tasks
        await self.cancel_all()
        
        logger.info(f"âœ… TaskManager '{self.name}' shutdown complete")
    
    def get_stats(self) -> ResourceStats:
        """Get current statistics."""
        return self._stats
    
    def get_active_tasks(self) -> List[asyncio.Task]:
        """Get list of active tasks."""
        return list(self._tasks)


class STTStreamManager:
    """
    Manages STT (Speech-to-Text) streams with proper cleanup.
    """
    
    def __init__(self):
        self._streams: Set[Any] = set()
        self._stream_metadata: Dict[Any, Dict[str, Any]] = {}
        self._stats = ResourceStats()
        logger.info("ðŸŽ¤ STTStreamManager initialized")
    
    @asynccontextmanager
    async def create_stream(self, stt_provider, participant_id: str):
        """
        Create and manage an STT stream.
        
        Args:
            stt_provider: STT provider instance
            participant_id: ID of the participant
            
        Yields:
            STT stream instance
        """
        stream = None
        try:
            # Create stream
            stream = stt_provider.stream()
            self._streams.add(stream)
            self._stream_metadata[stream] = {
                "participant_id": participant_id,
                "created_at": datetime.utcnow()
            }
            self._stats.streams_opened += 1
            self._stats.active_streams = len(self._streams)
            
            logger.info(f"ðŸŽ¤ Created STT stream for {participant_id}")
            yield stream
            
        finally:
            # Cleanup stream
            if stream:
                try:
                    await stream.aclose()
                    logger.info(f"âœ… STT stream closed for {participant_id}")
                except Exception as e:
                    logger.error(f"Error closing STT stream: {e}")
                finally:
                    self._streams.discard(stream)
                    self._stream_metadata.pop(stream, None)
                    self._stats.streams_closed += 1
                    self._stats.active_streams = len(self._streams)
    
    async def close_all(self):
        """Close all active streams."""
        if not self._streams:
            return
        
        logger.info(f"ðŸš« Closing {len(self._streams)} STT streams...")
        
        streams_to_close = list(self._streams)
        for stream in streams_to_close:
            try:
                await stream.aclose()
            except Exception as e:
                logger.error(f"Error closing stream: {e}")
            finally:
                self._streams.discard(stream)
                self._stream_metadata.pop(stream, None)
        
        self._stats.active_streams = 0
        logger.info("âœ… All STT streams closed")
    
    def get_stats(self) -> ResourceStats:
        """Get current statistics."""
        return self._stats


class ResourceManager:
    """
    Central resource manager for the application.
    Coordinates TaskManager and STTStreamManager.
    """
    
    def __init__(self):
        self.task_manager = TaskManager("main")
        self.stt_manager = STTStreamManager()
        self._shutdown_handlers: List[Callable] = []
        logger.info("ðŸ—ï¸ ResourceManager initialized")
    
    async def __aenter__(self):
        """Context manager entry."""
        await self.task_manager.__aenter__()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit with cleanup."""
        await self.shutdown()
    
    def add_shutdown_handler(self, handler: Callable):
        """Add a handler to be called on shutdown."""
        self._shutdown_handlers.append(handler)
    
    async def shutdown(self):
        """Shutdown all managed resources."""
        logger.info("ðŸ›‘ Starting ResourceManager shutdown...")
        
        # Run shutdown handlers
        for handler in self._shutdown_handlers:
            try:
                if asyncio.iscoroutinefunction(handler):
                    await handler()
                else:
                    handler()
            except Exception as e:
                logger.error(f"Error in shutdown handler: {e}")
        
        # Shutdown managers
        await self.task_manager.shutdown()
        await self.stt_manager.close_all()
        
        logger.info("âœ… ResourceManager shutdown complete")
    
    def get_all_stats(self) -> Dict[str, ResourceStats]:
        """Get statistics from all managers."""
        return {
            "tasks": self.task_manager.get_stats(),
            "stt_streams": self.stt_manager.get_stats()
        }
    
    def log_stats(self):
        """Log current resource statistics."""
        stats = self.get_all_stats()
        logger.info(
            f"ðŸ“Š Resource Stats - "
            f"Tasks: {stats['tasks'].active_tasks} active "
            f"({stats['tasks'].tasks_completed} completed, "
            f"{stats['tasks'].tasks_failed} failed, "
            f"{stats['tasks'].tasks_cancelled} cancelled), "
            f"STT Streams: {stats['stt_streams'].active_streams} active"
        )


================================================
FILE: backup_20250723_173804/translation_helpers.py
================================================
"""
Translation helper functions for LiveKit AI Translation Server.
Handles translation orchestration and related utilities.
"""
import asyncio
import logging
from typing import List, Dict, Any, Optional

logger = logging.getLogger("transcriber.translation")


async def translate_sentences(
    sentences: List[str], 
    translators: Dict[str, Any],
    source_language: str = "ar"
) -> None:
    """
    Translate complete sentences to all target languages.
    
    This function takes a list of sentences and sends them to all available
    translators concurrently for better performance.
    
    Args:
        sentences: List of sentences to translate
        translators: Dictionary of language code to translator instances
        source_language: Source language code (default: "ar" for Arabic)
    """
    if not sentences or not translators:
        return
        
    for sentence in sentences:
        if sentence.strip():
            logger.info(f"ðŸŽ¯ TRANSLATING COMPLETE {source_language.upper()} SENTENCE: '{sentence}'")
            logger.info(f"ðŸ“Š Processing sentence for {len(translators)} translators")
            
            # Send to all translators concurrently for better performance
            translation_tasks = []
            for lang, translator in translators.items():
                logger.info(f"ðŸ“¤ Sending complete {source_language.upper()} sentence '{sentence}' to {lang} translator")
                translation_tasks.append(translator.translate(sentence, None))
            
            # Execute all translations concurrently
            if translation_tasks:
                results = await asyncio.gather(*translation_tasks, return_exceptions=True)
                # Check for any exceptions
                for i, result in enumerate(results):
                    if isinstance(result, Exception):
                        logger.error(f"âŒ Translation failed: {result}")


async def translate_single_sentence(
    sentence: str,
    translator: Any,
    target_language: str
) -> Optional[str]:
    """
    Translate a single sentence to a specific target language.
    
    Args:
        sentence: The sentence to translate
        translator: The translator instance to use
        target_language: Target language code
        
    Returns:
        Translated text or None if translation failed
    """
    try:
        if not sentence.strip():
            return None
            
        logger.debug(f"Translating to {target_language}: '{sentence}'")
        result = await translator.translate(sentence, None)
        return result
    except Exception as e:
        logger.error(f"Translation to {target_language} failed: {e}")
        return None


def should_translate_text(text: str, min_length: int = 3) -> bool:
    """
    Determine if text should be translated based on various criteria.
    
    Args:
        text: The text to evaluate
        min_length: Minimum length for translation (default: 3 characters)
        
    Returns:
        True if text should be translated
    """
    if not text or not text.strip():
        return False
    
    # Don't translate very short text
    if len(text.strip()) < min_length:
        return False
    
    # Don't translate if it's only punctuation
    if all(c in '.!?ØŸ,ØŒ;Ø›:' for c in text.strip()):
        return False
    
    return True


def format_translation_output(
    original_text: str,
    translated_text: str,
    source_lang: str,
    target_lang: str
) -> Dict[str, str]:
    """
    Format translation output for consistent structure.
    
    Args:
        original_text: Original text
        translated_text: Translated text
        source_lang: Source language code
        target_lang: Target language code
        
    Returns:
        Formatted translation dictionary
    """
    return {
        "original": original_text,
        "translated": translated_text,
        "source_language": source_lang,
        "target_language": target_lang,
        "type": "translation"
    }


async def batch_translate(
    texts: List[str],
    translators: Dict[str, Any],
    batch_size: int = 5
) -> Dict[str, List[str]]:
    """
    Translate multiple texts in batches for efficiency.
    
    Args:
        texts: List of texts to translate
        translators: Dictionary of language code to translator instances
        batch_size: Number of texts to process in each batch
        
    Returns:
        Dictionary mapping language codes to lists of translations
    """
    results = {lang: [] for lang in translators.keys()}
    
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]
        
        # Process each batch
        for text in batch:
            if should_translate_text(text):
                await translate_sentences([text], translators)
    
    return results


================================================
FILE: backup_20250723_173804/translator.py
================================================
"""
Translator module for LiveKit AI Translation Server.
Handles real-time translation with context management and error handling.
"""
import asyncio
import logging
from typing import Optional, Dict, Any, List, Callable
from collections import deque
from enum import Enum

from livekit import rtc
from livekit.agents import llm, utils
from livekit.plugins import openai

from config import get_config
from prompt_builder import get_prompt_builder

logger = logging.getLogger("transcriber.translator")
config = get_config()
prompt_builder = get_prompt_builder()


class TranslationError(Exception):
    """Custom exception for translation-related errors."""
    pass


class Translator:
    """
    Handles translation from source language to target language with context management.
    
    Features:
    - Sliding window context for better translation coherence
    - Automatic retry on failures
    - Comprehensive error handling
    - Real-time broadcasting to displays
    """
    
    # Class-level configuration from config module
    use_context = config.translation.use_context
    max_context_pairs = config.translation.max_context_pairs
    
    def __init__(self, room: rtc.Room, lang: Enum, tenant_context: Optional[Dict[str, Any]] = None, broadcast_callback: Optional[Callable] = None):
        """
        Initialize the Translator.
        
        Args:
            room: LiveKit room instance
            lang: Target language enum
            tenant_context: Optional context containing room_id, mosque_id, etc.
            broadcast_callback: Optional callback function for broadcasting translations
        """
        self.room = room
        self.lang = lang
        self.tenant_context = tenant_context or {}
        self.broadcast_callback = broadcast_callback
        self.llm = openai.LLM()
        
        # Initialize system prompt as None - will be built dynamically
        self.system_prompt = None
        self._prompt_template = None
        
        # Use deque for automatic sliding window (old messages auto-removed)
        if self.use_context:
            self.message_history: deque = deque(maxlen=(self.max_context_pairs * 2))
        
        # Track translation statistics
        self.translation_count = 0
        self.error_count = 0
        
        # Log the context mode being used
        context_mode = f"TRUE SLIDING WINDOW ({self.max_context_pairs}-pair memory)" if self.use_context else "FRESH CONTEXT (no memory)"
        logger.info(f"ðŸ§  Translator initialized for {lang.value} with {context_mode} mode")
        
        # Initialize prompt asynchronously on first use
        self._prompt_initialized = False

    async def translate(self, message: str, track: Optional[rtc.Track] = None, max_retries: int = 2) -> str:
        """
        Translate a message from source to target language.
        
        Args:
            message: Text to translate
            track: Optional audio track reference
            max_retries: Maximum number of retry attempts on failure
            
        Returns:
            Translated text (empty string on failure)
            
        Raises:
            TranslationError: If translation fails after all retries
        """
        if not message or not message.strip():
            logger.debug("Empty message, skipping translation")
            return ""
        
        retry_count = 0
        last_error = None
        
        while retry_count <= max_retries:
            try:
                translated_message = await self._perform_translation(message)
                
                if translated_message:
                    # Publish transcription to LiveKit room
                    await self._publish_transcription(translated_message, track)
                    
                    # Broadcast to displays
                    await self._broadcast_translation(translated_message)
                    
                    # Update statistics
                    self.translation_count += 1
                    
                    # Log successful translation
                    logger.info(f"âœ… Translated to {self.lang.value}: '{message}' â†’ '{translated_message}'")
                    
                    return translated_message
                else:
                    logger.warning(f"Empty translation result for: '{message}'")
                    return ""
                    
            except Exception as e:
                last_error = e
                retry_count += 1
                self.error_count += 1
                
                if retry_count <= max_retries:
                    wait_time = retry_count * 0.5  # Exponential backoff
                    logger.warning(
                        f"Translation attempt {retry_count} failed: {e}. "
                        f"Retrying in {wait_time}s..."
                    )
                    await asyncio.sleep(wait_time)
                else:
                    logger.error(
                        f"âŒ Translation failed after {max_retries} retries: {e}\n"
                        f"Message: '{message}'"
                    )
                    
        # If we get here, all retries failed
        error_msg = f"Translation failed for '{message}' after {max_retries} retries"
        if last_error:
            error_msg += f": {last_error}"
        
        # Don't raise exception - return empty string to keep stream going
        logger.error(error_msg)
        return ""

    async def _initialize_prompt(self):
        """Initialize the system prompt using the prompt builder."""
        if self._prompt_initialized:
            return
            
        try:
            # Get room ID from tenant context
            room_id = self.tenant_context.get('room_id')
            
            # Get source language from room config
            source_lang_code = self.tenant_context.get('transcription_language', 'ar')
            source_lang_name = config.translation.supported_languages.get(
                source_lang_code, 
                {"name": "Arabic"}
            )["name"]
            
            # Build the prompt using prompt builder
            self.system_prompt = await prompt_builder.get_prompt_for_room(
                room_id=room_id,
                source_lang=source_lang_name,
                target_lang=self.lang.value,
                room_config=self.tenant_context
            )
            
            logger.info(f"ðŸ“ Initialized translation prompt for room {room_id}: {source_lang_name} â†’ {self.lang.value}")
            self._prompt_initialized = True
            
        except Exception as e:
            logger.error(f"Failed to initialize prompt: {e}")
            # Fallback to default prompt with dynamic source language
            source_lang_code = self.tenant_context.get('transcription_language', 'ar')
            source_lang_name = config.translation.supported_languages.get(
                source_lang_code, 
                {"name": "Arabic"}
            )["name"]
            
            self.system_prompt = (
                f"You are an expert simultaneous interpreter. Your task is to translate from {source_lang_name} to {self.lang.value}. "
                f"Provide a direct and accurate translation of the user's input. Be concise and use natural-sounding language. "
                f"Do not add any additional commentary, explanations, or introductory phrases."
            )
            self._prompt_initialized = True
    
    async def _perform_translation(self, message: str) -> str:
        """
        Perform the actual translation using the LLM.
        
        Args:
            message: Text to translate
            
        Returns:
            Translated text
        """
        # Ensure prompt is initialized
        await self._initialize_prompt()
        # Build a fresh context for every translation (rebuild method)
        temp_context = llm.ChatContext()
        temp_context.add_message(role="system", content=self.system_prompt)
        
        # If using context, add the message history from our deque
        if self.use_context and hasattr(self, 'message_history'):
            logger.debug(f"ðŸ”„ Building context with {len(self.message_history)} historical messages")
            for msg in self.message_history:
                temp_context.add_message(role=msg['role'], content=msg['content'])
        
        # Add the current message to translate
        temp_context.add_message(content=message, role="user")
        
        # Get translation from LLM with the freshly built context
        stream = self.llm.chat(chat_ctx=temp_context)
        
        translated_message = ""
        async for chunk in stream:
            if chunk.delta is None:
                continue
            content = chunk.delta.content
            if content is None:
                break
            translated_message += content
        
        # If using context, update our history (deque will auto-remove old messages)
        if self.use_context and translated_message:
            self.message_history.append({"role": "user", "content": message})
            self.message_history.append({"role": "assistant", "content": translated_message})
            logger.debug(f"ðŸ’¾ History updated. Current size: {len(self.message_history)} messages")
        
        return translated_message

    async def _publish_transcription(self, translated_text: str, track: Optional[rtc.Track]) -> None:
        """
        Publish the translation as a transcription to the LiveKit room.
        
        Args:
            translated_text: The translated text to publish
            track: Optional track reference
        """
        try:
            segment = rtc.TranscriptionSegment(
                id=utils.misc.shortuuid("SG_"),
                text=translated_text,
                start_time=0,
                end_time=0,
                language=self.lang.value,
                final=True,
            )
            transcription = rtc.Transcription(
                self.room.local_participant.identity, 
                track.sid if track else "", 
                [segment]
            )
            await self.room.local_participant.publish_transcription(transcription)
            logger.debug(f"ðŸ“¤ Published {self.lang.value} transcription to LiveKit room")
        except Exception as e:
            logger.error(f"Failed to publish transcription: {e}")
            # Don't re-raise - translation was successful even if publishing failed

    async def _broadcast_translation(self, translated_text: str) -> None:
        """
        Broadcast the translation to WebSocket displays.
        
        Args:
            translated_text: The translated text to broadcast
        """
        if self.broadcast_callback:
            try:
                # Use asyncio.create_task to avoid blocking
                asyncio.create_task(
                    self.broadcast_callback(
                        "translation", 
                        self.lang.value, 
                        translated_text, 
                        self.tenant_context
                    )
                )
                logger.debug(f"ðŸ“¡ Broadcasted {self.lang.value} translation to displays")
            except Exception as e:
                logger.error(f"Failed to broadcast translation: {e}")
                # Don't re-raise - translation was successful even if broadcasting failed
        else:
            logger.debug("No broadcast callback provided, skipping broadcast")

    def get_statistics(self) -> Dict[str, Any]:
        """
        Get translation statistics.
        
        Returns:
            Dictionary containing translation stats
        """
        return {
            "language": self.lang.value,
            "translation_count": self.translation_count,
            "error_count": self.error_count,
            "error_rate": self.error_count / max(1, self.translation_count),
            "context_enabled": self.use_context,
            "context_size": len(self.message_history) if self.use_context else 0
        }

    def clear_context(self) -> None:
        """Clear the translation context history."""
        if self.use_context and hasattr(self, 'message_history'):
            self.message_history.clear()
            logger.info(f"ðŸ§¹ Cleared translation context for {self.lang.value}")

    def __repr__(self) -> str:
        """String representation of the Translator."""
        return (
            f"Translator(lang={self.lang.value}, "
            f"context={self.use_context}, "
            f"translations={self.translation_count}, "
            f"errors={self.error_count})"
        )


================================================
FILE: backup_20250727_213400/broadcasting.py
================================================
"""
Broadcasting module for LiveKit AI Translation Server.
Handles real-time broadcasting of transcriptions and translations to displays.
"""
import asyncio
import logging
import hashlib
import uuid
from typing import Optional, Dict, Any
from datetime import datetime

from config import get_config
from database import broadcast_to_channel, store_transcript_in_database

logger = logging.getLogger("transcriber.broadcasting")
config = get_config()


class BroadcastError(Exception):
    """Custom exception for broadcasting-related errors."""
    pass


async def broadcast_to_displays(
    message_type: str, 
    language: str, 
    text: str, 
    tenant_context: Optional[Dict[str, Any]] = None,
    sentence_context: Optional[Dict[str, Any]] = None
) -> bool:
    """
    Send transcription/translation to frontend via Supabase Broadcast and store in database.
    
    This function handles both real-time broadcasting and database storage of
    transcriptions and translations. It uses Supabase's broadcast feature for
    real-time updates and stores the data for persistence.
    
    Args:
        message_type: Type of message ("transcription" or "translation")
        language: Language code (e.g., "ar", "nl")
        text: The text content to broadcast
        tenant_context: Optional context containing room_id, mosque_id, etc.
        sentence_context: Optional context for sentence tracking (sentence_id, is_complete, etc.)
        
    Returns:
        bool: True if broadcast was successful, False otherwise
    """
    if not text or not text.strip():
        logger.debug("Empty text provided, skipping broadcast")
        return False
    
    success = False
    
    # Phase 1: Immediate broadcast via Supabase for real-time display
    if tenant_context and tenant_context.get("room_id") and tenant_context.get("mosque_id"):
        try:
            channel_name = f"live-transcription-{tenant_context['room_id']}-{tenant_context['mosque_id']}"
            
            # Generate unique message ID based on timestamp and content hash
            timestamp = datetime.utcnow().isoformat() + "Z"
            text_hash = hashlib.md5(text.encode('utf-8')).hexdigest()[:8]
            msg_id = f"{timestamp}_{text_hash}"
            
            # Build payload with optional sentence context
            data_payload = {
                "text": text,
                "language": language,
                "timestamp": timestamp,
                "msg_id": msg_id
            }
            
            # Add sentence context if provided
            if sentence_context:
                data_payload.update({
                    "sentence_id": sentence_context.get("sentence_id"),
                    "is_complete": sentence_context.get("is_complete", False),
                    "is_fragment": sentence_context.get("is_fragment", True)
                })
            
            payload = {
                "type": message_type,
                "room_id": tenant_context["room_id"],
                "mosque_id": tenant_context["mosque_id"],
                "data": data_payload
            }
            
            # Use the broadcast_to_channel function from database module
            success = await broadcast_to_channel(channel_name, message_type, payload)
            
            if success:
                logger.info(
                    f"ðŸ“¡ LIVE: Sent {message_type} ({language}) via Supabase broadcast: "
                    f"{text[:50]}{'...' if len(text) > 50 else ''}"
                )
            else:
                logger.warning(f"âš ï¸ Failed to broadcast {message_type} to Supabase")
                
        except Exception as e:
            logger.error(f"âŒ Broadcast error: {e}")
            success = False
    else:
        logger.warning("âš ï¸ Missing tenant context for Supabase broadcast")
    
    # Phase 2: Direct database storage (no batching)
    if tenant_context and tenant_context.get("room_id") and tenant_context.get("mosque_id"):
        try:
            # Store directly in database using existing function
            # Use create_task to avoid blocking the broadcast with proper error handling
            task = asyncio.create_task(
                _store_with_error_handling(message_type, language, text, tenant_context)
            )
            task.add_done_callback(lambda t: None if not t.exception() else logger.error(f"Storage task failed: {t.exception()}"))
            logger.debug(
                f"ðŸ’¾ DIRECT: Storing {message_type} directly to database "
                f"for room {tenant_context['room_id']}"
            )
        except Exception as e:
            logger.error(f"âŒ Failed to initiate database storage: {e}")
    else:
        logger.warning("âš ï¸ Missing tenant context for database storage")
    
    return success


async def _store_with_error_handling(
    message_type: str, 
    language: str, 
    text: str, 
    tenant_context: Dict[str, Any]
) -> None:
    """
    Store transcript with proper error handling.
    
    This is a wrapper around store_transcript_in_database that ensures
    errors don't propagate and crash the application.
    
    Args:
        message_type: Type of message ("transcription" or "translation")
        language: Language code
        text: The text content to store
        tenant_context: Context containing room_id, mosque_id, etc.
    """
    try:
        success = await store_transcript_in_database(
            message_type, language, text, tenant_context
        )
        if not success:
            logger.warning(
                f"âš ï¸ Failed to store {message_type} in database for "
                f"room {tenant_context.get('room_id')}"
            )
    except Exception as e:
        logger.error(
            f"âŒ Database storage error for {message_type}: {e}\n"
            f"Room: {tenant_context.get('room_id')}, "
            f"Language: {language}"
        )


async def broadcast_batch(
    messages: list[tuple[str, str, str, Dict[str, Any]]]
) -> Dict[str, int]:
    """
    Broadcast multiple messages in batch for efficiency.
    
    Args:
        messages: List of tuples (message_type, language, text, tenant_context)
        
    Returns:
        Dictionary with counts of successful and failed broadcasts
    """
    results = {"success": 0, "failed": 0}
    
    # Process all broadcasts concurrently
    tasks = []
    for message_type, language, text, tenant_context in messages:
        task = broadcast_to_displays(message_type, language, text, tenant_context)
        tasks.append(task)
    
    # Wait for all broadcasts to complete
    broadcast_results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # Count results
    for result in broadcast_results:
        if isinstance(result, Exception):
            results["failed"] += 1
            logger.error(f"Batch broadcast error: {result}")
        elif result:
            results["success"] += 1
        else:
            results["failed"] += 1
    
    logger.info(
        f"ðŸ“Š Batch broadcast complete: "
        f"{results['success']} successful, {results['failed']} failed"
    )
    
    return results


def create_broadcast_payload(
    message_type: str,
    language: str,
    text: str,
    room_id: int,
    mosque_id: int,
    additional_data: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """
    Create a standardized broadcast payload.
    
    Args:
        message_type: Type of message
        language: Language code
        text: The text content
        room_id: Room ID
        mosque_id: Mosque ID
        additional_data: Optional additional data to include
        
    Returns:
        Formatted payload dictionary
    """
    # Generate unique message ID
    timestamp = datetime.utcnow().isoformat() + "Z"
    text_hash = hashlib.md5(text.encode('utf-8')).hexdigest()[:8]
    msg_id = f"{timestamp}_{text_hash}"
    
    payload = {
        "type": message_type,
        "room_id": room_id,
        "mosque_id": mosque_id,
        "data": {
            "text": text,
            "language": language,
            "timestamp": timestamp,
            "msg_id": msg_id
        }
    }
    
    if additional_data:
        payload["data"].update(additional_data)
    
    return payload


def get_channel_name(room_id: int, mosque_id: int) -> str:
    """
    Generate the channel name for a room.
    
    Args:
        room_id: Room ID
        mosque_id: Mosque ID
        
    Returns:
        Channel name string
    """
    return f"live-transcription-{room_id}-{mosque_id}"


================================================
FILE: backup_20250727_213400/config.py
================================================
"""
Configuration management for LiveKit AI Translation Server.
Centralizes all configuration values and environment variables.
"""
import os
from dataclasses import dataclass
from typing import Optional, Dict
# Try to load dotenv if available
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    # dotenv not available, will use system environment variables
    pass


@dataclass
class SupabaseConfig:
    """Supabase database configuration."""
    url: str
    service_role_key: str
    anon_key: Optional[str] = None
    
    # Timeouts
    http_timeout: float = 5.0  # General HTTP request timeout
    broadcast_timeout: float = 2.0  # Broadcast API timeout
    
    @classmethod
    def from_env(cls) -> 'SupabaseConfig':
        """Load configuration from environment variables."""
        url = os.getenv('SUPABASE_URL')
        service_key = os.getenv('SUPABASE_SERVICE_ROLE_KEY')
        anon_key = os.getenv('SUPABASE_ANON_KEY')
        
        if not url:
            raise ValueError("SUPABASE_URL environment variable is required")
        if not service_key:
            raise ValueError("SUPABASE_SERVICE_ROLE_KEY environment variable is required")
            
        return cls(
            url=url,
            service_role_key=service_key,
            anon_key=anon_key
        )


@dataclass
class TranslationConfig:
    """Translation-related configuration."""
    # Language settings
    default_source_language: str = "ar"  # Arabic
    default_target_language: str = "nl"  # Dutch
    
    # Context window settings
    use_context: bool = True
    max_context_pairs: int = 6  # Number of translation pairs to keep in memory
    
    # Timing settings
    translation_delay: float = 10.0  # Delay before translating incomplete sentences
    
    # Supported languages
    supported_languages: Dict[str, Dict[str, str]] = None
    
    def __post_init__(self):
        if self.supported_languages is None:
            self.supported_languages = {
                "ar": {"name": "Arabic", "flag": "ðŸ‡¸ðŸ‡¦"},
                "en": {"name": "English", "flag": "ðŸ‡ºðŸ‡¸"},
                "es": {"name": "Spanish", "flag": "ðŸ‡ªðŸ‡¸"},
                "fr": {"name": "French", "flag": "ðŸ‡«ðŸ‡·"},
                "de": {"name": "German", "flag": "ðŸ‡©ðŸ‡ª"},
                "ja": {"name": "Japanese", "flag": "ðŸ‡¯ðŸ‡µ"},
                "nl": {"name": "Dutch", "flag": "ðŸ‡³ðŸ‡±"},
            }
    
    def get_target_language(self, room_config: Optional[Dict[str, any]] = None) -> str:
        """Get target language from room config or use default."""
        if room_config:
            # Check both possible field names (translation_language and translation__language)
            if 'translation_language' in room_config and room_config['translation_language']:
                return room_config['translation_language']
            elif 'translation__language' in room_config and room_config['translation__language']:
                return room_config['translation__language']
        return self.default_target_language
    
    def get_source_language(self, room_config: Optional[Dict[str, any]] = None) -> str:
        """Get source language from room config or use default."""
        if room_config and 'transcription_language' in room_config and room_config['transcription_language']:
            return room_config['transcription_language']
        return self.default_source_language


@dataclass
class SpeechmaticsConfig:
    """Speechmatics STT configuration."""
    language: str = "ar"
    operating_point: str = "enhanced"
    enable_partials: bool = True
    max_delay: float = 2.0
    punctuation_sensitivity: float = 0.5
    diarization: str = "speaker"
    
    def with_room_settings(self, room_config: Optional[Dict[str, any]] = None) -> 'SpeechmaticsConfig':
        """Create a new config with room-specific overrides."""
        if not room_config:
            return self
            
        # Create a copy with room-specific overrides
        import copy
        new_config = copy.deepcopy(self)
        
        # Override with room settings if available
        if 'transcription_language' in room_config and room_config['transcription_language']:
            new_config.language = room_config['transcription_language']
        if 'max_delay' in room_config and room_config['max_delay'] is not None:
            new_config.max_delay = float(room_config['max_delay'])
        if 'punctuation_sensitivity' in room_config and room_config['punctuation_sensitivity'] is not None:
            new_config.punctuation_sensitivity = float(room_config['punctuation_sensitivity'])
            
        return new_config


@dataclass
class ApplicationConfig:
    """Main application configuration."""
    # Component configurations
    supabase: SupabaseConfig
    translation: TranslationConfig
    speechmatics: SpeechmaticsConfig
    
    # Logging
    log_level: str = "INFO"
    
    # Testing/Development
    default_mosque_id: int = 1
    test_mosque_id: int = 546012  # Hardcoded test mosque
    test_room_id: int = 192577    # Hardcoded test room
    
    @classmethod
    def load(cls) -> 'ApplicationConfig':
        """Load complete configuration from environment and defaults."""
        return cls(
            supabase=SupabaseConfig.from_env(),
            translation=TranslationConfig(),
            speechmatics=SpeechmaticsConfig()
        )
    
    def validate(self) -> None:
        """Validate configuration at startup."""
        # Print configuration status
        print("ðŸ”§ Configuration loaded:")
        print(f"   SUPABASE_URL: {self.supabase.url[:50]}...")
        print(f"   SERVICE_KEY: {'âœ… SET' if self.supabase.service_role_key else 'âŒ NOT SET'}")
        print(f"   Default Languages: {self.translation.default_source_language} â†’ {self.translation.default_target_language}")
        print(f"   Context Window: {'âœ… ENABLED' if self.translation.use_context else 'âŒ DISABLED'} ({self.translation.max_context_pairs} pairs)")
        print(f"   STT Defaults: delay={self.speechmatics.max_delay}s, punctuation={self.speechmatics.punctuation_sensitivity}")


# Global configuration instance
_config: Optional[ApplicationConfig] = None


def get_config() -> ApplicationConfig:
    """Get or create the global configuration instance."""
    global _config
    if _config is None:
        _config = ApplicationConfig.load()
        _config.validate()
    return _config


def reset_config() -> None:
    """Reset configuration (mainly for testing)."""
    global _config
    _config = None


================================================
FILE: backup_20250727_213400/database.py
================================================
"""
Database operations for LiveKit AI Translation Server.
Handles all Supabase database interactions with connection pooling and async support.
FIXED: Thread-safe connection pool that works with LiveKit's multi-process architecture.
"""
import asyncio
import logging
import uuid
from typing import Optional, Dict, Any
from datetime import datetime
import aiohttp
from contextlib import asynccontextmanager
import threading

from config import get_config

logger = logging.getLogger("transcriber.database")
config = get_config()


class ThreadSafeDatabasePool:
    """Thread-safe database connection pool that creates separate pools per thread/process."""
    
    def __init__(self, max_connections: int = 10):
        self.max_connections = max_connections
        self._local = threading.local()
        self._lock = threading.Lock()
        
    async def get_session(self) -> aiohttp.ClientSession:
        """Get or create a session for the current thread."""
        # Check if current thread has a session
        if not hasattr(self._local, 'session') or self._local.session is None or self._local.session.closed:
            # Create new session for this thread
            connector = aiohttp.TCPConnector(
                limit=self.max_connections,
                limit_per_host=self.max_connections,
                force_close=True  # Force close to avoid connection issues
            )
            self._local.session = aiohttp.ClientSession(
                connector=connector,
                trust_env=True  # Trust environment proxy settings
            )
            logger.debug(f"Created new connection pool for thread {threading.current_thread().ident}")
        
        return self._local.session
    
    async def close(self):
        """Close the session for current thread."""
        if hasattr(self._local, 'session') and self._local.session and not self._local.session.closed:
            await self._local.session.close()
            self._local.session = None
            logger.debug(f"Closed connection pool for thread {threading.current_thread().ident}")


# Use thread-safe pool
_pool = ThreadSafeDatabasePool()


@asynccontextmanager
async def get_db_headers():
    """Get headers for Supabase API requests."""
    if not config.supabase.service_role_key:
        raise ValueError("SUPABASE_SERVICE_ROLE_KEY not configured")
    
    yield {
        'apikey': config.supabase.service_role_key,
        'Authorization': f'Bearer {config.supabase.service_role_key}',
        'Content-Type': 'application/json'
    }


async def ensure_active_session(room_id: int, mosque_id: int) -> Optional[str]:
    """
    Ensure there's an active session for the room and return session_id.
    
    This function:
    1. Checks for existing active sessions
    2. Creates a new session if none exists
    3. Returns the session ID or None on failure
    """
    try:
        # Get session from thread-safe pool
        session = await _pool.get_session()
        
        async with get_db_headers() as headers:
            # Check for existing active session
            url = f"{config.supabase.url}/rest/v1/room_sessions"
            params = {
                "room_id": f"eq.{room_id}",
                "status": "eq.active",
                "select": "id,started_at",
                "order": "started_at.desc",
                "limit": "1"
            }
            
            timeout = aiohttp.ClientTimeout(total=config.supabase.http_timeout)
            
            try:
                async with session.get(url, headers=headers, params=params, timeout=timeout) as response:
                    if response.status == 200:
                        sessions = await response.json()
                        if sessions and len(sessions) > 0:
                            session_id = sessions[0]["id"]
                            logger.debug(f"ðŸ“ Using existing active session: {session_id}")
                            return session_id
                    else:
                        error_text = await response.text()
                        logger.warning(f"Failed to check existing sessions: {response.status} - {error_text}")
            except asyncio.TimeoutError:
                logger.warning("Timeout checking for existing sessions")
            except Exception as e:
                logger.error(f"Error checking sessions: {e}")
            
            # Create new session if none exists
            new_session_id = str(uuid.uuid4())
            session_data = {
                "id": new_session_id,
                "room_id": room_id,
                "mosque_id": mosque_id,
                "status": "active",
                "started_at": datetime.utcnow().isoformat() + "Z",
                "logging_enabled": True
            }
            
            try:
                async with session.post(
                    url,
                    json=session_data,
                    headers={**headers, 'Prefer': 'return=minimal'},
                    timeout=timeout
                ) as response:
                    if response.status in [200, 201]:
                        logger.info(f"ðŸ“ Created new session: {new_session_id}")
                        return new_session_id
                    else:
                        error_text = await response.text()
                        logger.error(f"âŒ Failed to create session: {response.status} - {error_text}")
                        return None
            except asyncio.TimeoutError:
                logger.error("Timeout creating new session")
                return None
            except Exception as e:
                logger.error(f"Error creating session: {e}")
                return None
                    
    except Exception as e:
        logger.error(f"âŒ Session management failed: {e}")
        return None


async def store_transcript_in_database(
    message_type: str, 
    language: str, 
    text: str, 
    tenant_context: Dict[str, Any]
) -> bool:
    """
    Store transcription/translation in Supabase database.
    
    Args:
        message_type: Either "transcription" or "translation"
        language: Language code (e.g., "ar", "nl")
        text: The text to store
        tenant_context: Context containing room_id, mosque_id, session_id
        
    Returns:
        bool: True if successful, False otherwise
    """
    try:
        if not config.supabase.service_role_key:
            logger.error("âŒ SUPABASE_SERVICE_ROLE_KEY not found - cannot store transcripts")
            return False
            
        room_id = tenant_context.get("room_id")
        mosque_id = tenant_context.get("mosque_id")
        session_id = tenant_context.get("session_id")
        
        if not room_id or not mosque_id:
            logger.warning(f"âš ï¸ Missing room context: room_id={room_id}, mosque_id={mosque_id}")
            return False
            
        # Ensure we have an active session
        if not session_id:
            session_id = await ensure_active_session(room_id, mosque_id)
            if session_id:
                tenant_context["session_id"] = session_id
            else:
                logger.error("âŒ Could not establish session - skipping database storage")
                return False
        
        # Prepare transcript data
        transcript_data = {
            "room_id": room_id,
            "session_id": session_id,
            "timestamp": datetime.utcnow().isoformat() + "Z",
        }
        
        # Set appropriate field based on message type
        if message_type == "transcription":
            transcript_data["transcription_segment"] = text
        else:  # translation
            transcript_data["translation_segment"] = text
            
        # Store in database
        session = await _pool.get_session()
        async with get_db_headers() as headers:
            timeout = aiohttp.ClientTimeout(total=config.supabase.http_timeout)
            
            try:
                async with session.post(
                    f"{config.supabase.url}/rest/v1/transcripts",
                    json=transcript_data,
                    headers={**headers, 'Prefer': 'return=minimal'},
                    timeout=timeout
                ) as response:
                    if response.status in [200, 201]:
                        logger.debug(f"âœ… Stored {message_type} in database: room_id={room_id}, session_id={session_id[:8]}")
                        return True
                    else:
                        error_text = await response.text()
                        logger.warning(f"âš ï¸ Database storage failed with status {response.status}: {error_text}")
                        return False
            except asyncio.TimeoutError:
                logger.warning("Timeout storing transcript")
                return False
            except Exception as e:
                logger.error(f"Error storing transcript: {e}")
                return False
                    
    except Exception as e:
        logger.error(f"âŒ Database storage error: {e}")
        return False


async def query_room_by_name(room_name: str) -> Optional[Dict[str, Any]]:
    """
    Query room information by LiveKit room name.
    
    Args:
        room_name: The LiveKit room name
        
    Returns:
        Room data dictionary or None if not found
    """
    try:
        session = await _pool.get_session()
        async with get_db_headers() as headers:
            url = f"{config.supabase.url}/rest/v1/rooms"
            params = {"Livekit_room_name": f"eq.{room_name}"}
            
            timeout = aiohttp.ClientTimeout(total=config.supabase.http_timeout)
            
            try:
                async with session.get(url, headers=headers, params=params, timeout=timeout) as response:
                    if response.status == 200:
                        rooms = await response.json()
                        if rooms and len(rooms) > 0:
                            return rooms[0]
                    else:
                        error_text = await response.text()
                        logger.warning(f"Failed to query room: {response.status} - {error_text}")
            except asyncio.TimeoutError:
                logger.warning("Timeout querying room")
            except Exception as e:
                logger.error(f"Error querying room: {e}")
        
        return None
        
    except Exception as e:
        logger.error(f"âŒ Room query failed: {e}")
        return None


async def get_active_session_for_room(room_id: int) -> Optional[str]:
    """
    Get the active session ID for a room if one exists.
    
    Args:
        room_id: The room ID
        
    Returns:
        Session ID or None if no active session
    """
    try:
        session = await _pool.get_session()
        async with get_db_headers() as headers:
            url = f"{config.supabase.url}/rest/v1/room_sessions"
            params = {
                "room_id": f"eq.{room_id}",
                "status": "eq.active",
                "select": "id",
                "order": "started_at.desc",
                "limit": "1"
            }
            
            timeout = aiohttp.ClientTimeout(total=config.supabase.http_timeout)
            
            try:
                async with session.get(url, headers=headers, params=params, timeout=timeout) as response:
                    if response.status == 200:
                        sessions = await response.json()
                        if sessions and len(sessions) > 0:
                            return sessions[0].get("id")
            except asyncio.TimeoutError:
                logger.warning("Timeout getting active session")
            except Exception as e:
                logger.error(f"Error getting active session: {e}")
        
        return None
        
    except Exception as e:
        logger.error(f"âŒ Active session query failed: {e}")
        return None


async def broadcast_to_channel(
    channel_name: str,
    event_type: str,
    payload: Dict[str, Any]
) -> bool:
    """
    Broadcast a message to a Supabase channel.
    
    Args:
        channel_name: The channel to broadcast to
        event_type: The event type (e.g., "transcription", "translation")
        payload: The data to broadcast
        
    Returns:
        bool: True if successful, False otherwise
    """
    try:
        if not config.supabase.service_role_key:
            logger.warning("âš ï¸ SUPABASE_SERVICE_ROLE_KEY not found - skipping broadcast")
            return False
            
        session = await _pool.get_session()
        
        async with get_db_headers() as headers:
            # Use broadcast-specific timeout
            broadcast_timeout = aiohttp.ClientTimeout(total=config.supabase.broadcast_timeout)
            
            try:
                async with session.post(
                    f"{config.supabase.url}/functions/v1/broadcast",
                    json={
                        "channel": channel_name,
                        "event": event_type,
                        "payload": payload
                    },
                    headers=headers,
                    timeout=broadcast_timeout
                ) as response:
                    if response.status == 200:
                        return True
                    else:
                        error_text = await response.text()
                        logger.warning(f"âš ï¸ Broadcast failed: {response.status} - {error_text}")
                        return False
            except asyncio.TimeoutError:
                logger.warning(f"âš ï¸ Broadcast timeout for channel {channel_name}")
                return False
            except Exception as e:
                logger.error(f"Error broadcasting: {e}")
                return False
                    
    except Exception as e:
        logger.error(f"âŒ Broadcast error: {e}")
        return False


async def query_prompt_template_for_room(room_id: int) -> Optional[Dict[str, Any]]:
    """
    Query the prompt template for a specific room.
    
    Args:
        room_id: The room ID
        
    Returns:
        Template data dictionary or None if not found
    """
    try:
        session = await _pool.get_session()
        async with get_db_headers() as headers:
            # Use the database function to get the appropriate template
            url = f"{config.supabase.url}/rest/v1/rpc/get_room_prompt_template"
            data = {"room_id": room_id}
            
            timeout = aiohttp.ClientTimeout(total=config.supabase.http_timeout)
            
            try:
                async with session.post(url, headers=headers, json=data, timeout=timeout) as response:
                    if response.status == 200:
                        result = await response.json()
                        if result and len(result) > 0:
                            template = result[0]
                            # Parse template_variables if it's a string
                            if isinstance(template.get('template_variables'), str):
                                try:
                                    import json
                                    template['template_variables'] = json.loads(template['template_variables'])
                                except:
                                    template['template_variables'] = {}
                            return template
                    else:
                        error_text = await response.text()
                        logger.warning(f"Failed to query prompt template: {response.status} - {error_text}")
            except asyncio.TimeoutError:
                logger.warning("Timeout querying prompt template")
            except Exception as e:
                logger.error(f"Error querying prompt template: {e}")
        
        return None
        
    except Exception as e:
        logger.error(f"âŒ Prompt template query failed: {e}")
        return None


async def close_database_connections():
    """Close all database connections. Call this on shutdown."""
    await _pool.close()
    logger.info("âœ… Database connections closed")


================================================
FILE: backup_20250727_213400/Dockerfile
================================================
# Use Python 3.10 as base image
FROM python:3.10-slim

# Set working directory
WORKDIR /app

# Install system dependencies for audio processing and LiveKit
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    portaudio19-dev \
    python3-dev \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements and install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir --root-user-action=ignore -r requirements.txt

# Copy application code
COPY . .

# Create non-root user for security
RUN useradd -m -u 1000 agent && chown -R agent:agent /app
USER agent

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
  CMD python -c "import asyncio; import sys; print('Agent is running')" || exit 1

# Default command to run the agent in production mode
CMD ["python", "main_production.py", "start"] 


================================================
FILE: backup_20250727_213400/main.py
================================================
import asyncio
import logging
import json
import time
import re
import os
import uuid
from typing import Set, Any, Dict, Optional
from collections import defaultdict, deque
from datetime import datetime, timedelta

from enum import Enum
from dataclasses import dataclass, asdict

from livekit import rtc
from livekit.agents import (
    AutoSubscribe,
    JobContext,
    JobProcess,
    JobRequest,
    WorkerOptions,
    cli,
    stt,
    utils,
)
from livekit.plugins import silero, speechmatics
from livekit.plugins.speechmatics.types import TranscriptionConfig

# Import configuration
from config import get_config, ApplicationConfig

# Import database operations
from database import (
    ensure_active_session,
    store_transcript_in_database,
    query_room_by_name,
    get_active_session_for_room,
    broadcast_to_channel,
    close_database_connections
)

# Import text processing and translation helpers
from text_processing import extract_complete_sentences
from translation_helpers import translate_sentences

# Import Translator class
from translator import Translator

# Import broadcasting function
from broadcasting import broadcast_to_displays

# Import resource management
from resource_management import ResourceManager, TaskManager, STTStreamManager

# Import webhook handler for room context
try:
    from webhook_handler import get_room_context as get_webhook_room_context
except ImportError:
    # Webhook handler not available, use empty context
    def get_webhook_room_context(room_name: str):
        return {}


# Load configuration
config = get_config()

logger = logging.getLogger("transcriber")


@dataclass
class Language:
    code: str
    name: str
    flag: str


# Build languages dictionary from config
languages = {}
for code, lang_info in config.translation.supported_languages.items():
    languages[code] = Language(
        code=code,
        name=lang_info["name"],
        flag=lang_info["flag"]
    )

LanguageCode = Enum(
    "LanguageCode",  # Name of the Enum
    {lang.name: code for code, lang in languages.items()},  # Enum entries: name -> code mapping
)


# Translator class has been moved to translator.py


def prewarm(proc: JobProcess):
    proc.userdata["vad"] = silero.VAD.load()


async def entrypoint(job: JobContext):
    # Configure source language - ARABIC as default
    # This will be the language that users are actually speaking (host/speaker language)
    source_language = config.translation.default_source_language
    
    # Initialize resource manager
    resource_manager = ResourceManager()
    
    # Register heartbeat timeout callback
    async def on_participant_timeout(participant_id: str):
        logger.warning(f"ðŸ’” Participant {participant_id} timed out - initiating cleanup")
        # Could trigger cleanup here if needed
    
    resource_manager.heartbeat_monitor.register_callback(on_participant_timeout)
    
    # Extract tenant context from room metadata or webhook handler
    tenant_context = {}
    try:
        # Try to query Supabase directly for room information
        if job.room and job.room.name:
            logger.info(f"ðŸ” Looking up room context for: {job.room.name}")
            
            # Check if database is configured
            logger.info(f"ðŸ”‘ Supabase URL: {config.supabase.url}")
            logger.info(f"ðŸ”‘ Supabase key available: {'Yes' if config.supabase.service_role_key else 'No'}")
            
            if config.supabase.service_role_key:
                try:
                    # Query room by LiveKit room name using the new database module
                    logger.info(f"ðŸ” Querying database for room: {job.room.name}")
                    # Query room directly without task wrapper
                    room_data = await query_room_by_name(job.room.name)
                    
                    if room_data:
                        tenant_context = {
                            "room_id": room_data.get("id"),
                            "mosque_id": room_data.get("mosque_id"),
                            "room_title": room_data.get("Title"),
                            "transcription_language": room_data.get("transcription_language", "ar"),
                            "translation_language": room_data.get("translation__language", "nl"),
                            "created_at": room_data.get("created_at")
                        }
                        # Also store the double underscore version for compatibility
                        if room_data.get("translation__language"):
                            tenant_context["translation__language"] = room_data.get("translation__language")
                        
                        logger.info(f"âœ… Found room in database: room_id={tenant_context.get('room_id')}, mosque_id={tenant_context.get('mosque_id')}")
                        logger.info(f"ðŸ—£ï¸ Languages: transcription={tenant_context.get('transcription_language')}, translation={tenant_context.get('translation_language')} (or {tenant_context.get('translation__language')})")
                        
                        # Try to get active session for this room
                        session_id = await get_active_session_for_room(tenant_context['room_id'])
                        if session_id:
                            tenant_context["session_id"] = session_id
                            logger.info(f"ðŸ“ Found active session: {tenant_context['session_id']}")
                except Exception as e:
                    logger.warning(f"âš ï¸ Could not query Supabase: {e}")
        
        # Fallback to webhook handler if available
        if not tenant_context:
            webhook_context = get_webhook_room_context(job.room.name if job.room else "")
            if webhook_context:
                tenant_context = {
                    "room_id": webhook_context.get("room_id"),
                    "mosque_id": webhook_context.get("mosque_id"),
                    "session_id": webhook_context.get("session_id"),
                    "room_title": webhook_context.get("room_title"),
                    "transcription_language": webhook_context.get("transcription_language", "ar"),
                    "translation_language": webhook_context.get("translation_language", "nl"),
                    "created_at": webhook_context.get("created_at")
                }
                logger.info(f"ðŸ¢ Tenant context from webhook handler: mosque_id={tenant_context.get('mosque_id')}, room_id={tenant_context.get('room_id')}")
        
        # Fallback to room metadata if available
        if not tenant_context and job.room and job.room.metadata:
            try:
                metadata = json.loads(job.room.metadata)
                tenant_context = {
                    "room_id": metadata.get("room_id"),
                    "mosque_id": metadata.get("mosque_id"),
                    "session_id": metadata.get("session_id"),
                    "room_title": metadata.get("room_title"),
                    "transcription_language": metadata.get("transcription_language", "ar"),
                    "translation_language": metadata.get("translation_language", "nl"),
                    "created_at": metadata.get("created_at")
                }
                logger.info(f"ðŸ¢ Tenant context from room metadata: mosque_id={tenant_context.get('mosque_id')}, room_id={tenant_context.get('room_id')}")
            except:
                pass
        
        # Final fallback to default context with hardcoded values for testing
        if not tenant_context:
            logger.warning(f"âš ï¸ No tenant context available for room: {job.room.name if job.room else 'unknown'}")
            # TEMPORARY: Use hardcoded values for mosque_546012 rooms
            if job.room and f"mosque_{config.test_mosque_id}" in job.room.name:
                tenant_context = {
                    "room_id": config.test_room_id,
                    "mosque_id": config.test_mosque_id,
                    "session_id": None,
                    "transcription_language": "ar",
                    "translation_language": "nl"
                }
                logger.info(f"ðŸ”§ Using hardcoded tenant context for testing: mosque_id={tenant_context['mosque_id']}, room_id={tenant_context['room_id']}")
            else:
                tenant_context = {
                    "room_id": None,
                    "mosque_id": int(os.getenv('DEFAULT_MOSQUE_ID', str(config.default_mosque_id))),
                    "session_id": None,
                    "transcription_language": "ar",
                    "translation_language": "nl"
                }
    except Exception as e:
        logger.warning(f"âš ï¸ Could not extract tenant context: {e}")
    
    # Configure Speechmatics STT with room-specific settings
    # Use tenant_context which already has room configuration
    room_config = None
    if tenant_context and tenant_context.get('room_id'):
        # We already have the room data in tenant_context from earlier query
        room_config = tenant_context
        logger.info(f"ðŸ“‹ Using room-specific configuration from context: "
                  f"lang={room_config.get('transcription_language', 'ar')}, "
                  f"target={room_config.get('translation_language', 'nl')}, "
                  f"delay={room_config.get('max_delay', 2.0)}, "
                  f"punct={room_config.get('punctuation_sensitivity', 0.5)}")
        
        # If we need full room data and it's not in context, query it
        if not room_config.get('max_delay'):
            try:
                full_room_data = await query_room_by_name(job.room.name if job.room else None)
                if full_room_data:
                    # Merge the full room data with tenant context
                    room_config.update({
                        'max_delay': full_room_data.get('max_delay'),
                        'punctuation_sensitivity': full_room_data.get('punctuation_sensitivity'),
                        'translation__language': full_room_data.get('translation__language')
                    })
                    logger.info(f"ðŸ“‹ Fetched additional room config: delay={room_config.get('max_delay')}, punct={room_config.get('punctuation_sensitivity')}")
            except Exception as e:
                logger.warning(f"Failed to fetch additional room config: {e}")
    
    # Create STT configuration with room-specific overrides
    stt_config = config.speechmatics.with_room_settings(room_config)
    
    # Initialize STT provider with configured settings
    stt_provider = speechmatics.STT(
        transcription_config=TranscriptionConfig(
            language=stt_config.language,
            operating_point=stt_config.operating_point,
            enable_partials=stt_config.enable_partials,
            max_delay=stt_config.max_delay,
            punctuation_overrides={"sensitivity": stt_config.punctuation_sensitivity},
            diarization=stt_config.diarization
        )
    )
    
    # Update source language based on room config
    source_language = config.translation.get_source_language(room_config)
    logger.info(f"ðŸ—£ï¸ STT configured for {languages[source_language].name} speech recognition")
    
    translators = {}
    
    # Get target language from room config or use default
    target_language = config.translation.get_target_language(room_config)
    logger.info(f"ðŸŽ¯ Target language resolved to: '{target_language}' (from room_config: {room_config.get('translation_language') if room_config else 'None'} or {room_config.get('translation__language') if room_config else 'None'})")
    
    # Create translator for the configured target language
    if target_language in languages:
        # Get language enum dynamically
        lang_info = languages[target_language]
        lang_enum = getattr(LanguageCode, lang_info.name)
        translators[target_language] = Translator(job.room, lang_enum, tenant_context, broadcast_to_displays)
        logger.info(f"ðŸ“ Initialized {lang_info.name} translator ({target_language})")
    else:
        logger.warning(f"âš ï¸ Target language '{target_language}' not supported, falling back to Dutch")
        dutch_enum = getattr(LanguageCode, 'Dutch')
        translators["nl"] = Translator(job.room, dutch_enum, tenant_context, broadcast_to_displays)
    
    # Sentence accumulation for proper sentence-by-sentence translation
    accumulated_text = ""  # Accumulates text until we get a complete sentence
    last_final_transcript = ""  # Keep track of the last final transcript to avoid duplicates
    current_sentence_id = None  # Track current sentence being built
    
    logger.info(f"ðŸš€ Starting entrypoint for room: {job.room.name if job.room else 'unknown'}")
    logger.info(f"ðŸ” Translators dict ID: {id(translators)}")
    logger.info(f"ðŸŽ¯ Configuration: {languages[source_language].name} â†’ {languages.get(target_language, languages['nl']).name}")
    logger.info(f"âš™ï¸ STT Settings: delay={stt_config.max_delay}s, punctuation={stt_config.punctuation_sensitivity}")

    async def _forward_transcription(
        stt_stream: stt.SpeechStream,
        track: rtc.Track,
    ):
        """Forward the transcription and log the transcript in the console"""
        nonlocal accumulated_text, last_final_transcript, current_sentence_id
        
        try:
            async for ev in stt_stream:
                # Log to console for interim (word-by-word)
                if ev.type == stt.SpeechEventType.INTERIM_TRANSCRIPT:
                    print(ev.alternatives[0].text, end="", flush=True)
                    
                    # Publish interim transcription for real-time word-by-word display
                    interim_text = ev.alternatives[0].text.strip()
                    if interim_text:
                        try:
                            interim_segment = rtc.TranscriptionSegment(
                                id=utils.misc.shortuuid("SG_"),
                                text=interim_text,
                                start_time=0,
                                end_time=0,
                                language=source_language,  # Arabic
                                final=False,  # This is interim, not final
                            )
                            interim_transcription = rtc.Transcription(
                                job.room.local_participant.identity, "", [interim_segment]
                            )
                            await job.room.local_participant.publish_transcription(interim_transcription)
                        except Exception as e:
                            logger.debug(f"Failed to publish interim transcription: {str(e)}")
                    
                elif ev.type == stt.SpeechEventType.FINAL_TRANSCRIPT:
                    print("\n")
                    final_text = ev.alternatives[0].text.strip()
                    print(" -> ", final_text)
                    logger.info(f"Final Arabic transcript: {final_text}")

                    if final_text and final_text != last_final_transcript:
                        last_final_transcript = final_text
                        
                        # Publish final transcription for the original language (Arabic)
                        try:
                            final_segment = rtc.TranscriptionSegment(
                                id=utils.misc.shortuuid("SG_"),
                                text=final_text,
                                start_time=0,
                                end_time=0,
                                language=source_language,  # Arabic
                                final=True,
                            )
                            final_transcription = rtc.Transcription(
                                job.room.local_participant.identity, "", [final_segment]
                            )
                            await job.room.local_participant.publish_transcription(final_transcription)
                            
                            logger.info(f"âœ… Published final {languages[source_language].name} transcription: '{final_text}'")
                        except Exception as e:
                            logger.error(f"âŒ Failed to publish final transcription: {str(e)}")
                        
                        # Generate sentence ID if we don't have one for this sentence
                        if not current_sentence_id:
                            current_sentence_id = str(uuid.uuid4())
                        
                        # Broadcast final transcription text for real-time display
                        print(f"ðŸ“¡ Broadcasting Arabic text to frontend: '{final_text}'")
                        # Broadcast directly without task wrapper
                        await broadcast_to_displays(
                            "transcription", 
                            source_language, 
                            final_text, 
                            tenant_context,
                            sentence_context={
                                "sentence_id": current_sentence_id,
                                "is_complete": False,  # Will be marked complete when sentence ends
                                "is_fragment": True
                            }
                        )
                        
                        # Handle translation logic
                        if translators:
                            # SIMPLE ACCUMULATION LOGIC - ONLY APPEND, NEVER REPLACE
                            if accumulated_text:
                                # ALWAYS append new final transcript to existing accumulated text
                                accumulated_text = accumulated_text.strip() + " " + final_text
                            else:
                                # First transcript - start accumulation
                                accumulated_text = final_text
                            
                            logger.info(f"ðŸ“ Updated accumulated Arabic text: '{accumulated_text}'")
                            
                            # Extract complete sentences from accumulated text
                            complete_sentences, remaining_text = extract_complete_sentences(accumulated_text)
                            
                            # Handle special punctuation completion signal
                            if complete_sentences and complete_sentences[0] == "PUNCTUATION_COMPLETE":
                                if accumulated_text.strip():
                                    # Complete the accumulated sentence with this punctuation
                                    print(f"ðŸ“ PUNCTUATION SIGNAL: Completing accumulated text: '{accumulated_text}'")
                                    
                                    # Broadcast sentence completion
                                    if current_sentence_id:
                                        await broadcast_to_displays(
                                            "transcription", 
                                            source_language, 
                                            accumulated_text, 
                                            tenant_context,
                                            sentence_context={
                                                "sentence_id": current_sentence_id,
                                                "is_complete": True,
                                                "is_fragment": False
                                            }
                                        )
                                    
                                    # Translate the completed sentence (don't include the punctuation marker)
                                    await translate_sentences([accumulated_text], translators, source_language, current_sentence_id)
                                    
                                    # Clear accumulated text as sentence is now complete
                                    accumulated_text = ""
                                    current_sentence_id = None
                                    print(f"ðŸ“ Cleared accumulated text after punctuation completion")
                                else:
                                    print(f"âš ï¸ Received punctuation completion signal but no accumulated text")
                            elif complete_sentences:
                                # We have complete sentences - translate them immediately
                                print(f"ðŸŽ¯ Found {len(complete_sentences)} complete Arabic sentences: {complete_sentences}")
                                
                                # Broadcast each complete sentence
                                for sentence in complete_sentences:
                                    sentence_id = current_sentence_id if len(complete_sentences) == 1 else str(uuid.uuid4())
                                    await broadcast_to_displays(
                                        "transcription", 
                                        source_language, 
                                        sentence, 
                                        tenant_context,
                                        sentence_context={
                                            "sentence_id": sentence_id,
                                            "is_complete": True,
                                            "is_fragment": False
                                        }
                                    )
                                    # Translate complete sentences with sentence ID
                                    await translate_sentences([sentence], translators, source_language, sentence_id)
                                
                                # Update accumulated text to only remaining incomplete text
                                accumulated_text = remaining_text
                                # Generate new sentence ID for the remaining text
                                current_sentence_id = str(uuid.uuid4()) if remaining_text else None
                                print(f"ðŸ“ Updated accumulated Arabic text after sentence extraction: '{accumulated_text}'")
                            
                            # Log remaining incomplete text (no delayed translation)
                            if accumulated_text.strip():
                                logger.info(f"ðŸ“ Incomplete Arabic text remaining: '{accumulated_text}'")
                                # Note: Incomplete text will be translated when the next sentence completes
                        else:
                            logger.warning(f"âš ï¸ No translators available in room {job.room.name}, only {languages[source_language].name} transcription published")
                    else:
                        logger.debug("Empty or duplicate transcription, skipping")
        except Exception as e:
            logger.error(f"STT transcription error: {str(e)}")
            raise

    async def transcribe_track(participant: rtc.RemoteParticipant, track: rtc.Track):
        logger.info(f"ðŸŽ¤ Starting Arabic transcription for participant {participant.identity}, track {track.sid}")
        
        try:
            audio_stream = rtc.AudioStream(track)
            
            # Use context manager for STT stream
            async with resource_manager.stt_manager.create_stream(stt_provider, participant.identity) as stt_stream:
                # Create transcription task with tracking
                stt_task = resource_manager.task_manager.create_task(
                    _forward_transcription(stt_stream, track),
                    name=f"transcribe-{participant.identity}",
                    metadata={"participant": participant.identity, "track": track.sid}
                )
                
                frame_count = 0
                async for ev in audio_stream:
                    frame_count += 1
                    if frame_count % 100 == 0:  # Log every 100 frames to avoid spam
                        logger.debug(f"ðŸ”Š Received audio frame #{frame_count} from {participant.identity}")
                        # Update heartbeat every 100 frames
                        await resource_manager.heartbeat_monitor.update_heartbeat(
                            participant.identity, 
                            tenant_context.get('session_id')
                        )
                    stt_stream.push_frame(ev.frame)
                    
                logger.warning(f"ðŸ”‡ Audio stream ended for {participant.identity}")
                
                # Cancel the transcription task if still running
                if not stt_task.done():
                    stt_task.cancel()
                    try:
                        await stt_task
                    except asyncio.CancelledError:
                        logger.debug(f"STT task cancelled for {participant.identity}")
                        
        except Exception as e:
            logger.error(f"âŒ Transcription track error for {participant.identity}: {str(e)}")
        
        logger.info(f"ðŸ§¹ Transcription cleanup completed for {participant.identity}")

    @job.room.on("track_subscribed")
    def on_track_subscribed(
        track: rtc.Track,
        publication: rtc.TrackPublication,
        participant: rtc.RemoteParticipant,
    ):
        logger.info(f"ðŸŽµ Track subscribed: {track.kind} from {participant.identity} (track: {track.sid})")
        logger.info(f"Track details - muted: {publication.muted}")
        if track.kind == rtc.TrackKind.KIND_AUDIO:
            logger.info(f"âœ… Adding Arabic transcriber for participant: {participant.identity}")
            resource_manager.task_manager.create_task(
                transcribe_track(participant, track),
                name=f"track-handler-{participant.identity}",
                metadata={"participant": participant.identity, "track": track.sid}
            )
        else:
            logger.info(f"âŒ Ignoring non-audio track: {track.kind}")

    @job.room.on("track_published")
    def on_track_published(publication: rtc.TrackPublication, participant: rtc.RemoteParticipant):
        logger.info(f"ðŸ“¡ Track published: {publication.kind} from {participant.identity} (track: {publication.sid})")
        logger.info(f"Publication details - muted: {publication.muted}")

    @job.room.on("track_unpublished") 
    def on_track_unpublished(publication: rtc.TrackPublication, participant: rtc.RemoteParticipant):
        logger.info(f"ðŸ“¡ Track unpublished: {publication.kind} from {participant.identity}")

    @job.room.on("participant_connected")
    def on_participant_connected(participant: rtc.RemoteParticipant):
        logger.info(f"ðŸ‘¥ Participant connected: {participant.identity}")
        
        # Try to extract metadata from participant if available
        if hasattr(participant, 'metadata') and participant.metadata:
            try:
                participant_metadata = json.loads(participant.metadata)
                if participant_metadata:
                    # Update tenant context with participant metadata
                    tenant_context.update({
                        "room_id": participant_metadata.get("room_id", tenant_context.get("room_id")),
                        "mosque_id": participant_metadata.get("mosque_id", tenant_context.get("mosque_id")),
                        "session_id": participant_metadata.get("session_id", tenant_context.get("session_id")),
                        "room_title": participant_metadata.get("room_title", tenant_context.get("room_title"))
                    })
                    logger.info(f"ðŸ“‹ Updated tenant context from participant metadata: {tenant_context}")
                    
                    # Update all translators with new context
                    for translator in translators.values():
                        translator.tenant_context = tenant_context
            except Exception as e:
                logger.debug(f"Could not parse participant metadata: {e}")

    @job.room.on("participant_disconnected")
    def on_participant_disconnected(participant: rtc.RemoteParticipant):
        logger.info(f"ðŸ‘¥ Participant disconnected: {participant.identity}")
        
        # Remove from heartbeat monitoring
        resource_manager.heartbeat_monitor.remove_participant(participant.identity)
        
        # Resource cleanup is now handled by ResourceManager
        # Log current resource statistics
        resource_manager.log_stats()
        logger.info(f"ðŸ§¹ Participant cleanup completed for {participant.identity}")

    @job.room.on("participant_attributes_changed")
    def on_attributes_changed(
        changed_attributes: dict[str, str], participant: rtc.Participant
    ):
        """
        When participant attributes change, handle new translation requests.
        """
        logger.info(f"ðŸŒ Participant {participant.identity} attributes changed: {changed_attributes}")
        lang = changed_attributes.get("captions_language", None)
        if lang:
            if lang == source_language:
                logger.info(f"âœ… Participant {participant.identity} requested {languages[source_language].name} (source language - Arabic)")
            elif lang in translators:
                logger.info(f"âœ… Participant {participant.identity} requested existing language: {lang}")
                logger.info(f"ðŸ“Š Current translators for this room: {list(translators.keys())}")
            else:
                # Check if the language is supported and different from source language
                if lang in languages:
                    try:
                        # Create a translator for the requested language using the language enum
                        language_obj = languages[lang]
                        language_enum = getattr(LanguageCode, language_obj.name)
                        translators[lang] = Translator(job.room, language_enum, tenant_context, broadcast_to_displays)
                        logger.info(f"ðŸ†• Added translator for ROOM {job.room.name} (requested by {participant.identity}), language: {language_obj.name}")
                        logger.info(f"ðŸ¢ Translator created with tenant context: mosque_id={tenant_context.get('mosque_id')}")
                        logger.info(f"ðŸ“Š Total translators for room {job.room.name}: {len(translators)} -> {list(translators.keys())}")
                        logger.info(f"ðŸ” Translators dict ID: {id(translators)}")
                        
                        # Debug: Verify the translator was actually added
                        if lang in translators:
                            logger.info(f"âœ… Translator verification: {lang} successfully added to room translators")
                        else:
                            logger.error(f"âŒ Translator verification FAILED: {lang} not found in translators dict")
                            
                    except Exception as e:
                        logger.error(f"âŒ Error creating translator for {lang}: {str(e)}")
                else:
                    logger.warning(f"âŒ Unsupported language requested by {participant.identity}: {lang}")
                    logger.info(f"ðŸ’¡ Supported languages: {list(languages.keys())}")
        else:
            logger.debug(f"No caption language change for participant {participant.identity}")

    logger.info("Connecting to room...")
    await job.connect(auto_subscribe=AutoSubscribe.AUDIO_ONLY)
    logger.info(f"Successfully connected to room: {job.room.name}")
    logger.info(f"ðŸ“¡ Real-time transcription data will be sent via Supabase Broadcast")
    
    # Debug room state after connection
    logger.info(f"Room participants: {len(job.room.remote_participants)}")
    for participant in job.room.remote_participants.values():
        logger.info(f"Participant: {participant.identity}")
        logger.info(f"  Audio tracks: {len(participant.track_publications)}")
        for sid, pub in participant.track_publications.items():
            logger.info(f"    Track {sid}: {pub.kind}, muted: {pub.muted}")

    # Also check local participant
    logger.info(f"Local participant: {job.room.local_participant.identity}")
    logger.info(f"Local participant tracks: {len(job.room.local_participant.track_publications)}")

    @job.room.local_participant.register_rpc_method("get/languages")
    async def get_languages(data: rtc.RpcInvocationData):
        languages_list = [asdict(lang) for lang in languages.values()]
        return json.dumps(languages_list)
    
    @job.room.local_participant.register_rpc_method("request/cleanup")
    async def request_cleanup(data: rtc.RpcInvocationData):
        """Handle cleanup request from frontend"""
        try:
            payload = json.loads(data.payload)
            reason = payload.get('reason', 'unknown')
            session_id = payload.get('session_id')
            
            logger.info(f"ðŸ§¹ Cleanup requested by frontend: reason={reason}, session_id={session_id}")
            
            # Initiate graceful shutdown
            asyncio.create_task(perform_graceful_cleanup(reason, session_id))
            
            return json.dumps({
                "success": True,
                "message": "Cleanup initiated"
            })
        except Exception as e:
            logger.error(f"Error handling cleanup request: {e}")
            return json.dumps({
                "success": False,
                "error": str(e)
            })
    
    async def perform_graceful_cleanup(reason: str, session_id: Optional[str]):
        """Perform graceful cleanup when requested by frontend"""
        logger.info(f"ðŸ›‘ Starting graceful cleanup: {reason}")
        
        # Log current resource state
        resource_manager.log_stats()
        
        # If session_id provided, update it in database
        if session_id and tenant_context.get('room_id'):
            try:
                from database import query_database
                result = await query_database(
                    "SELECT cleanup_session_idempotent(%s, %s, %s)",
                    [session_id, f"frontend_{reason}", datetime.utcnow()]
                )
                logger.info(f"Session cleanup result: {result}")
            except Exception as e:
                logger.error(f"Error updating session: {e}")
        
        # Shutdown all resources
        await resource_manager.shutdown()
        
        # Verify cleanup is complete
        verification = await resource_manager.verify_cleanup_complete()
        logger.info(f"ðŸ” Cleanup verification: {verification}")
        
        # Disconnect from room (this will trigger on_room_disconnected)
        await job.room.disconnect()
        
        logger.info("âœ… Graceful cleanup completed")

    @job.room.on("disconnected")
    def on_room_disconnected():
        """Handle room disconnection - cleanup all resources"""
        logger.info("ðŸšª Room disconnected, starting cleanup...")
        
        # Create async task for cleanup
        async def cleanup():
            # Log final resource statistics
            resource_manager.log_stats()
            
            # Shutdown resource manager (cancels all tasks, closes all streams)
            await resource_manager.shutdown()
            
            # Close database connections
            try:
                await close_database_connections()
                logger.info("âœ… Database connections closed")
                
                # Force cleanup of any remaining sessions
                import gc
                gc.collect()  # Force garbage collection
                await asyncio.sleep(0.1)  # Give time for cleanup
            except Exception as e:
                logger.debug(f"Database cleanup error: {e}")
            
            # Final verification
            verification = await resource_manager.verify_cleanup_complete()
            logger.info(f"ðŸ” Final cleanup verification: {verification}")
            
            logger.info("âœ… Room cleanup completed")
        
        # Run cleanup in the event loop
        asyncio.create_task(cleanup())


async def request_fnc(req: JobRequest):
    logger.info(f"ðŸŽ¯ Received job request for room: {req.room.name if req.room else 'unknown'}")
    logger.info(f"ðŸ“‹ Request details: job_id={req.id}, room_name={req.room.name if req.room else 'unknown'}")
    await req.accept(
        name="agent",
        identity="agent",
    )
    logger.info(f"âœ… Accepted job request for room: {req.room.name if req.room else 'unknown'}")


if __name__ == "__main__":
    cli.run_app(
        WorkerOptions(
            entrypoint_fnc=entrypoint, prewarm_fnc=prewarm, request_fnc=request_fnc
        )
    )


================================================
FILE: backup_20250727_213400/main_production.py
================================================
#!/usr/bin/env python3
"""
Production-ready LiveKit Agent for Bayaan Translation Service
Optimized for Render deployment as a background worker
"""

import asyncio
import logging
import os
import sys
import signal
from datetime import datetime

# Setup production logging
def setup_production_logging():
    """Configure logging for production."""
    log_level = os.getenv('LOG_LEVEL', 'INFO').upper()
    
    logging.basicConfig(
        level=getattr(logging, log_level),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(sys.stdout)
        ]
    )

def health_check() -> bool:
    """Health check endpoint for Render."""
    try:
        # Basic health check - verify environment variables are set
        required_vars = [
            'LIVEKIT_URL', 'LIVEKIT_API_KEY', 'LIVEKIT_API_SECRET',
            'OPENAI_API_KEY', 'SUPABASE_URL', 'SUPABASE_SERVICE_ROLE_KEY'
        ]
        
        for var in required_vars:
            if not os.getenv(var):
                print(f"Missing required environment variable: {var}")
                return False
        
        print("Health check passed - all required environment variables are set")
        return True
        
    except Exception as e:
        print(f"Health check failed: {e}")
        return False

def main():
    """Main production entry point."""
    # Setup production logging
    setup_production_logging()
    
    logger = logging.getLogger(__name__)
    logger.info("Starting Bayaan LiveKit Agent for production deployment")
    
    # Set production environment
    os.environ['ENVIRONMENT'] = 'production'
    
    try:
        # Import and run the agent directly
        from livekit.agents import WorkerOptions, cli
        from main import entrypoint, prewarm, request_fnc
        
        # Production worker configuration
        worker_opts = WorkerOptions(
            entrypoint_fnc=entrypoint,
            prewarm_fnc=prewarm,
            request_fnc=request_fnc
        )
        
        logger.info("Starting LiveKit CLI with production configuration")
        
        # Run the agent - this handles its own event loop
        cli.run_app(worker_opts)
        
    except Exception as e:
        logger.error(f"Fatal error in production agent: {e}")
        sys.exit(1)

if __name__ == "__main__":
    # Handle different command line arguments
    if len(sys.argv) > 1:
        if sys.argv[1] == "health":
            # Health check endpoint
            is_healthy = health_check()
            sys.exit(0 if is_healthy else 1)
        elif sys.argv[1] == "start":
            # Start the agent
            main()
        else:
            # Default to original main.py behavior
            from main import *
            # Run with original CLI
            from livekit.agents import cli, WorkerOptions
            cli.run_app(WorkerOptions(
                entrypoint_fnc=entrypoint,
                prewarm_fnc=prewarm,
                request_fnc=request_fnc
            ))
    else:
        # Run the main agent
        main() 


================================================
FILE: backup_20250727_213400/prompt_builder.py
================================================
"""
Prompt Builder for customizable translation prompts.
Handles loading and formatting of prompt templates with variable substitution.
"""
import logging
from typing import Dict, Optional, Any
import json

from config import get_config
from database import query_prompt_template_for_room

logger = logging.getLogger("transcriber.prompt_builder")
config = get_config()


class PromptBuilder:
    """
    Builds customized translation prompts based on templates and room configuration.
    """
    
    # Default fallback prompt if no template is found
    DEFAULT_PROMPT = (
        "You are an expert simultaneous interpreter. Your task is to translate from {source_lang} to {target_lang}. "
        "Provide a direct and accurate translation of the user's input. "
        "Do not add any additional commentary, explanations, or introductory phrases. "
        "Be concise for real-time delivery."
    )
    
    def __init__(self):
        """Initialize the prompt builder."""
        logger.info("ðŸŽ¨ PromptBuilder initialized")
    
    async def get_prompt_for_room(
        self, 
        room_id: Optional[int],
        source_lang: str,
        target_lang: str,
        room_config: Optional[Dict[str, Any]] = None
    ) -> str:
        """
        Get the appropriate prompt for a room with variable substitution.
        
        Args:
            room_id: The room ID to get prompt for
            source_lang: Source language name (e.g., "Arabic")
            target_lang: Target language name (e.g., "Dutch")
            room_config: Optional room configuration with additional context
            
        Returns:
            Formatted prompt string ready for use
        """
        try:
            # Try to get template from database if room_id provided
            template = None
            if room_id:
                template = await self._fetch_template_for_room(room_id)
            
            if template:
                prompt = template.get('prompt_template', self.DEFAULT_PROMPT)
                # Ensure template_variables is always a dict, never None
                variables = template.get('template_variables') or {}
                if not isinstance(variables, dict):
                    logger.warning(f"template_variables is not a dict: {type(variables)}, using empty dict")
                    variables = {}
                logger.info(f"ðŸ“‹ Using prompt template: {template.get('name', 'Unknown')}")
            else:
                # Use default prompt
                prompt = self.DEFAULT_PROMPT
                variables = {}
                logger.info("ðŸ“‹ Using default prompt template")
            
            # Prepare substitution variables
            substitutions = {
                'source_lang': source_lang,
                'target_lang': target_lang,
                **variables  # Include template-specific variables
            }
            
            # Add room-specific context if available
            if room_config:
                if room_config.get('mosque_name'):
                    substitutions['mosque_name'] = room_config['mosque_name']
                if room_config.get('speaker_role'):
                    substitutions['speaker_role'] = room_config['speaker_role']
            
            # Format the prompt with variables
            formatted_prompt = prompt.format(**substitutions)
            
            # Log the generated prompt for debugging
            logger.debug(f"Generated prompt: {formatted_prompt[:100]}...")
            
            return formatted_prompt
            
        except Exception as e:
            logger.error(f"âŒ Error building prompt: {e}")
            # Fallback to basic default
            return self.DEFAULT_PROMPT.format(
                source_lang=source_lang,
                target_lang=target_lang
            )
    
    async def _fetch_template_for_room(self, room_id: int) -> Optional[Dict[str, Any]]:
        """
        Fetch prompt template for a specific room.
        
        Args:
            room_id: The room ID
            
        Returns:
            Template dictionary or None
        """
        try:
            # Always fetch fresh from database - no caching for production
            template = await query_prompt_template_for_room(room_id)
            
            if template:
                # Log what we received for debugging
                logger.info(f"ðŸ“‹ Fetched template for room {room_id}: {template.get('name', 'Unknown')}")
                logger.debug(f"Template data: {template}")
            else:
                logger.info(f"ðŸ“‹ No template found for room {room_id}")
                
            return template
            
        except Exception as e:
            logger.warning(f"Failed to fetch template for room {room_id}: {e}")
            return None
    
    def build_prompt_with_context(
        self,
        base_prompt: str,
        context_type: str,
        additional_context: Optional[Dict[str, str]] = None
    ) -> str:
        """
        Enhance a prompt with additional context based on content type.
        
        Args:
            base_prompt: The base prompt template
            context_type: Type of content (sermon, announcement, etc.)
            additional_context: Additional context variables
            
        Returns:
            Enhanced prompt with context
        """
        context_additions = {
            'sermon': (
                " Remember this is a religious sermon, so maintain appropriate "
                "reverence and formality. Preserve the spiritual tone."
            ),
            'announcement': (
                " This is a community announcement, so prioritize clarity and "
                "practical information over stylistic concerns."
            ),
            'dua': (
                " This is a prayer or supplication. Maintain the devotional "
                "atmosphere and emotional depth of the original."
            ),
            'lecture': (
                " This is an educational lecture. You may add brief clarifications "
                "in parentheses for complex religious terms if needed."
            )
        }
        
        # Add context-specific guidance
        addition = context_additions.get(context_type, "")
        
        # Add any additional context
        if additional_context:
            for key, value in additional_context.items():
                if value:
                    addition += f" {key}: {value}."
        
        return base_prompt + addition
    
    def get_preserved_terms_for_template(self, template_variables: Dict) -> list:
        """
        Extract list of terms to preserve from template variables.
        
        Args:
            template_variables: Template variables dictionary
            
        Returns:
            List of terms to preserve in original language
        """
        return template_variables.get('preserve_terms', [])


# Global instance
_prompt_builder: Optional[PromptBuilder] = None


def get_prompt_builder() -> PromptBuilder:
    """Get or create the global prompt builder instance."""
    global _prompt_builder
    if _prompt_builder is None:
        _prompt_builder = PromptBuilder()
    return _prompt_builder


================================================
FILE: backup_20250727_213400/render.yaml
================================================
services:
  - type: worker
    name: bayaan-livekit-agent
    env: docker
    dockerfilePath: ./Dockerfile
    plan: starter
    region: oregon
    scaling:
      minInstances: 1
      maxInstances: 3
      targetMemoryPercent: 80
      targetCPUPercent: 80
    envVars:
      - key: ENVIRONMENT
        value: production
      - key: LIVEKIT_URL
        fromSecret: livekit_url
      - key: LIVEKIT_API_KEY
        fromSecret: livekit_api_key
      - key: LIVEKIT_API_SECRET
        fromSecret: livekit_api_secret
      - key: OPENAI_API_KEY
        fromSecret: openai_api_key
      - key: SPEECHMATICS_API_KEY
        fromSecret: speechmatics_api_key
      - key: SUPABASE_URL
        fromSecret: supabase_url
      - key: SUPABASE_SERVICE_ROLE_KEY
        fromSecret: supabase_service_role_key
      - key: SUPABASE_ANON_KEY
        fromSecret: supabase_anon_key
      - key: PYTHONPATH
        value: /app
      - key: AGENT_NAME
        value: bayaan-transcriber
      - key: WORKER_TYPE
        value: background
      - key: LOG_LEVEL
        value: INFO
      - key: PERSISTENT_MODE
        value: "true"
      - key: MAX_WORKERS
        value: "3"
      - key: IDLE_TIMEOUT
        value: "300" 


================================================
FILE: backup_20250727_213400/requirements.txt
================================================
# LiveKit Core Dependencies
livekit-agents>=1.0.0
livekit-plugins-openai>=0.8.0
livekit-plugins-speechmatics>=0.6.0
livekit-plugins-silero>=0.6.0

# AI/ML Dependencies
openai>=1.0.0

# Web Framework (for health checks)
fastapi>=0.104.0
uvicorn[standard]>=0.24.0

# HTTP Client
aiohttp>=3.8.0

# Database
asyncpg>=0.29.0

# Environment & Configuration
python-dotenv>=1.0.0

# Logging & Monitoring
# structlog>=23.0.0  # Removed - using standard logging

# Production Dependencies
gunicorn>=21.0.0
psutil>=5.9.0

# Audio Processing
pyaudio>=0.2.11

# Async utilities
asyncio-throttle>=1.0.0


================================================
FILE: backup_20250727_213400/resource_management.py
================================================
"""
Resource management module for LiveKit AI Translation Server.
Handles tracking and cleanup of async tasks, STT streams, and other resources.
"""
import asyncio
import logging
from typing import Set, List, Dict, Any, Optional, Callable
from contextlib import asynccontextmanager
from dataclasses import dataclass, field
from datetime import datetime, timedelta
import weakref

logger = logging.getLogger("transcriber.resources")


@dataclass
class ResourceStats:
    """Statistics about managed resources."""
    tasks_created: int = 0
    tasks_completed: int = 0
    tasks_failed: int = 0
    tasks_cancelled: int = 0
    streams_opened: int = 0
    streams_closed: int = 0
    active_tasks: int = 0
    active_streams: int = 0


class TaskManager:
    """
    Manages async tasks with proper tracking and cleanup.
    
    Features:
    - Automatic task tracking
    - Graceful cancellation
    - Resource leak prevention
    - Statistics tracking
    """
    
    def __init__(self, name: str = "default"):
        self.name = name
        self._tasks: Set[asyncio.Task] = set()
        self._task_metadata: Dict[asyncio.Task, Dict[str, Any]] = {}
        self._stats = ResourceStats()
        self._cleanup_interval = 30.0  # seconds
        self._cleanup_task: Optional[asyncio.Task] = None
        self._shutdown = False
        logger.info(f"ðŸ“‹ TaskManager '{self.name}' initialized")
    
    async def __aenter__(self):
        """Context manager entry."""
        self._cleanup_task = asyncio.create_task(self._periodic_cleanup())
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit with cleanup."""
        await self.shutdown()
    
    def create_task(
        self, 
        coro, 
        name: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None
    ) -> asyncio.Task:
        """
        Create and track an async task.
        
        Args:
            coro: Coroutine to run
            name: Optional task name
            metadata: Optional metadata for the task
            
        Returns:
            Created task
        """
        if self._shutdown:
            raise RuntimeError("TaskManager is shutting down")
        
        task = asyncio.create_task(coro, name=name)
        self._tasks.add(task)
        
        if metadata:
            self._task_metadata[task] = metadata
        
        # Add callback to clean up when done
        task.add_done_callback(self._task_done_callback)
        
        self._stats.tasks_created += 1
        self._stats.active_tasks = len(self._tasks)
        
        logger.debug(f"ðŸ“Œ Created task: {name or task.get_name()} (total: {len(self._tasks)})")
        return task
    
    def _task_done_callback(self, task: asyncio.Task):
        """Callback when a task completes."""
        self._tasks.discard(task)
        self._task_metadata.pop(task, None)
        
        try:
            if task.cancelled():
                self._stats.tasks_cancelled += 1
                logger.debug(f"ðŸš« Task cancelled: {task.get_name()}")
            elif task.exception():
                self._stats.tasks_failed += 1
                logger.error(f"âŒ Task failed: {task.get_name()}", exc_info=task.exception())
            else:
                self._stats.tasks_completed += 1
                logger.debug(f"âœ… Task completed: {task.get_name()}")
        except Exception as e:
            logger.debug(f"Error in task callback: {e}")
        
        self._stats.active_tasks = len(self._tasks)
    
    async def _periodic_cleanup(self):
        """Periodically clean up completed tasks."""
        while not self._shutdown:
            try:
                await asyncio.sleep(self._cleanup_interval)
                
                # Clean up any references to completed tasks
                completed = [t for t in self._tasks if t.done()]
                for task in completed:
                    self._tasks.discard(task)
                    self._task_metadata.pop(task, None)
                
                if completed:
                    logger.debug(f"ðŸ§¹ Cleaned up {len(completed)} completed tasks")
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"Error in periodic cleanup: {e}")
    
    async def cancel_all(self, timeout: float = 5.0) -> int:
        """
        Cancel all active tasks.
        
        Args:
            timeout: Maximum time to wait for cancellation
            
        Returns:
            Number of tasks cancelled
        """
        if not self._tasks:
            return 0
        
        tasks_to_cancel = list(self._tasks)
        cancelled_count = 0
        
        logger.info(f"ðŸš« Cancelling {len(tasks_to_cancel)} tasks...")
        
        # Cancel all tasks
        for task in tasks_to_cancel:
            if not task.done():
                task.cancel()
                cancelled_count += 1
        
        # Wait for cancellation with timeout
        if tasks_to_cancel:
            try:
                await asyncio.wait_for(
                    asyncio.gather(*tasks_to_cancel, return_exceptions=True),
                    timeout=timeout
                )
            except asyncio.TimeoutError:
                logger.warning(f"â° Timeout waiting for {len(tasks_to_cancel)} tasks to cancel")
        
        logger.info(f"âœ… Cancelled {cancelled_count} tasks")
        return cancelled_count
    
    async def shutdown(self):
        """Shutdown the task manager and cleanup all resources."""
        if self._shutdown:
            return
        
        self._shutdown = True
        logger.info(f"ðŸ›‘ Shutting down TaskManager '{self.name}'...")
        
        # Cancel cleanup task
        if self._cleanup_task and not self._cleanup_task.done():
            self._cleanup_task.cancel()
            try:
                await self._cleanup_task
            except asyncio.CancelledError:
                pass
        
        # Cancel all managed tasks
        await self.cancel_all()
        
        logger.info(f"âœ… TaskManager '{self.name}' shutdown complete")
    
    def get_stats(self) -> ResourceStats:
        """Get current statistics."""
        return self._stats
    
    def get_active_tasks(self) -> List[asyncio.Task]:
        """Get list of active tasks."""
        return list(self._tasks)


class STTStreamManager:
    """
    Manages STT (Speech-to-Text) streams with proper cleanup.
    """
    
    def __init__(self):
        self._streams: Set[Any] = set()
        self._stream_metadata: Dict[Any, Dict[str, Any]] = {}
        self._stats = ResourceStats()
        logger.info("ðŸŽ¤ STTStreamManager initialized")
    
    @asynccontextmanager
    async def create_stream(self, stt_provider, participant_id: str):
        """
        Create and manage an STT stream.
        
        Args:
            stt_provider: STT provider instance
            participant_id: ID of the participant
            
        Yields:
            STT stream instance
        """
        stream = None
        try:
            # Create stream
            stream = stt_provider.stream()
            self._streams.add(stream)
            self._stream_metadata[stream] = {
                "participant_id": participant_id,
                "created_at": datetime.utcnow()
            }
            self._stats.streams_opened += 1
            self._stats.active_streams = len(self._streams)
            
            logger.info(f"ðŸŽ¤ Created STT stream for {participant_id}")
            yield stream
            
        finally:
            # Cleanup stream
            if stream:
                try:
                    await stream.aclose()
                    logger.info(f"âœ… STT stream closed for {participant_id}")
                except Exception as e:
                    logger.error(f"Error closing STT stream: {e}")
                finally:
                    self._streams.discard(stream)
                    self._stream_metadata.pop(stream, None)
                    self._stats.streams_closed += 1
                    self._stats.active_streams = len(self._streams)
    
    async def close_all(self):
        """Close all active streams."""
        if not self._streams:
            return
        
        logger.info(f"ðŸš« Closing {len(self._streams)} STT streams...")
        
        streams_to_close = list(self._streams)
        for stream in streams_to_close:
            try:
                await stream.aclose()
            except Exception as e:
                logger.error(f"Error closing stream: {e}")
            finally:
                self._streams.discard(stream)
                self._stream_metadata.pop(stream, None)
        
        self._stats.active_streams = 0
        logger.info("âœ… All STT streams closed")
    
    def get_stats(self) -> ResourceStats:
        """Get current statistics."""
        return self._stats


class HeartbeatMonitor:
    """
    Monitors participant activity and detects stuck sessions.
    """
    
    def __init__(self, timeout: float = 30.0):
        self.timeout = timeout
        self.participants: Dict[str, datetime] = {}
        self.session_info: Dict[str, Dict[str, Any]] = {}
        self._monitor_task: Optional[asyncio.Task] = None
        self._callbacks: List[Callable[[str], Any]] = []
        logger.info(f"ðŸ’“ HeartbeatMonitor initialized with {timeout}s timeout")
    
    def register_callback(self, callback: Callable[[str], Any]):
        """Register a callback to be called when a participant times out."""
        self._callbacks.append(callback)
    
    async def update_heartbeat(self, participant_id: str, session_id: Optional[str] = None):
        """Update the heartbeat timestamp for a participant."""
        self.participants[participant_id] = datetime.utcnow()
        if session_id:
            self.session_info[participant_id] = {
                "session_id": session_id,
                "last_seen": datetime.utcnow()
            }
        logger.debug(f"ðŸ’“ Heartbeat updated for {participant_id}")
    
    async def check_timeouts(self) -> List[str]:
        """Check for timed-out participants."""
        now = datetime.utcnow()
        timed_out = []
        
        for participant_id, last_seen in list(self.participants.items()):
            elapsed = (now - last_seen).total_seconds()
            if elapsed > self.timeout:
                timed_out.append(participant_id)
                logger.warning(f"â° Participant {participant_id} timed out (last seen {elapsed:.1f}s ago)")
                
                # Remove from tracking
                self.participants.pop(participant_id, None)
                session_info = self.session_info.pop(participant_id, None)
                
                # Call registered callbacks
                for callback in self._callbacks:
                    try:
                        if asyncio.iscoroutinefunction(callback):
                            await callback(participant_id)
                        else:
                            callback(participant_id)
                    except Exception as e:
                        logger.error(f"Error in heartbeat callback: {e}")
        
        return timed_out
    
    async def start_monitoring(self):
        """Start the heartbeat monitoring loop."""
        if self._monitor_task and not self._monitor_task.done():
            return
        
        async def monitor_loop():
            while True:
                try:
                    await asyncio.sleep(10)  # Check every 10 seconds
                    timed_out = await self.check_timeouts()
                    if timed_out:
                        logger.info(f"ðŸ’” {len(timed_out)} participants timed out")
                except asyncio.CancelledError:
                    break
                except Exception as e:
                    logger.error(f"Error in heartbeat monitor: {e}")
        
        self._monitor_task = asyncio.create_task(monitor_loop())
        logger.info("ðŸ’“ Heartbeat monitoring started")
    
    async def stop_monitoring(self):
        """Stop the heartbeat monitoring loop."""
        if self._monitor_task and not self._monitor_task.done():
            self._monitor_task.cancel()
            try:
                await self._monitor_task
            except asyncio.CancelledError:
                pass
        logger.info("ðŸ’” Heartbeat monitoring stopped")
    
    def remove_participant(self, participant_id: str):
        """Remove a participant from monitoring."""
        self.participants.pop(participant_id, None)
        self.session_info.pop(participant_id, None)
        logger.debug(f"ðŸ’” Participant {participant_id} removed from heartbeat monitoring")


class ResourceManager:
    """
    Central resource manager for the application.
    Coordinates TaskManager and STTStreamManager.
    """
    
    def __init__(self):
        self.task_manager = TaskManager("main")
        self.stt_manager = STTStreamManager()
        self.heartbeat_monitor = HeartbeatMonitor(timeout=45.0)  # 45 seconds timeout
        self._shutdown_handlers: List[Callable] = []
        logger.info("ðŸ—ï¸ ResourceManager initialized")
    
    async def __aenter__(self):
        """Context manager entry."""
        await self.task_manager.__aenter__()
        await self.heartbeat_monitor.start_monitoring()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit with cleanup."""
        await self.shutdown()
    
    def add_shutdown_handler(self, handler: Callable):
        """Add a handler to be called on shutdown."""
        self._shutdown_handlers.append(handler)
    
    async def shutdown(self):
        """Shutdown all managed resources."""
        logger.info("ðŸ›‘ Starting ResourceManager shutdown...")
        
        # Run shutdown handlers
        for handler in self._shutdown_handlers:
            try:
                if asyncio.iscoroutinefunction(handler):
                    await handler()
                else:
                    handler()
            except Exception as e:
                logger.error(f"Error in shutdown handler: {e}")
        
        # Shutdown managers
        await self.heartbeat_monitor.stop_monitoring()
        await self.task_manager.shutdown()
        await self.stt_manager.close_all()
        
        logger.info("âœ… ResourceManager shutdown complete")
    
    def get_all_stats(self) -> Dict[str, Any]:
        """Get statistics from all managers."""
        return {
            "tasks": self.task_manager.get_stats(),
            "stt_streams": self.stt_manager.get_stats(),
            "heartbeat": {
                "active_participants": len(self.heartbeat_monitor.participants),
                "timeout": self.heartbeat_monitor.timeout
            }
        }
    
    def log_stats(self):
        """Log current resource statistics."""
        stats = self.get_all_stats()
        logger.info(
            f"ðŸ“Š Resource Stats - "
            f"Tasks: {stats['tasks'].active_tasks} active "
            f"({stats['tasks'].tasks_completed} completed, "
            f"{stats['tasks'].tasks_failed} failed, "
            f"{stats['tasks'].tasks_cancelled} cancelled), "
            f"STT Streams: {stats['stt_streams'].active_streams} active, "
            f"Heartbeat: {stats['heartbeat']['active_participants']} participants"
        )
    
    async def verify_cleanup_complete(self) -> Dict[str, Any]:
        """Verify all resources are properly cleaned up."""
        active_tasks = self.task_manager.get_active_tasks()
        active_streams = len(self.stt_manager._streams)
        active_participants = len(self.heartbeat_monitor.participants)
        
        # Check if connection pool is closed
        db_closed = True
        try:
            from database import _pool
            if hasattr(_pool, '_local') and hasattr(_pool._local, 'session'):
                db_closed = _pool._local.session is None or _pool._local.session.closed
        except:
            pass
        
        cleanup_complete = (
            len(active_tasks) == 0 and 
            active_streams == 0 and 
            active_participants == 0 and
            db_closed
        )
        
        result = {
            "cleanup_complete": cleanup_complete,
            "tasks_remaining": len(active_tasks),
            "active_task_names": [t.get_name() for t in active_tasks],
            "streams_remaining": active_streams,
            "participants_remaining": active_participants,
            "participant_ids": list(self.heartbeat_monitor.participants.keys()),
            "db_connections_closed": db_closed,
            "timestamp": datetime.utcnow().isoformat()
        }
        
        if not cleanup_complete:
            logger.warning(f"âš ï¸ Cleanup verification failed: {result}")
        else:
            logger.info("âœ… Cleanup verification passed - all resources cleaned up")
        
        return result


================================================
FILE: backup_20250727_213400/text_processing.py
================================================
"""
Text processing utilities for LiveKit AI Translation Server.
Handles sentence extraction and text manipulation for Arabic and other languages.
"""
import re
import logging
from typing import Tuple, List

logger = logging.getLogger("transcriber.text_processing")


def extract_complete_sentences(text: str) -> Tuple[List[str], str]:
    """
    Extract complete sentences from text and return them along with remaining incomplete text.
    
    This function is designed to work with Arabic text and handles Arabic punctuation marks.
    It identifies complete sentences based on punctuation and returns both the complete
    sentences and any remaining incomplete text.
    
    Args:
        text: The input text to process
        
    Returns:
        A tuple containing:
        - List of complete sentences
        - Remaining incomplete text
    """
    if not text.strip():
        return [], ""
    
    # Arabic sentence ending punctuation marks
    sentence_endings = ['.', '!', '?', 'ØŸ']  # Including Arabic question mark
    
    complete_sentences = []
    remaining_text = ""
    
    logger.debug(f"ðŸ” Processing text for sentence extraction: '{text}'")
    
    # Check if this is standalone punctuation
    if text.strip() in sentence_endings:
        logger.debug(f"ðŸ“ Detected standalone punctuation: '{text.strip()}'")
        # This is standalone punctuation - signal to complete any accumulated sentence
        return ["PUNCTUATION_COMPLETE"], ""
    
    # Split text into parts ending with punctuation
    # This regex splits on punctuation but keeps the punctuation in the result
    parts = re.split(r'([.!?ØŸ])', text)
    
    current_building = ""
    i = 0
    while i < len(parts):
        part = parts[i].strip()
        if not part:
            i += 1
            continue
            
        if part in sentence_endings:
            # Found punctuation - complete the current sentence
            if current_building.strip():
                complete_sentence = current_building.strip() + part
                complete_sentences.append(complete_sentence)
                logger.debug(f"âœ… Complete sentence found: '{complete_sentence}'")
                current_building = ""
            i += 1
        else:
            # Regular text - add to current building
            current_building += (" " + part if current_building else part)
            i += 1
    
    # Any remaining text becomes the incomplete part
    if current_building.strip():
        remaining_text = current_building.strip()
        logger.debug(f"ðŸ”„ Remaining incomplete text: '{remaining_text}'")
    
    logger.debug(f"ðŸ“Š Extracted {len(complete_sentences)} complete sentences, remaining: '{remaining_text}'")
    return complete_sentences, remaining_text


def is_sentence_ending(text: str) -> bool:
    """
    Check if the text ends with a sentence-ending punctuation mark.
    
    Args:
        text: The text to check
        
    Returns:
        True if the text ends with sentence-ending punctuation
    """
    if not text.strip():
        return False
    
    sentence_endings = ['.', '!', '?', 'ØŸ']
    return text.strip()[-1] in sentence_endings


def clean_text(text: str) -> str:
    """
    Clean and normalize text for processing.
    
    Args:
        text: The text to clean
        
    Returns:
        Cleaned text
    """
    # Remove extra whitespace
    text = ' '.join(text.split())
    # Remove leading/trailing whitespace
    text = text.strip()
    return text


def split_into_chunks(text: str, max_length: int = 500) -> List[str]:
    """
    Split text into chunks of maximum length, preferring to split at sentence boundaries.
    
    Args:
        text: The text to split
        max_length: Maximum length of each chunk
        
    Returns:
        List of text chunks
    """
    if len(text) <= max_length:
        return [text]
    
    chunks = []
    sentences, _ = extract_complete_sentences(text)
    
    current_chunk = ""
    for sentence in sentences:
        if len(current_chunk) + len(sentence) + 1 <= max_length:
            current_chunk += (" " + sentence if current_chunk else sentence)
        else:
            if current_chunk:
                chunks.append(current_chunk)
            current_chunk = sentence
    
    if current_chunk:
        chunks.append(current_chunk)
    
    return chunks


================================================
FILE: backup_20250727_213400/translation_helpers.py
================================================
"""
Translation helper functions for LiveKit AI Translation Server.
Handles translation orchestration and related utilities.
"""
import asyncio
import logging
from typing import List, Dict, Any, Optional

logger = logging.getLogger("transcriber.translation")


async def translate_sentences(
    sentences: List[str], 
    translators: Dict[str, Any],
    source_language: str = "ar",
    sentence_id: Optional[str] = None
) -> None:
    """
    Translate complete sentences to all target languages.
    
    This function takes a list of sentences and sends them to all available
    translators concurrently for better performance.
    
    Args:
        sentences: List of sentences to translate
        translators: Dictionary of language code to translator instances
        source_language: Source language code (default: "ar" for Arabic)
        sentence_id: Optional sentence ID for tracking
    """
    if not sentences or not translators:
        return
        
    for sentence in sentences:
        if sentence.strip():
            logger.info(f"ðŸŽ¯ TRANSLATING COMPLETE {source_language.upper()} SENTENCE: '{sentence}'")
            logger.info(f"ðŸ“Š Processing sentence for {len(translators)} translators")
            
            # Send to all translators concurrently for better performance
            translation_tasks = []
            for lang, translator in translators.items():
                logger.info(f"ðŸ“¤ Sending complete {source_language.upper()} sentence '{sentence}' to {lang} translator")
                translation_tasks.append(translator.translate(sentence, sentence_id))
            
            # Execute all translations concurrently
            if translation_tasks:
                results = await asyncio.gather(*translation_tasks, return_exceptions=True)
                # Check for any exceptions
                for i, result in enumerate(results):
                    if isinstance(result, Exception):
                        logger.error(f"âŒ Translation failed: {result}")


async def translate_single_sentence(
    sentence: str,
    translator: Any,
    target_language: str
) -> Optional[str]:
    """
    Translate a single sentence to a specific target language.
    
    Args:
        sentence: The sentence to translate
        translator: The translator instance to use
        target_language: Target language code
        
    Returns:
        Translated text or None if translation failed
    """
    try:
        if not sentence.strip():
            return None
            
        logger.debug(f"Translating to {target_language}: '{sentence}'")
        result = await translator.translate(sentence, None)
        return result
    except Exception as e:
        logger.error(f"Translation to {target_language} failed: {e}")
        return None


def should_translate_text(text: str, min_length: int = 3) -> bool:
    """
    Determine if text should be translated based on various criteria.
    
    Args:
        text: The text to evaluate
        min_length: Minimum length for translation (default: 3 characters)
        
    Returns:
        True if text should be translated
    """
    if not text or not text.strip():
        return False
    
    # Don't translate very short text
    if len(text.strip()) < min_length:
        return False
    
    # Don't translate if it's only punctuation
    if all(c in '.!?ØŸ,ØŒ;Ø›:' for c in text.strip()):
        return False
    
    return True


def format_translation_output(
    original_text: str,
    translated_text: str,
    source_lang: str,
    target_lang: str
) -> Dict[str, str]:
    """
    Format translation output for consistent structure.
    
    Args:
        original_text: Original text
        translated_text: Translated text
        source_lang: Source language code
        target_lang: Target language code
        
    Returns:
        Formatted translation dictionary
    """
    return {
        "original": original_text,
        "translated": translated_text,
        "source_language": source_lang,
        "target_language": target_lang,
        "type": "translation"
    }


async def batch_translate(
    texts: List[str],
    translators: Dict[str, Any],
    batch_size: int = 5
) -> Dict[str, List[str]]:
    """
    Translate multiple texts in batches for efficiency.
    
    Args:
        texts: List of texts to translate
        translators: Dictionary of language code to translator instances
        batch_size: Number of texts to process in each batch
        
    Returns:
        Dictionary mapping language codes to lists of translations
    """
    results = {lang: [] for lang in translators.keys()}
    
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]
        
        # Process each batch
        for text in batch:
            if should_translate_text(text):
                await translate_sentences([text], translators)
    
    return results


================================================
FILE: backup_20250727_213400/translator.py
================================================
"""
Translator module for LiveKit AI Translation Server.
Handles real-time translation with context management and error handling.
"""
import asyncio
import logging
from typing import Optional, Dict, Any, List, Callable
from collections import deque
from enum import Enum

from livekit import rtc
from livekit.agents import llm, utils
from livekit.plugins import openai

from config import get_config
from prompt_builder import get_prompt_builder

logger = logging.getLogger("transcriber.translator")
config = get_config()
prompt_builder = get_prompt_builder()


class TranslationError(Exception):
    """Custom exception for translation-related errors."""
    pass


class Translator:
    """
    Handles translation from source language to target language with context management.
    
    Features:
    - Sliding window context for better translation coherence
    - Automatic retry on failures
    - Comprehensive error handling
    - Real-time broadcasting to displays
    """
    
    # Class-level configuration from config module
    use_context = config.translation.use_context
    max_context_pairs = config.translation.max_context_pairs
    
    def __init__(self, room: rtc.Room, lang: Enum, tenant_context: Optional[Dict[str, Any]] = None, broadcast_callback: Optional[Callable] = None):
        """
        Initialize the Translator.
        
        Args:
            room: LiveKit room instance
            lang: Target language enum
            tenant_context: Optional context containing room_id, mosque_id, etc.
            broadcast_callback: Optional callback function for broadcasting translations
        """
        self.room = room
        self.lang = lang
        self.tenant_context = tenant_context or {}
        self.broadcast_callback = broadcast_callback
        self.llm = openai.LLM()
        
        # Initialize system prompt as None - will be built dynamically
        self.system_prompt = None
        self._prompt_template = None
        
        # Use deque for automatic sliding window (old messages auto-removed)
        if self.use_context:
            self.message_history: deque = deque(maxlen=(self.max_context_pairs * 2))
        
        # Track translation statistics
        self.translation_count = 0
        self.error_count = 0
        
        # Log the context mode being used
        context_mode = f"TRUE SLIDING WINDOW ({self.max_context_pairs}-pair memory)" if self.use_context else "FRESH CONTEXT (no memory)"
        logger.info(f"ðŸ§  Translator initialized for {lang.value} with {context_mode} mode")
        
        # Initialize prompt asynchronously on first use
        self._prompt_initialized = False

    async def translate(self, message: str, sentence_id: Optional[str] = None, max_retries: int = 2) -> str:
        """
        Translate a message from source to target language.
        
        Args:
            message: Text to translate
            sentence_id: Optional sentence ID for tracking
            max_retries: Maximum number of retry attempts on failure
            
        Returns:
            Translated text (empty string on failure)
            
        Raises:
            TranslationError: If translation fails after all retries
        """
        if not message or not message.strip():
            logger.debug("Empty message, skipping translation")
            return ""
        
        retry_count = 0
        last_error = None
        
        while retry_count <= max_retries:
            try:
                translated_message = await self._perform_translation(message)
                
                if translated_message:
                    # Publish transcription to LiveKit room
                    await self._publish_transcription(translated_message, None)
                    
                    # Broadcast to displays
                    await self._broadcast_translation(translated_message, sentence_id)
                    
                    # Update statistics
                    self.translation_count += 1
                    
                    # Log successful translation
                    logger.info(f"âœ… Translated to {self.lang.value}: '{message}' â†’ '{translated_message}'")
                    
                    return translated_message
                else:
                    logger.warning(f"Empty translation result for: '{message}'")
                    return ""
                    
            except Exception as e:
                last_error = e
                retry_count += 1
                self.error_count += 1
                
                if retry_count <= max_retries:
                    wait_time = retry_count * 0.5  # Exponential backoff
                    logger.warning(
                        f"Translation attempt {retry_count} failed: {e}. "
                        f"Retrying in {wait_time}s..."
                    )
                    await asyncio.sleep(wait_time)
                else:
                    logger.error(
                        f"âŒ Translation failed after {max_retries} retries: {e}\n"
                        f"Message: '{message}'"
                    )
                    
        # If we get here, all retries failed
        error_msg = f"Translation failed for '{message}' after {max_retries} retries"
        if last_error:
            error_msg += f": {last_error}"
        
        # Don't raise exception - return empty string to keep stream going
        logger.error(error_msg)
        return ""

    async def _initialize_prompt(self):
        """Initialize the system prompt using the prompt builder."""
        if self._prompt_initialized:
            return
            
        try:
            # Get room ID from tenant context
            room_id = self.tenant_context.get('room_id')
            
            # Get source language from room config
            source_lang_code = self.tenant_context.get('transcription_language', 'ar')
            source_lang_name = config.translation.supported_languages.get(
                source_lang_code, 
                {"name": "Arabic"}
            )["name"]
            
            # Build the prompt using prompt builder
            self.system_prompt = await prompt_builder.get_prompt_for_room(
                room_id=room_id,
                source_lang=source_lang_name,
                target_lang=self.lang.value,
                room_config=self.tenant_context
            )
            
            logger.info(f"ðŸ“ Initialized translation prompt for room {room_id}: {source_lang_name} â†’ {self.lang.value}")
            self._prompt_initialized = True
            
        except Exception as e:
            logger.error(f"Failed to initialize prompt: {e}")
            # Fallback to default prompt with dynamic source language
            source_lang_code = self.tenant_context.get('transcription_language', 'ar')
            source_lang_name = config.translation.supported_languages.get(
                source_lang_code, 
                {"name": "Arabic"}
            )["name"]
            
            self.system_prompt = (
                f"You are an expert simultaneous interpreter. Your task is to translate from {source_lang_name} to {self.lang.value}. "
                f"Provide a direct and accurate translation of the user's input. Be concise and use natural-sounding language. "
                f"Do not add any additional commentary, explanations, or introductory phrases."
            )
            self._prompt_initialized = True
    
    async def _perform_translation(self, message: str) -> str:
        """
        Perform the actual translation using the LLM.
        
        Args:
            message: Text to translate
            
        Returns:
            Translated text
        """
        # Ensure prompt is initialized
        await self._initialize_prompt()
        # Build a fresh context for every translation (rebuild method)
        temp_context = llm.ChatContext()
        temp_context.add_message(role="system", content=self.system_prompt)
        
        # If using context, add the message history from our deque
        if self.use_context and hasattr(self, 'message_history'):
            logger.debug(f"ðŸ”„ Building context with {len(self.message_history)} historical messages")
            for msg in self.message_history:
                temp_context.add_message(role=msg['role'], content=msg['content'])
        
        # Add the current message to translate
        temp_context.add_message(content=message, role="user")
        
        # Get translation from LLM with the freshly built context
        stream = self.llm.chat(chat_ctx=temp_context)
        
        translated_message = ""
        async for chunk in stream:
            if chunk.delta is None:
                continue
            content = chunk.delta.content
            if content is None:
                break
            translated_message += content
        
        # If using context, update our history (deque will auto-remove old messages)
        if self.use_context and translated_message:
            self.message_history.append({"role": "user", "content": message})
            self.message_history.append({"role": "assistant", "content": translated_message})
            logger.debug(f"ðŸ’¾ History updated. Current size: {len(self.message_history)} messages")
        
        return translated_message

    async def _publish_transcription(self, translated_text: str, track: Optional[rtc.Track]) -> None:
        """
        Publish the translation as a transcription to the LiveKit room.
        
        Args:
            translated_text: The translated text to publish
            track: Optional track reference
        """
        try:
            segment = rtc.TranscriptionSegment(
                id=utils.misc.shortuuid("SG_"),
                text=translated_text,
                start_time=0,
                end_time=0,
                language=self.lang.value,
                final=True,
            )
            transcription = rtc.Transcription(
                self.room.local_participant.identity, 
                track.sid if track else "", 
                [segment]
            )
            await self.room.local_participant.publish_transcription(transcription)
            logger.debug(f"ðŸ“¤ Published {self.lang.value} transcription to LiveKit room")
        except Exception as e:
            logger.error(f"Failed to publish transcription: {e}")
            # Don't re-raise - translation was successful even if publishing failed

    async def _broadcast_translation(self, translated_text: str, sentence_id: Optional[str] = None) -> None:
        """
        Broadcast the translation to WebSocket displays.
        
        Args:
            translated_text: The translated text to broadcast
            sentence_id: Optional sentence ID for tracking
        """
        if self.broadcast_callback:
            try:
                # Use asyncio.create_task to avoid blocking
                # Include sentence context if provided
                sentence_context = None
                if sentence_id:
                    sentence_context = {
                        "sentence_id": sentence_id,
                        "is_complete": True,
                        "is_fragment": False
                    }
                
                asyncio.create_task(
                    self.broadcast_callback(
                        "translation", 
                        self.lang.value, 
                        translated_text, 
                        self.tenant_context,
                        sentence_context
                    )
                )
                logger.debug(f"ðŸ“¡ Broadcasted {self.lang.value} translation to displays")
            except Exception as e:
                logger.error(f"Failed to broadcast translation: {e}")
                # Don't re-raise - translation was successful even if broadcasting failed
        else:
            logger.debug("No broadcast callback provided, skipping broadcast")

    def get_statistics(self) -> Dict[str, Any]:
        """
        Get translation statistics.
        
        Returns:
            Dictionary containing translation stats
        """
        return {
            "language": self.lang.value,
            "translation_count": self.translation_count,
            "error_count": self.error_count,
            "error_rate": self.error_count / max(1, self.translation_count),
            "context_enabled": self.use_context,
            "context_size": len(self.message_history) if self.use_context else 0
        }

    def clear_context(self) -> None:
        """Clear the translation context history."""
        if self.use_context and hasattr(self, 'message_history'):
            self.message_history.clear()
            logger.info(f"ðŸ§¹ Cleared translation context for {self.lang.value}")

    def __repr__(self) -> str:
        """String representation of the Translator."""
        return (
            f"Translator(lang={self.lang.value}, "
            f"context={self.use_context}, "
            f"translations={self.translation_count}, "
            f"errors={self.error_count})"
        )


================================================
FILE: backup_20250727_213400/update_from_server_dev.sh
================================================
#!/bin/bash

# Server_Dev to Production Update Script for Bayaan Server
# This script intelligently syncs changes from server_dev while preserving production optimizations
# Author: Bayaan DevOps Team
# Date: $(date +%Y-%m-%d)

set -euo pipefail  # Exit on error, undefined variables, pipe failures

# Configuration
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
DEV_DIR="${SCRIPT_DIR}/server_dev"
PROD_DIR="${SCRIPT_DIR}"
BACKUP_DIR="${PROD_DIR}/backup_$(date +%Y%m%d_%H%M%S)"
LOG_FILE="${PROD_DIR}/update_server_dev_$(date +%Y%m%d_%H%M%S).log"
MERGE_REPORT="${PROD_DIR}/merge_report_$(date +%Y%m%d_%H%M%S).md"

# Color codes for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color

# Logging function
log() {
    echo -e "$1" | tee -a "$LOG_FILE"
}

# Error handling
error_exit() {
    log "${RED}ERROR: $1${NC}"
    log "${YELLOW}Rolling back changes...${NC}"
    rollback
    exit 1
}

# Initialize merge report
init_merge_report() {
    cat > "$MERGE_REPORT" << EOF
# Server_Dev â†’ Production Update Report
**Date:** $(date)
**Source:** $DEV_DIR
**Target:** $PROD_DIR

## Update Summary

EOF
}

# Add to merge report
report() {
    echo "$1" >> "$MERGE_REPORT"
}

# Rollback function
rollback() {
    if [ -d "$BACKUP_DIR" ]; then
        log "${YELLOW}Restoring from backup: $BACKUP_DIR${NC}"
        
        # Restore all backed up files
        for file in "$BACKUP_DIR"/*; do
            if [ -f "$file" ]; then
                filename=$(basename "$file")
                cp -f "$file" "$PROD_DIR/$filename" 2>/dev/null || true
                log "Restored: $filename"
            fi
        done
        
        log "${GREEN}Rollback completed${NC}"
    else
        log "${RED}No backup found for rollback${NC}"
    fi
}

# Pre-flight checks
preflight_checks() {
    log "${CYAN}=== Starting Pre-flight Checks ===${NC}"
    
    # Check if server_dev directory exists
    if [ ! -d "$DEV_DIR" ]; then
        error_exit "Server_dev directory not found: $DEV_DIR"
    fi
    
    # Check if we're in the production directory
    if [ "$(pwd)" != "$PROD_DIR" ]; then
        log "${YELLOW}Changing to production directory${NC}"
        cd "$PROD_DIR" || error_exit "Cannot change to production directory"
    fi
    
    # Check for critical production files
    local critical_files=("main_production.py" "render.yaml" "requirements.txt" "Dockerfile")
    for file in "${critical_files[@]}"; do
        if [ ! -f "$file" ]; then
            log "${RED}WARNING: Critical production file missing: $file${NC}"
        fi
    done
    
    # Verify Python is available
    if ! command -v python3 &> /dev/null; then
        error_exit "Python3 is required but not installed"
    fi
    
    log "${GREEN}Pre-flight checks passed${NC}"
}

# Create backup
create_backup() {
    log "${CYAN}=== Creating Backup ===${NC}"
    
    # Create backup directory
    mkdir -p "$BACKUP_DIR" || error_exit "Cannot create backup directory"
    
    # Backup all Python files and critical configs
    local files_to_backup=(
        "*.py"
        ".env"
        "requirements.txt"
        "Dockerfile"
        "render.yaml"
        ".gitignore"
        "*.sh"
    )
    
    for pattern in "${files_to_backup[@]}"; do
        for file in $pattern; do
            if [ -f "$file" ]; then
                cp -p "$file" "$BACKUP_DIR/" 2>/dev/null || true
                log "Backed up: $file"
            fi
        done
    done
    
    log "${GREEN}Backup created at: $BACKUP_DIR${NC}"
    report "### Backup Location\n\`$BACKUP_DIR\`\n"
}

# Compare files and determine update strategy
compare_and_update() {
    local file=$1
    local src_file="$DEV_DIR/$file"
    local dst_file="$PROD_DIR/$file"
    
    if [ ! -f "$src_file" ]; then
        log "${YELLOW}Source file not found, skipping: $file${NC}"
        return 1
    fi
    
    # If destination doesn't exist, it's a simple copy
    if [ ! -f "$dst_file" ]; then
        cp -f "$src_file" "$dst_file"
        log "${GREEN}Added new file: $file${NC}"
        report "- **Added:** \`$file\` (new file from server_dev)"
        return 0
    fi
    
    # Check if files are different
    if ! diff -q "$src_file" "$dst_file" > /dev/null 2>&1; then
        return 0  # Files are different, needs update
    else
        return 1  # Files are identical
    fi
}

# Update simple files (direct replacement)
update_simple_files() {
    log "${CYAN}=== Updating Simple Files ===${NC}"
    report "\n### Simple File Updates\n"
    
    # Files that can be safely replaced without merging
    local simple_files=(
        "config.py"
        "prompt_builder.py"
        "text_processing.py"
        "translation_helpers.py"
        "translator.py"
        "webhook_handler.py"
    )
    
    for file in "${simple_files[@]}"; do
        if compare_and_update "$file"; then
            cp -f "$DEV_DIR/$file" "$PROD_DIR/$file"
            log "${GREEN}Updated: $file${NC}"
            report "- **Updated:** \`$file\`"
        else
            log "${BLUE}No changes needed: $file${NC}"
        fi
    done
}

# Handle complex files with merge logic
update_complex_files() {
    log "${CYAN}=== Handling Complex Files ===${NC}"
    report "\n### Complex File Handling\n"
    
    # main.py - Preserve production optimizations
    if [ -f "$DEV_DIR/main.py" ]; then
        log "${YELLOW}Analyzing main.py differences...${NC}"
        
        # Create a comparison report
        if diff -u "$PROD_DIR/main.py" "$DEV_DIR/main.py" > /tmp/main_diff.txt 2>&1; then
            log "${BLUE}main.py is identical in both versions${NC}"
        else
            log "${YELLOW}main.py has differences - preserving production optimizations${NC}"
            report "- **main.py:** Differences detected - production optimizations preserved"
            report "  - Kept production's interim transcript handling"
            report "  - Kept production's simplified cleanup approach"
            report "  - Review \`/tmp/main_diff.txt\` for detailed differences"
            
            # Don't update main.py automatically - requires manual review
            log "${YELLOW}âš ï¸  main.py requires manual review due to production-specific optimizations${NC}"
        fi
    fi
    
    # database.py - Check for schema changes
    if [ -f "$DEV_DIR/database.py" ]; then
        if compare_and_update "database.py"; then
            log "${YELLOW}database.py has changes - reviewing for compatibility...${NC}"
            
            # Check if new functions are added that don't exist in production
            if grep -q "close_room_session\|update_session_heartbeat" "$DEV_DIR/database.py"; then
                log "${YELLOW}New database functions detected (heartbeat/session management)${NC}"
                report "- **database.py:** New session management functions detected"
                report "  - Contains heartbeat monitoring functions not used in production"
                report "  - Manual review recommended"
            fi
            
            # For now, skip automatic update of database.py
            log "${YELLOW}âš ï¸  database.py update skipped - manual review required${NC}"
        fi
    fi
    
    # broadcasting.py
    if compare_and_update "broadcasting.py"; then
        cp -f "$DEV_DIR/broadcasting.py" "$PROD_DIR/broadcasting.py"
        log "${GREEN}Updated: broadcasting.py${NC}"
        report "- **Updated:** \`broadcasting.py\`"
    fi
    
    # resource_management.py
    if compare_and_update "resource_management.py"; then
        cp -f "$DEV_DIR/resource_management.py" "$PROD_DIR/resource_management.py"
        log "${GREEN}Updated: resource_management.py${NC}"
        report "- **Updated:** \`resource_management.py\`"
    fi
}

# Handle new files from server_dev
handle_new_files() {
    log "${CYAN}=== Checking for New Files ===${NC}"
    report "\n### New Files Analysis\n"
    
    # Files to explicitly exclude
    local exclude_patterns=(
        "*_cleanup*.py"
        "*_fix.py"
        "*.sql"
        "start_server.sh"
        "production_deployment.md"
        "DEPLOYMENT.md"
        "CLEANUP_SUMMARY.md"
    )
    
    # Check for new Python files
    for file in "$DEV_DIR"/*.py; do
        if [ -f "$file" ]; then
            filename=$(basename "$file")
            
            # Skip if file exists in production
            if [ -f "$PROD_DIR/$filename" ]; then
                continue
            fi
            
            # Check exclusion patterns
            local skip=false
            for pattern in "${exclude_patterns[@]}"; do
                if [[ "$filename" == $pattern ]]; then
                    skip=true
                    break
                fi
            done
            
            if [ "$skip" = true ]; then
                log "${YELLOW}Excluded new file: $filename${NC}"
                report "- **Excluded:** \`$filename\` (development/cleanup file)"
            else
                log "${CYAN}Found new file: $filename - requires review${NC}"
                report "- **New file found:** \`$filename\` - manual review required"
            fi
        fi
    done
}

# Update requirements.txt intelligently
update_requirements() {
    log "${CYAN}=== Checking requirements.txt ===${NC}"
    report "\n### Dependencies Update\n"
    
    if [ -f "$DEV_DIR/requirements.txt" ] && [ -f "$PROD_DIR/requirements.txt" ]; then
        # Create sorted unique lists
        sort "$DEV_DIR/requirements.txt" | grep -v "^#" | grep -v "^$" > /tmp/dev_reqs.txt
        sort "$PROD_DIR/requirements.txt" | grep -v "^#" | grep -v "^$" > /tmp/prod_reqs.txt
        
        # Find new requirements in dev
        comm -23 /tmp/dev_reqs.txt /tmp/prod_reqs.txt > /tmp/new_reqs.txt
        
        if [ -s /tmp/new_reqs.txt ]; then
            log "${YELLOW}New dependencies found in server_dev:${NC}"
            cat /tmp/new_reqs.txt | while read -r req; do
                log "  + $req"
                report "- New dependency: \`$req\`"
            done
            report "\nâš ï¸  **Action Required:** Review and add new dependencies to production requirements.txt"
        else
            log "${GREEN}No new dependencies found${NC}"
            report "- No new dependencies detected"
        fi
        
        # Cleanup temp files
        rm -f /tmp/dev_reqs.txt /tmp/prod_reqs.txt /tmp/new_reqs.txt
    fi
}

# Verify Python syntax
verify_python_syntax() {
    log "${CYAN}=== Verifying Python Syntax ===${NC}"
    
    local all_good=true
    
    for file in *.py; do
        if [ -f "$file" ]; then
            if python3 -m py_compile "$file" 2>/dev/null; then
                log "${GREEN}âœ“ Syntax OK: $file${NC}"
            else
                log "${RED}âœ— Syntax error in: $file${NC}"
                all_good=false
            fi
        fi
    done
    
    # Clean up __pycache__
    rm -rf __pycache__ 2>/dev/null || true
    
    if [ "$all_good" = false ]; then
        error_exit "Python syntax verification failed"
    fi
    
    log "${GREEN}All Python files passed syntax check${NC}"
}

# Generate final recommendations
generate_recommendations() {
    log "${CYAN}=== Generating Recommendations ===${NC}"
    
    report "\n## Post-Update Recommendations\n"
    report "### Manual Review Required:"
    report "1. **main.py** - Review differences between dev and production versions"
    report "2. **database.py** - Check if new session management functions are needed"
    report "3. **New files** - Evaluate any new files from server_dev for inclusion"
    report "4. **Dependencies** - Review and update requirements.txt if needed"
    report ""
    report "### Testing Checklist:"
    report "- [ ] Run local tests with updated code"
    report "- [ ] Verify WebSocket connections work correctly"
    report "- [ ] Test transcript handling (both final and interim)"
    report "- [ ] Confirm database operations function properly"
    report "- [ ] Check resource cleanup on disconnection"
    report ""
    report "### Deployment Steps:"
    report "1. Review this report and the update log"
    report "2. Manually review complex files if needed"
    report "3. Run \`git status\` to see all changes"
    report "4. Test locally if possible"
    report "5. Commit changes with descriptive message"
    report "6. Deploy to Render following standard procedure"
    report ""
    report "### Rollback Instructions:"
    report "If issues occur, run: \`bash $0 --rollback\`"
    report ""
    report "**Backup Location:** \`$BACKUP_DIR\`"
    report "**Log File:** \`$LOG_FILE\`"
}

# Post-update summary
post_update_summary() {
    log ""
    log "${GREEN}=== Update Completed Successfully ===${NC}"
    log ""
    log "${CYAN}Important Files:${NC}"
    log "  ðŸ“„ Merge Report: ${YELLOW}$MERGE_REPORT${NC}"
    log "  ðŸ“‹ Log File: ${YELLOW}$LOG_FILE${NC}"
    log "  ðŸ’¾ Backup: ${YELLOW}$BACKUP_DIR${NC}"
    log ""
    log "${YELLOW}Next Steps:${NC}"
    log "  1. Review the merge report for detailed changes"
    log "  2. Manually review files marked for attention"
    log "  3. Run tests before deploying"
    log ""
    log "To rollback if needed: ${CYAN}bash $0 --rollback${NC}"
}

# Rollback functionality
if [ "${1:-}" == "--rollback" ]; then
    # Find the most recent backup
    LATEST_BACKUP=$(ls -td ${PROD_DIR}/backup_* 2>/dev/null | head -1)
    if [ -n "$LATEST_BACKUP" ]; then
        BACKUP_DIR="$LATEST_BACKUP"
        log "${YELLOW}Rolling back to: $BACKUP_DIR${NC}"
        rollback
        exit 0
    else
        log "${RED}No backup found for rollback${NC}"
        exit 1
    fi
fi

# Main execution
main() {
    log "${GREEN}=== Bayaan Server_Dev â†’ Production Update Script ===${NC}"
    log "Started at: $(date)"
    log "Source: $DEV_DIR"
    log "Target: $PROD_DIR"
    log ""
    
    # Initialize merge report
    init_merge_report
    
    # Execute update steps
    preflight_checks
    create_backup
    update_simple_files
    update_complex_files
    handle_new_files
    update_requirements
    verify_python_syntax
    generate_recommendations
    post_update_summary
    
    log "Completed at: $(date)"
}

# Run main function
main "$@"


================================================
FILE: backup_20250727_213400/update_prod.sh
================================================
#!/bin/bash

# Production Update Script for Bayaan Server
# This script safely updates production files from development while preserving production-specific configurations
# Author: Senior DevOps Engineer
# Date: $(date +%Y-%m-%d)

set -euo pipefail  # Exit on error, undefined variables, pipe failures

# Configuration
DEV_DIR="/mnt/c/Users/hassa/OneDrive/Desktop/0.2 Bayan/bayan-platform-admin-login/Backend/LiveKit-ai-translation/server"
PROD_DIR="/mnt/c/Users/hassa/OneDrive/Desktop/0.2 Bayan/bayaan-server-production"
BACKUP_DIR="${PROD_DIR}/backup_$(date +%Y%m%d_%H%M%S)"
LOG_FILE="${PROD_DIR}/update_$(date +%Y%m%d_%H%M%S).log"

# Color codes for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

# Logging function
log() {
    echo -e "$1" | tee -a "$LOG_FILE"
}

# Error handling
error_exit() {
    log "${RED}ERROR: $1${NC}"
    log "${YELLOW}Rolling back changes...${NC}"
    rollback
    exit 1
}

# Rollback function
rollback() {
    if [ -d "$BACKUP_DIR" ]; then
        log "${YELLOW}Restoring from backup: $BACKUP_DIR${NC}"
        
        # Restore Python files
        for file in "$BACKUP_DIR"/*.py; do
            if [ -f "$file" ]; then
                filename=$(basename "$file")
                cp -f "$file" "$PROD_DIR/$filename" 2>/dev/null || true
                log "Restored: $filename"
            fi
        done
        
        log "${GREEN}Rollback completed${NC}"
    else
        log "${RED}No backup found for rollback${NC}"
    fi
}

# Pre-flight checks
preflight_checks() {
    log "${YELLOW}=== Starting Pre-flight Checks ===${NC}"
    
    # Check if dev directory exists
    if [ ! -d "$DEV_DIR" ]; then
        error_exit "Development directory not found: $DEV_DIR"
    fi
    
    # Check if we're in the production directory
    if [ "$(pwd)" != "$PROD_DIR" ]; then
        log "${YELLOW}Changing to production directory${NC}"
        cd "$PROD_DIR" || error_exit "Cannot change to production directory"
    fi
    
    # Check for critical production files
    local critical_files=("main_production.py" "render.yaml" "requirements.txt" "Dockerfile" ".env")
    for file in "${critical_files[@]}"; do
        if [ ! -f "$file" ]; then
            log "${RED}WARNING: Critical production file missing: $file${NC}"
        fi
    done
    
    log "${GREEN}Pre-flight checks passed${NC}"
}

# Create backup
create_backup() {
    log "${YELLOW}=== Creating Backup ===${NC}"
    
    # Create backup directory
    mkdir -p "$BACKUP_DIR" || error_exit "Cannot create backup directory"
    
    # Backup all Python files and critical configs
    local files_to_backup=(
        "*.py"
        ".env"
        "requirements.txt"
        "Dockerfile"
        "render.yaml"
        ".gitignore"
    )
    
    for pattern in "${files_to_backup[@]}"; do
        for file in $pattern; do
            if [ -f "$file" ]; then
                cp -p "$file" "$BACKUP_DIR/" 2>/dev/null || true
                log "Backed up: $file"
            fi
        done
    done
    
    log "${GREEN}Backup created at: $BACKUP_DIR${NC}"
}

# Update files from development
update_files() {
    log "${YELLOW}=== Updating Files from Development ===${NC}"
    
    # Core Python modules to update (excluding dev-specific files)
    local python_files=(
        "main.py"
        "prompt_builder.py"
        "broadcasting.py"
        "config.py"
        "database.py"
        "resource_management.py"
        "text_processing.py"
        "translation_helpers.py"
        "translator.py"
        "webhook_handler.py"
    )
    
    # Files to explicitly exclude
    local exclude_patterns=(
        "*_backup.py"
        "*_fixed.py"
        "*_cleanup*.py"
        "production_deployment.md"
    )
    
    # Update each Python file
    for file in "${python_files[@]}"; do
        local src_file="$DEV_DIR/$file"
        
        if [ -f "$src_file" ]; then
            # Check if file exists in exclude patterns
            local skip=false
            for pattern in "${exclude_patterns[@]}"; do
                if [[ "$file" == $pattern ]]; then
                    skip=true
                    break
                fi
            done
            
            if [ "$skip" = false ]; then
                cp -f "$src_file" "$PROD_DIR/$file" || error_exit "Failed to copy $file"
                log "${GREEN}Updated: $file${NC}"
            else
                log "${YELLOW}Skipped (excluded): $file${NC}"
            fi
        else
            log "${YELLOW}Not found in dev (skipping): $file${NC}"
        fi
    done
    
    # Handle .env.example if it exists and production doesn't have it
    if [ -f "$DEV_DIR/.env.example" ] && [ ! -f "$PROD_DIR/.env.example" ]; then
        cp -f "$DEV_DIR/.env.example" "$PROD_DIR/.env.example"
        log "${GREEN}Added: .env.example${NC}"
    fi
    
    # Update .gitignore if needed
    if [ -f "$DEV_DIR/.gitignore" ]; then
        cp -f "$DEV_DIR/.gitignore" "$PROD_DIR/.gitignore"
        log "${GREEN}Updated: .gitignore${NC}"
    fi
}

# Verify production integrity
verify_production() {
    log "${YELLOW}=== Verifying Production Integrity ===${NC}"
    
    # Check that production-specific files are still present
    local prod_files=("main_production.py" "render.yaml" "requirements.txt" "Dockerfile" ".env")
    local all_good=true
    
    for file in "${prod_files[@]}"; do
        if [ -f "$file" ]; then
            log "${GREEN}âœ“ Production file intact: $file${NC}"
        else
            log "${RED}âœ— Production file missing: $file${NC}"
            all_good=false
        fi
    done
    
    # Check Python syntax for all .py files
    log "${YELLOW}Checking Python syntax...${NC}"
    for file in *.py; do
        if [ -f "$file" ]; then
            if python3 -m py_compile "$file" 2>/dev/null; then
                log "${GREEN}âœ“ Syntax OK: $file${NC}"
                rm -f "__pycache__/${file%.py}.cpython-*.pyc" 2>/dev/null
            else
                log "${RED}âœ— Syntax error in: $file${NC}"
                all_good=false
            fi
        fi
    done
    
    # Clean up __pycache__
    rmdir __pycache__ 2>/dev/null || true
    
    if [ "$all_good" = false ]; then
        error_exit "Production integrity check failed"
    fi
    
    log "${GREEN}Production integrity verified${NC}"
}

# Post-update recommendations
post_update_recommendations() {
    log "${YELLOW}=== Post-Update Recommendations ===${NC}"
    log ""
    log "1. ${YELLOW}Test locally:${NC} Test the updated code in a staging environment if available"
    log "2. ${YELLOW}Review logs:${NC} Check $LOG_FILE for any warnings"
    log "3. ${YELLOW}Git status:${NC} Run 'git status' to review changes before committing"
    log "4. ${YELLOW}Deploy:${NC} Follow your standard Render deployment process"
    log ""
    log "${GREEN}Update completed successfully!${NC}"
    log ""
    log "Backup location: $BACKUP_DIR"
    log "To rollback if needed, run: ${YELLOW}bash $0 --rollback${NC}"
}

# Standalone rollback option
if [ "${1:-}" == "--rollback" ]; then
    # Find the most recent backup
    LATEST_BACKUP=$(ls -td ${PROD_DIR}/backup_* 2>/dev/null | head -1)
    if [ -n "$LATEST_BACKUP" ]; then
        BACKUP_DIR="$LATEST_BACKUP"
        log "${YELLOW}Rolling back to: $BACKUP_DIR${NC}"
        rollback
        exit 0
    else
        log "${RED}No backup found for rollback${NC}"
        exit 1
    fi
fi

# Main execution
main() {
    log "${GREEN}=== Bayaan Production Update Script ===${NC}"
    log "Started at: $(date)"
    log "Dev source: $DEV_DIR"
    log "Production: $PROD_DIR"
    log ""
    
    # Execute update steps
    preflight_checks
    create_backup
    update_files
    verify_production
    post_update_recommendations
    
    log "Completed at: $(date)"
}

# Run main function
main "$@"


================================================
FILE: backup_20250727_213400/update_server.sh
================================================
#!/bin/bash

# Bayaan Server Update Script
# Updates production server with development files

set -e  # Exit on error

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

# Paths
PROD_DIR="/mnt/c/Users/hassa/OneDrive/Desktop/0.2 Bayan/bayaan-server-production"
DEV_DIR="/mnt/c/Users/hassa/OneDrive/Desktop/0.2 Bayan/Dev/bayan-platform-admin-login/Backend/LiveKit-ai-translation/server"
BACKUP_DIR="$PROD_DIR/backup_$(date +%Y%m%d_%H%M%S)"

echo -e "${GREEN}=== Bayaan Server Update Script ===${NC}"
echo "Production Dir: $PROD_DIR"
echo "Development Dir: $DEV_DIR"
echo ""

# Check if DEV directory exists
if [ ! -d "$DEV_DIR" ]; then
    echo -e "${RED}Error: Development directory not found!${NC}"
    exit 1
fi

# Step 1: Create backup
echo -e "${YELLOW}Step 1: Creating backup...${NC}"
mkdir -p "$BACKUP_DIR"

# Backup files that will be updated
cp "$PROD_DIR/main.py" "$BACKUP_DIR/" 2>/dev/null || true
cp "$PROD_DIR/resource_management.py" "$BACKUP_DIR/" 2>/dev/null || true
cp "$PROD_DIR/translation_helpers.py" "$BACKUP_DIR/" 2>/dev/null || true
cp "$PROD_DIR/broadcasting.py" "$BACKUP_DIR/" 2>/dev/null || true

echo -e "${GREEN}âœ“ Backup created at: $BACKUP_DIR${NC}"

# Step 2: Copy development files to production
echo -e "${YELLOW}Step 2: Copying development files...${NC}"

# Copy the main files
cp "$DEV_DIR/main.py" "$PROD_DIR/"
echo "  âœ“ Copied main.py"

cp "$DEV_DIR/resource_management.py" "$PROD_DIR/"
echo "  âœ“ Copied resource_management.py"

cp "$DEV_DIR/translation_helpers.py" "$PROD_DIR/"
echo "  âœ“ Copied translation_helpers.py"

cp "$DEV_DIR/broadcasting.py" "$PROD_DIR/"
echo "  âœ“ Copied broadcasting.py"

# Check if other files need updating
echo -e "${YELLOW}Step 3: Checking other files...${NC}"

# List of other files that might need updating
OTHER_FILES=("config.py" "database.py" "text_processing.py" "translator.py" "webhook_handler.py" "prompt_builder.py")

for file in "${OTHER_FILES[@]}"; do
    if [ -f "$DEV_DIR/$file" ]; then
        # Check if files are different
        if ! cmp -s "$PROD_DIR/$file" "$DEV_DIR/$file"; then
            echo -e "  ${YELLOW}! $file differs between DEV and PROD${NC}"
            cp "$PROD_DIR/$file" "$BACKUP_DIR/" 2>/dev/null || true
            cp "$DEV_DIR/$file" "$PROD_DIR/"
            echo "    âœ“ Updated $file"
        else
            echo "  - $file is identical (no update needed)"
        fi
    fi
done

# Step 4: Log the update
echo -e "${YELLOW}Step 4: Creating update log...${NC}"
cat > "$PROD_DIR/update_$(date +%Y%m%d_%H%M%S).log" << EOF
Update performed at: $(date)
Files updated:
- main.py (added heartbeat monitoring and sentence tracking)
- resource_management.py (added HeartbeatMonitor class)
- translation_helpers.py (added sentence_id parameter)
- broadcasting.py (added sentence context support)

Key improvements:
1. Heartbeat monitoring - Detects stuck participant sessions (45s timeout)
2. Sentence tracking - Unique IDs for better UI synchronization
3. Fragment handling - Improved real-time display
4. Resource management - Better cleanup and monitoring

Backup location: $BACKUP_DIR
EOF

echo -e "${GREEN}âœ“ Update log created${NC}"

# Step 5: Verify installation
echo -e "${YELLOW}Step 5: Verifying installation...${NC}"

# Check if key imports work
python3 -c "
import sys
sys.path.insert(0, '$PROD_DIR')
try:
    from resource_management import HeartbeatMonitor
    print('  âœ“ HeartbeatMonitor class imported successfully')
except ImportError as e:
    print('  âœ— Failed to import HeartbeatMonitor:', e)
    sys.exit(1)
"

if [ $? -eq 0 ]; then
    echo -e "${GREEN}âœ“ Verification passed${NC}"
else
    echo -e "${RED}âœ— Verification failed${NC}"
    echo -e "${YELLOW}Rolling back...${NC}"
    # Rollback
    for file in "$BACKUP_DIR"/*.py; do
        if [ -f "$file" ]; then
            filename=$(basename "$file")
            cp "$file" "$PROD_DIR/$filename"
        fi
    done
    echo -e "${GREEN}âœ“ Rollback completed${NC}"
    exit 1
fi

echo ""
echo -e "${GREEN}=== Update Complete ===${NC}"
echo ""
echo "Next steps:"
echo "1. Review the changes in your version control system"
echo "2. Restart the Bayaan server:"
echo "   - If using systemd: sudo systemctl restart bayaan"
echo "   - If using PM2: pm2 restart bayaan"
echo "   - If running directly: restart the Python process"
echo "3. Monitor logs for any errors"
echo "4. Test the heartbeat monitoring feature"
echo ""
echo "To rollback if needed:"
echo "  cp $BACKUP_DIR/*.py $PROD_DIR/"
echo ""


================================================
FILE: backup_20250727_213400/webhook_handler.py
================================================
#!/usr/bin/env python3
"""
Webhook handler for Supabase integration
Receives notifications about room creation and management from the dashboard
"""

import asyncio
import json
import logging
import os
from aiohttp import web
from typing import Dict, Any

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Webhook secret for validation (should match Supabase webhook secret)
WEBHOOK_SECRET = os.environ.get("SUPABASE_WEBHOOK_SECRET", "")

class WebhookHandler:
    def __init__(self):
        self.active_sessions: Dict[str, Dict[str, Any]] = {}
        
    async def handle_room_created(self, payload: dict):
        """Handle room creation webhook from Supabase"""
        try:
            # Extract room information (aligning with your Supabase schema)
            room_data = payload.get("record", {})
            room_name = room_data.get("Livekit_room_name")  # Note: Capital L in your schema
            mosque_id = room_data.get("mosque_id")
            room_id = room_data.get("id")
            room_title = room_data.get("Title")
            transcription_language = room_data.get("transcription_language")
            translation_language = room_data.get("translation__language")  # Note: double underscore
            
            if not room_name or not mosque_id:
                logger.error(f"Missing required fields in room creation webhook: {payload}")
                return {"status": "error", "message": "Missing Livekit_room_name or mosque_id"}
            
            # Store session information
            self.active_sessions[room_name] = {
                "room_id": room_id,
                "mosque_id": mosque_id,
                "room_title": room_title,
                "transcription_language": transcription_language or "ar",  # Default to Arabic
                "translation_language": translation_language or "nl",     # Default to Dutch
                "created_at": room_data.get("created_at"),
                "status": "active"
            }
            
            logger.info(f"ðŸ›ï¸ Room created for mosque {mosque_id}: {room_name} (ID: {room_id})")
            logger.info(f"ðŸ—£ï¸ Transcription: {transcription_language}, Translation: {translation_language}")
            logger.info(f"ðŸ“Š Active sessions: {len(self.active_sessions)}")
            
            return {"status": "success", "room_name": room_name, "room_id": room_id}
            
        except Exception as e:
            logger.error(f"Error handling room creation webhook: {e}")
            return {"status": "error", "message": str(e)}
    
    async def handle_room_deleted(self, payload: dict):
        """Handle room deletion webhook from Supabase"""
        try:
            # Extract room information
            room_data = payload.get("old_record", {})
            room_name = room_data.get("livekit_room_name")
            
            if room_name and room_name in self.active_sessions:
                del self.active_sessions[room_name]
                logger.info(f"ðŸ—‘ï¸ Room deleted: {room_name}")
                logger.info(f"ðŸ“Š Active sessions: {len(self.active_sessions)}")
            
            return {"status": "success", "room_name": room_name}
            
        except Exception as e:
            logger.error(f"Error handling room deletion webhook: {e}")
            return {"status": "error", "message": str(e)}
    
    async def handle_session_started(self, payload: dict):
        """Handle session start webhook from Supabase"""
        try:
            session_data = payload.get("record", {})
            room_id = session_data.get("room_id")
            session_id = session_data.get("id")
            mosque_id = session_data.get("mosque_id")
            logging_enabled = session_data.get("logging_enabled", False)
            
            logger.info(f"ðŸŽ¤ Session started: {session_id} for room {room_id}, mosque {mosque_id}")
            logger.info(f"ðŸ“ Logging enabled: {logging_enabled}")
            
            # Find matching room by room_id and update with session info
            room_found = False
            for room_name, room_info in self.active_sessions.items():
                if room_info.get("room_id") == room_id:
                    room_info["session_id"] = session_id
                    room_info["session_started_at"] = session_data.get("started_at")
                    room_info["logging_enabled"] = logging_enabled
                    room_info["status"] = "recording" if logging_enabled else "active"
                    logger.info(f"ðŸ›ï¸ Updated room {room_name} with session {session_id}")
                    room_found = True
                    break
            
            if not room_found:
                # Create temporary session entry if room not found
                logger.warning(f"âš ï¸ Room not found for session {session_id}, creating temporary entry")
                temp_room_name = f"session_{session_id[:8]}"
                self.active_sessions[temp_room_name] = {
                    "room_id": room_id,
                    "mosque_id": mosque_id,
                    "session_id": session_id,
                    "session_started_at": session_data.get("started_at"),
                    "logging_enabled": logging_enabled,
                    "status": "recording" if logging_enabled else "active",
                    "transcription_language": "ar",  # Default
                    "translation_language": "nl"    # Default
                }
                    
            return {"status": "success", "session_id": session_id, "logging_enabled": logging_enabled}
            
        except Exception as e:
            logger.error(f"Error handling session start webhook: {e}")
            return {"status": "error", "message": str(e)}
    
    async def handle_session_ended(self, payload: dict):
        """Handle session end webhook from Supabase"""
        try:
            session_data = payload.get("record", {})
            session_id = session_data.get("id")
            
            # Update session status
            for room_name, room_info in self.active_sessions.items():
                if room_info.get("session_id") == session_id:
                    room_info["session_ended_at"] = session_data.get("ended_at")
                    room_info["status"] = "ended"
                    logger.info(f"ðŸ›‘ Session ended for room {room_name}: {session_id}")
                    break
                    
            return {"status": "success", "session_id": session_id}
            
        except Exception as e:
            logger.error(f"Error handling session end webhook: {e}")
            return {"status": "error", "message": str(e)}
    
    def get_room_context(self, room_name: str) -> Dict[str, Any]:
        """Get tenant context for a specific room"""
        return self.active_sessions.get(room_name, {})

# Global webhook handler instance
webhook_handler = WebhookHandler()

async def handle_webhook(request):
    """Main webhook endpoint handler"""
    try:
        # Validate webhook secret if configured
        if WEBHOOK_SECRET:
            webhook_signature = request.headers.get("X-Supabase-Signature", "")
            # TODO: Implement proper signature validation
            
        # Parse webhook payload
        payload = await request.json()
        webhook_type = payload.get("type")
        table = payload.get("table")
        
        logger.info(f"ðŸ“¨ Received webhook: type={webhook_type}, table={table}")
        
        # Route to appropriate handler
        result = {"status": "error", "message": "Unknown webhook type"}
        
        if table == "rooms":
            if webhook_type == "INSERT":
                result = await webhook_handler.handle_room_created(payload)
            elif webhook_type == "DELETE":
                result = await webhook_handler.handle_room_deleted(payload)
                
        elif table == "room_sessions":
            if webhook_type == "INSERT":
                result = await webhook_handler.handle_session_started(payload)
            elif webhook_type == "UPDATE":
                # Check if session is ending
                if payload.get("record", {}).get("ended_at"):
                    result = await webhook_handler.handle_session_ended(payload)
                    
        return web.json_response(result)
        
    except json.JSONDecodeError:
        return web.json_response({"status": "error", "message": "Invalid JSON"}, status=400)
    except Exception as e:
        logger.error(f"Error processing webhook: {e}")
        return web.json_response({"status": "error", "message": str(e)}, status=500)

async def handle_status(request):
    """Status endpoint to check webhook handler health"""
    return web.json_response({
        "status": "healthy",
        "active_sessions": len(webhook_handler.active_sessions),
        "sessions": list(webhook_handler.active_sessions.keys())
    })

async def start_webhook_server():
    """Start the webhook server"""
    app = web.Application()
    
    # Add routes
    app.router.add_post('/webhook', handle_webhook)
    app.router.add_get('/status', handle_status)
    
    # Add CORS middleware
    @web.middleware
    async def cors_middleware(request, handler):
        if request.method == 'OPTIONS':
            return web.Response(headers={
                'Access-Control-Allow-Origin': '*',
                'Access-Control-Allow-Methods': 'POST, GET, OPTIONS',
                'Access-Control-Allow-Headers': 'Content-Type, X-Supabase-Signature',
            })
        response = await handler(request)
        response.headers['Access-Control-Allow-Origin'] = '*'
        return response
    
    app.middlewares.append(cors_middleware)
    
    # Start server
    runner = web.AppRunner(app)
    await runner.setup()
    site = web.TCPSite(runner, '0.0.0.0', 8767)
    await site.start()
    
    logger.info("ðŸš€ Webhook server started on http://0.0.0.0:8767")
    logger.info("ðŸ“¨ Webhook endpoint: POST http://0.0.0.0:8767/webhook")
    logger.info("ðŸ“Š Status endpoint: GET http://0.0.0.0:8767/status")
    
    try:
        await asyncio.Future()  # Run forever
    except KeyboardInterrupt:
        logger.info("Shutting down webhook server...")
        await runner.cleanup()

# Export the handler for use in main.py
def get_room_context(room_name: str) -> Dict[str, Any]:
    """Get tenant context for a room from webhook handler"""
    return webhook_handler.get_room_context(room_name)

if __name__ == "__main__":
    try:
        asyncio.run(start_webhook_server())
    except KeyboardInterrupt:
        logger.info("Webhook server stopped by user")
    except Exception as e:
        logger.error(f"Webhook server error: {e}")


================================================
FILE: backup_20250728_212515/broadcasting.py
================================================
"""
Broadcasting module for LiveKit AI Translation Server.
Handles real-time broadcasting of transcriptions and translations to displays.
"""
import asyncio
import logging
import hashlib
import uuid
from typing import Optional, Dict, Any
from datetime import datetime

from config import get_config
from database import broadcast_to_channel, store_transcript_in_database

logger = logging.getLogger("transcriber.broadcasting")
config = get_config()


class BroadcastError(Exception):
    """Custom exception for broadcasting-related errors."""
    pass


async def broadcast_to_displays(
    message_type: str, 
    language: str, 
    text: str, 
    tenant_context: Optional[Dict[str, Any]] = None,
    sentence_context: Optional[Dict[str, Any]] = None
) -> bool:
    """
    Send transcription/translation to frontend via Supabase Broadcast and store in database.
    
    This function handles both real-time broadcasting and database storage of
    transcriptions and translations. It uses Supabase's broadcast feature for
    real-time updates and stores the data for persistence.
    
    Args:
        message_type: Type of message ("transcription" or "translation")
        language: Language code (e.g., "ar", "nl")
        text: The text content to broadcast
        tenant_context: Optional context containing room_id, mosque_id, etc.
        sentence_context: Optional context for sentence tracking (sentence_id, is_complete, etc.)
        
    Returns:
        bool: True if broadcast was successful, False otherwise
    """
    if not text or not text.strip():
        logger.debug("Empty text provided, skipping broadcast")
        return False
    
    success = False
    
    # Phase 1: Immediate broadcast via Supabase for real-time display
    if tenant_context and tenant_context.get("room_id") and tenant_context.get("mosque_id"):
        try:
            channel_name = f"live-transcription-{tenant_context['room_id']}-{tenant_context['mosque_id']}"
            
            # Generate unique message ID based on timestamp and content hash
            timestamp = datetime.utcnow().isoformat() + "Z"
            text_hash = hashlib.md5(text.encode('utf-8')).hexdigest()[:8]
            msg_id = f"{timestamp}_{text_hash}"
            
            # Build payload with optional sentence context
            data_payload = {
                "text": text,
                "language": language,
                "timestamp": timestamp,
                "msg_id": msg_id
            }
            
            # Add sentence context if provided
            if sentence_context:
                data_payload.update({
                    "sentence_id": sentence_context.get("sentence_id"),
                    "is_complete": sentence_context.get("is_complete", False),
                    "is_fragment": sentence_context.get("is_fragment", True)
                })
            
            payload = {
                "type": message_type,
                "room_id": tenant_context["room_id"],
                "mosque_id": tenant_context["mosque_id"],
                "data": data_payload
            }
            
            # Use the broadcast_to_channel function from database module
            success = await broadcast_to_channel(channel_name, message_type, payload)
            
            if success:
                logger.info(
                    f"ðŸ“¡ LIVE: Sent {message_type} ({language}) via Supabase broadcast: "
                    f"{text[:50]}{'...' if len(text) > 50 else ''}"
                )
            else:
                logger.warning(f"âš ï¸ Failed to broadcast {message_type} to Supabase")
                
        except Exception as e:
            logger.error(f"âŒ Broadcast error: {e}")
            success = False
    else:
        logger.warning("âš ï¸ Missing tenant context for Supabase broadcast")
    
    # Phase 2: Direct database storage (no batching)
    if tenant_context and tenant_context.get("room_id") and tenant_context.get("mosque_id"):
        try:
            # Store directly in database using existing function
            # Use create_task to avoid blocking the broadcast with proper error handling
            task = asyncio.create_task(
                _store_with_error_handling(message_type, language, text, tenant_context, sentence_context)
            )
            task.add_done_callback(lambda t: None if not t.exception() else logger.error(f"Storage task failed: {t.exception()}"))
            logger.debug(
                f"ðŸ’¾ DIRECT: Storing {message_type} directly to database "
                f"for room {tenant_context['room_id']}"
            )
        except Exception as e:
            logger.error(f"âŒ Failed to initiate database storage: {e}")
    else:
        logger.warning("âš ï¸ Missing tenant context for database storage")
    
    return success


async def _store_with_error_handling(
    message_type: str, 
    language: str, 
    text: str, 
    tenant_context: Dict[str, Any],
    sentence_context: Optional[Dict[str, Any]] = None
) -> None:
    """
    Store transcript with proper error handling.
    
    This is a wrapper around store_transcript_in_database that ensures
    errors don't propagate and crash the application.
    
    Args:
        message_type: Type of message ("transcription" or "translation")
        language: Language code
        text: The text content to store
        tenant_context: Context containing room_id, mosque_id, etc.
        sentence_context: Optional context containing sentence_id, is_complete, is_fragment
    """
    try:
        success = await store_transcript_in_database(
            message_type, language, text, tenant_context, sentence_context
        )
        if not success:
            logger.warning(
                f"âš ï¸ Failed to store {message_type} in database for "
                f"room {tenant_context.get('room_id')}"
            )
    except Exception as e:
        logger.error(
            f"âŒ Database storage error for {message_type}: {e}\n"
            f"Room: {tenant_context.get('room_id')}, "
            f"Language: {language}"
        )


async def broadcast_batch(
    messages: list[tuple[str, str, str, Dict[str, Any]]]
) -> Dict[str, int]:
    """
    Broadcast multiple messages in batch for efficiency.
    
    Args:
        messages: List of tuples (message_type, language, text, tenant_context)
        
    Returns:
        Dictionary with counts of successful and failed broadcasts
    """
    results = {"success": 0, "failed": 0}
    
    # Process all broadcasts concurrently
    tasks = []
    for message_type, language, text, tenant_context in messages:
        task = broadcast_to_displays(message_type, language, text, tenant_context)
        tasks.append(task)
    
    # Wait for all broadcasts to complete
    broadcast_results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # Count results
    for result in broadcast_results:
        if isinstance(result, Exception):
            results["failed"] += 1
            logger.error(f"Batch broadcast error: {result}")
        elif result:
            results["success"] += 1
        else:
            results["failed"] += 1
    
    logger.info(
        f"ðŸ“Š Batch broadcast complete: "
        f"{results['success']} successful, {results['failed']} failed"
    )
    
    return results


def create_broadcast_payload(
    message_type: str,
    language: str,
    text: str,
    room_id: int,
    mosque_id: int,
    additional_data: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """
    Create a standardized broadcast payload.
    
    Args:
        message_type: Type of message
        language: Language code
        text: The text content
        room_id: Room ID
        mosque_id: Mosque ID
        additional_data: Optional additional data to include
        
    Returns:
        Formatted payload dictionary
    """
    # Generate unique message ID
    timestamp = datetime.utcnow().isoformat() + "Z"
    text_hash = hashlib.md5(text.encode('utf-8')).hexdigest()[:8]
    msg_id = f"{timestamp}_{text_hash}"
    
    payload = {
        "type": message_type,
        "room_id": room_id,
        "mosque_id": mosque_id,
        "data": {
            "text": text,
            "language": language,
            "timestamp": timestamp,
            "msg_id": msg_id
        }
    }
    
    if additional_data:
        payload["data"].update(additional_data)
    
    return payload


def get_channel_name(room_id: int, mosque_id: int) -> str:
    """
    Generate the channel name for a room.
    
    Args:
        room_id: Room ID
        mosque_id: Mosque ID
        
    Returns:
        Channel name string
    """
    return f"live-transcription-{room_id}-{mosque_id}"


================================================
FILE: backup_20250728_212515/config.py
================================================
"""
Configuration management for LiveKit AI Translation Server.
Centralizes all configuration values and environment variables.
"""
import os
from dataclasses import dataclass
from typing import Optional, Dict
# Try to load dotenv if available
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    # dotenv not available, will use system environment variables
    pass


@dataclass
class SupabaseConfig:
    """Supabase database configuration."""
    url: str
    service_role_key: str
    anon_key: Optional[str] = None
    
    # Timeouts
    http_timeout: float = 5.0  # General HTTP request timeout
    broadcast_timeout: float = 2.0  # Broadcast API timeout
    
    @classmethod
    def from_env(cls) -> 'SupabaseConfig':
        """Load configuration from environment variables."""
        url = os.getenv('SUPABASE_URL')
        service_key = os.getenv('SUPABASE_SERVICE_ROLE_KEY')
        anon_key = os.getenv('SUPABASE_ANON_KEY')
        
        if not url:
            raise ValueError("SUPABASE_URL environment variable is required")
        if not service_key:
            raise ValueError("SUPABASE_SERVICE_ROLE_KEY environment variable is required")
            
        return cls(
            url=url,
            service_role_key=service_key,
            anon_key=anon_key
        )


@dataclass
class TranslationConfig:
    """Translation-related configuration."""
    # Language settings
    default_source_language: str = "ar"  # Arabic
    default_target_language: str = "nl"  # Dutch
    
    # Context window settings
    use_context: bool = True
    max_context_pairs: int = 6  # Number of translation pairs to keep in memory
    
    # Timing settings
    translation_delay: float = 10.0  # Delay before translating incomplete sentences
    
    # Supported languages
    supported_languages: Dict[str, Dict[str, str]] = None
    
    def __post_init__(self):
        if self.supported_languages is None:
            self.supported_languages = {
                "ar": {"name": "Arabic", "flag": "ðŸ‡¸ðŸ‡¦"},
                "en": {"name": "English", "flag": "ðŸ‡ºðŸ‡¸"},
                "es": {"name": "Spanish", "flag": "ðŸ‡ªðŸ‡¸"},
                "fr": {"name": "French", "flag": "ðŸ‡«ðŸ‡·"},
                "de": {"name": "German", "flag": "ðŸ‡©ðŸ‡ª"},
                "ja": {"name": "Japanese", "flag": "ðŸ‡¯ðŸ‡µ"},
                "nl": {"name": "Dutch", "flag": "ðŸ‡³ðŸ‡±"},
            }
    
    def get_target_language(self, room_config: Optional[Dict[str, any]] = None) -> str:
        """Get target language from room config or use default."""
        if room_config:
            # Check both possible field names (translation_language and translation__language)
            if 'translation_language' in room_config and room_config['translation_language']:
                return room_config['translation_language']
            elif 'translation__language' in room_config and room_config['translation__language']:
                return room_config['translation__language']
        return self.default_target_language
    
    def get_source_language(self, room_config: Optional[Dict[str, any]] = None) -> str:
        """Get source language from room config or use default."""
        if room_config and 'transcription_language' in room_config and room_config['transcription_language']:
            return room_config['transcription_language']
        return self.default_source_language
    
    def get_context_window_size(self, room_config: Optional[Dict[str, any]] = None) -> int:
        """Get context window size from room config or use default."""
        if room_config and 'context_window_size' in room_config and room_config['context_window_size']:
            # Ensure the value is within valid range (3-20)
            size = int(room_config['context_window_size'])
            return max(3, min(20, size))
        return self.max_context_pairs


@dataclass
class SpeechmaticsConfig:
    """Speechmatics STT configuration."""
    language: str = "ar"
    operating_point: str = "enhanced"
    enable_partials: bool = False  # Disabled to reduce API costs - frontend doesn't use partials
    max_delay: float = 2.0
    punctuation_sensitivity: float = 0.5
    diarization: str = "speaker"
    
    def with_room_settings(self, room_config: Optional[Dict[str, any]] = None) -> 'SpeechmaticsConfig':
        """Create a new config with room-specific overrides."""
        if not room_config:
            return self
            
        # Create a copy with room-specific overrides
        import copy
        new_config = copy.deepcopy(self)
        
        # Override with room settings if available
        if 'transcription_language' in room_config and room_config['transcription_language']:
            new_config.language = room_config['transcription_language']
        if 'max_delay' in room_config and room_config['max_delay'] is not None:
            new_config.max_delay = float(room_config['max_delay'])
        if 'punctuation_sensitivity' in room_config and room_config['punctuation_sensitivity'] is not None:
            new_config.punctuation_sensitivity = float(room_config['punctuation_sensitivity'])
            
        return new_config


@dataclass
class ApplicationConfig:
    """Main application configuration."""
    # Component configurations
    supabase: SupabaseConfig
    translation: TranslationConfig
    speechmatics: SpeechmaticsConfig
    
    # Logging
    log_level: str = "INFO"
    
    # Testing/Development
    default_mosque_id: int = 1
    test_mosque_id: int = 546012  # Hardcoded test mosque
    test_room_id: int = 192577    # Hardcoded test room
    
    @classmethod
    def load(cls) -> 'ApplicationConfig':
        """Load complete configuration from environment and defaults."""
        return cls(
            supabase=SupabaseConfig.from_env(),
            translation=TranslationConfig(),
            speechmatics=SpeechmaticsConfig()
        )
    
    def validate(self) -> None:
        """Validate configuration at startup."""
        # Print configuration status
        print("ðŸ”§ Configuration loaded:")
        print(f"   SUPABASE_URL: {self.supabase.url[:50]}...")
        print(f"   SERVICE_KEY: {'âœ… SET' if self.supabase.service_role_key else 'âŒ NOT SET'}")
        print(f"   Default Languages: {self.translation.default_source_language} â†’ {self.translation.default_target_language}")
        print(f"   Context Window: {'âœ… ENABLED' if self.translation.use_context else 'âŒ DISABLED'} ({self.translation.max_context_pairs} pairs)")
        print(f"   STT Defaults: delay={self.speechmatics.max_delay}s, punctuation={self.speechmatics.punctuation_sensitivity}, partials={'âœ…' if self.speechmatics.enable_partials else 'âŒ'}")


# Global configuration instance
_config: Optional[ApplicationConfig] = None


def get_config() -> ApplicationConfig:
    """Get or create the global configuration instance."""
    global _config
    if _config is None:
        _config = ApplicationConfig.load()
        _config.validate()
    return _config


def reset_config() -> None:
    """Reset configuration (mainly for testing)."""
    global _config
    _config = None


================================================
FILE: backup_20250728_212515/database.py
================================================
"""
Database operations for LiveKit AI Translation Server.
Handles all Supabase database interactions with connection pooling and async support.
FIXED: Thread-safe connection pool that works with LiveKit's multi-process architecture.
"""
import asyncio
import logging
import uuid
from typing import Optional, Dict, Any
from datetime import datetime
import aiohttp
from contextlib import asynccontextmanager
import threading

from config import get_config
from database_enhanced import (
    ensure_active_session_atomic as _ensure_active_session_atomic,
    SessionHealthMonitor,
    update_session_heartbeat_enhanced
)

logger = logging.getLogger("transcriber.database")
config = get_config()


class ThreadSafeDatabasePool:
    """Thread-safe database connection pool that creates separate pools per thread/process."""
    
    def __init__(self, max_connections: int = 10):
        self.max_connections = max_connections
        self._local = threading.local()
        self._lock = threading.Lock()
        
    async def get_session(self) -> aiohttp.ClientSession:
        """Get or create a session for the current thread."""
        # Check if current thread has a session
        if not hasattr(self._local, 'session') or self._local.session is None or self._local.session.closed:
            # Create new session for this thread
            connector = aiohttp.TCPConnector(
                limit=self.max_connections,
                limit_per_host=self.max_connections,
                force_close=True  # Force close to avoid connection issues
            )
            self._local.session = aiohttp.ClientSession(
                connector=connector,
                trust_env=True  # Trust environment proxy settings
            )
            logger.debug(f"Created new connection pool for thread {threading.current_thread().ident}")
        
        return self._local.session
    
    async def close(self):
        """Close the session for current thread."""
        if hasattr(self._local, 'session') and self._local.session and not self._local.session.closed:
            await self._local.session.close()
            self._local.session = None
            logger.debug(f"Closed connection pool for thread {threading.current_thread().ident}")


# Use thread-safe pool
_pool = ThreadSafeDatabasePool()

# Initialize health monitor
_health_monitor = SessionHealthMonitor()


@asynccontextmanager
async def get_db_headers():
    """Get headers for Supabase API requests."""
    if not config.supabase.service_role_key:
        raise ValueError("SUPABASE_SERVICE_ROLE_KEY not configured")
    
    yield {
        'apikey': config.supabase.service_role_key,
        'Authorization': f'Bearer {config.supabase.service_role_key}',
        'Content-Type': 'application/json'
    }


async def ensure_active_session(room_id: int, mosque_id: int) -> Optional[str]:
    """
    Enhanced version with atomic session creation and ghost prevention.
    """
    try:
        session = await _pool.get_session()
        async with get_db_headers() as headers:
            return await _ensure_active_session_atomic(
                room_id, mosque_id, session, headers
            )
    except Exception as e:
        logger.error(f"Failed to ensure active session: {e}")
        return None


async def store_transcript_in_database(
    message_type: str, 
    language: str, 
    text: str, 
    tenant_context: Dict[str, Any],
    sentence_context: Optional[Dict[str, Any]] = None
) -> bool:
    """
    Store transcription/translation in Supabase database.
    
    Args:
        message_type: Either "transcription" or "translation"
        language: Language code (e.g., "ar", "nl")
        text: The text to store
        tenant_context: Context containing room_id, mosque_id, session_id
        sentence_context: Optional context containing sentence_id, is_complete, is_fragment
        
    Returns:
        bool: True if successful, False otherwise
    """
    try:
        if not config.supabase.service_role_key:
            logger.error("âŒ SUPABASE_SERVICE_ROLE_KEY not found - cannot store transcripts")
            return False
            
        room_id = tenant_context.get("room_id")
        mosque_id = tenant_context.get("mosque_id")
        session_id = tenant_context.get("session_id")
        
        if not room_id or not mosque_id:
            logger.warning(f"âš ï¸ Missing room context: room_id={room_id}, mosque_id={mosque_id}")
            return False
            
        # Ensure we have an active session
        if not session_id:
            session_id = await ensure_active_session(room_id, mosque_id)
            if session_id:
                tenant_context["session_id"] = session_id
            else:
                logger.error("âŒ Could not establish session - skipping database storage")
                return False
        
        # Prepare transcript data
        transcript_data = {
            "room_id": room_id,
            "session_id": session_id,
            "timestamp": datetime.utcnow().isoformat() + "Z",
        }
        
        # Add sentence context if provided
        if sentence_context:
            transcript_data["sentence_id"] = sentence_context.get("sentence_id")
            transcript_data["is_complete"] = sentence_context.get("is_complete", False)
            transcript_data["is_fragment"] = sentence_context.get("is_fragment", True)
        
        # Set appropriate field based on message type
        if message_type == "transcription":
            transcript_data["transcription_segment"] = text
        else:  # translation
            transcript_data["translation_segment"] = text
            
        # Store in database
        session = await _pool.get_session()
        async with get_db_headers() as headers:
            timeout = aiohttp.ClientTimeout(total=config.supabase.http_timeout)
            
            try:
                async with session.post(
                    f"{config.supabase.url}/rest/v1/transcripts",
                    json=transcript_data,
                    headers={**headers, 'Prefer': 'return=minimal'},
                    timeout=timeout
                ) as response:
                    if response.status in [200, 201]:
                        logger.debug(f"âœ… Stored {message_type} in database: room_id={room_id}, session_id={session_id[:8]}")
                        return True
                    else:
                        error_text = await response.text()
                        logger.warning(f"âš ï¸ Database storage failed with status {response.status}: {error_text}")
                        return False
            except asyncio.TimeoutError:
                logger.warning("Timeout storing transcript")
                return False
            except Exception as e:
                logger.error(f"Error storing transcript: {e}")
                return False
                    
    except Exception as e:
        logger.error(f"âŒ Database storage error: {e}")
        return False


async def query_room_by_name(room_name: str) -> Optional[Dict[str, Any]]:
    """
    Query room information by LiveKit room name.
    
    Args:
        room_name: The LiveKit room name
        
    Returns:
        Room data dictionary or None if not found
    """
    try:
        session = await _pool.get_session()
        async with get_db_headers() as headers:
            url = f"{config.supabase.url}/rest/v1/rooms"
            params = {"Livekit_room_name": f"eq.{room_name}"}
            
            timeout = aiohttp.ClientTimeout(total=config.supabase.http_timeout)
            
            try:
                async with session.get(url, headers=headers, params=params, timeout=timeout) as response:
                    if response.status == 200:
                        rooms = await response.json()
                        if rooms and len(rooms) > 0:
                            return rooms[0]
                    else:
                        error_text = await response.text()
                        logger.warning(f"Failed to query room: {response.status} - {error_text}")
            except asyncio.TimeoutError:
                logger.warning("Timeout querying room")
            except Exception as e:
                logger.error(f"Error querying room: {e}")
        
        return None
        
    except Exception as e:
        logger.error(f"âŒ Room query failed: {e}")
        return None


async def get_active_session_for_room(room_id: int) -> Optional[str]:
    """
    Get the active session ID for a room if one exists.
    
    Args:
        room_id: The room ID
        
    Returns:
        Session ID or None if no active session
    """
    try:
        session = await _pool.get_session()
        async with get_db_headers() as headers:
            url = f"{config.supabase.url}/rest/v1/room_sessions"
            params = {
                "room_id": f"eq.{room_id}",
                "status": "eq.active",
                "select": "id",
                "order": "started_at.desc",
                "limit": "1"
            }
            
            timeout = aiohttp.ClientTimeout(total=config.supabase.http_timeout)
            
            try:
                async with session.get(url, headers=headers, params=params, timeout=timeout) as response:
                    if response.status == 200:
                        sessions = await response.json()
                        if sessions and len(sessions) > 0:
                            return sessions[0].get("id")
            except asyncio.TimeoutError:
                logger.warning("Timeout getting active session")
            except Exception as e:
                logger.error(f"Error getting active session: {e}")
        
        return None
        
    except Exception as e:
        logger.error(f"âŒ Active session query failed: {e}")
        return None


async def broadcast_to_channel(
    channel_name: str,
    event_type: str,
    payload: Dict[str, Any]
) -> bool:
    """
    Broadcast a message to a Supabase channel.
    
    Args:
        channel_name: The channel to broadcast to
        event_type: The event type (e.g., "transcription", "translation")
        payload: The data to broadcast
        
    Returns:
        bool: True if successful, False otherwise
    """
    try:
        if not config.supabase.service_role_key:
            logger.warning("âš ï¸ SUPABASE_SERVICE_ROLE_KEY not found - skipping broadcast")
            return False
            
        session = await _pool.get_session()
        
        async with get_db_headers() as headers:
            # Use broadcast-specific timeout
            broadcast_timeout = aiohttp.ClientTimeout(total=config.supabase.broadcast_timeout)
            
            try:
                async with session.post(
                    f"{config.supabase.url}/functions/v1/broadcast",
                    json={
                        "channel": channel_name,
                        "event": event_type,
                        "payload": payload
                    },
                    headers=headers,
                    timeout=broadcast_timeout
                ) as response:
                    if response.status == 200:
                        return True
                    else:
                        error_text = await response.text()
                        logger.warning(f"âš ï¸ Broadcast failed: {response.status} - {error_text}")
                        return False
            except asyncio.TimeoutError:
                logger.warning(f"âš ï¸ Broadcast timeout for channel {channel_name}")
                return False
            except Exception as e:
                logger.error(f"Error broadcasting: {e}")
                return False
                    
    except Exception as e:
        logger.error(f"âŒ Broadcast error: {e}")
        return False


async def query_prompt_template_for_room(room_id: int) -> Optional[Dict[str, Any]]:
    """
    Query the prompt template for a specific room.
    
    Args:
        room_id: The room ID
        
    Returns:
        Template data dictionary or None if not found
    """
    try:
        session = await _pool.get_session()
        async with get_db_headers() as headers:
            # Use the database function to get the appropriate template
            url = f"{config.supabase.url}/rest/v1/rpc/get_room_prompt_template"
            data = {"room_id": room_id}
            
            timeout = aiohttp.ClientTimeout(total=config.supabase.http_timeout)
            
            try:
                async with session.post(url, headers=headers, json=data, timeout=timeout) as response:
                    if response.status == 200:
                        result = await response.json()
                        if result and len(result) > 0:
                            template = result[0]
                            # Parse template_variables if it's a string
                            if isinstance(template.get('template_variables'), str):
                                try:
                                    import json
                                    template['template_variables'] = json.loads(template['template_variables'])
                                except:
                                    template['template_variables'] = {}
                            return template
                    else:
                        error_text = await response.text()
                        logger.warning(f"Failed to query prompt template: {response.status} - {error_text}")
            except asyncio.TimeoutError:
                logger.warning("Timeout querying prompt template")
            except Exception as e:
                logger.error(f"Error querying prompt template: {e}")
        
        return None
        
    except Exception as e:
        logger.error(f"âŒ Prompt template query failed: {e}")
        return None


async def update_session_heartbeat(session_id: str) -> bool:
    """
    Update the last_active timestamp for a session to prevent it from being cleaned up.
    
    Args:
        session_id: The session ID to update
        
    Returns:
        True if successful, False otherwise
    """
    if not session_id:
        return False
        
    try:
        session = await _pool.get_session()
        async with get_db_headers() as headers:
            # Update the last_active timestamp
            url = f"{config.supabase.url}/rest/v1/room_sessions"
            params = {"id": f"eq.{session_id}"}
            data = {"last_active": datetime.utcnow().isoformat()}
            
            timeout = aiohttp.ClientTimeout(total=config.supabase.http_timeout)
            
            try:
                async with session.patch(url, headers=headers, params=params, json=data, timeout=timeout) as response:
                    if response.status in [200, 204]:
                        logger.debug(f"ðŸ’“ Session heartbeat updated for {session_id}")
                        return True
                    else:
                        error_text = await response.text()
                        logger.warning(f"Failed to update session heartbeat: {response.status} - {error_text}")
                        return False
            except asyncio.TimeoutError:
                logger.warning(f"Timeout updating session heartbeat {session_id}")
                return False
            except Exception as e:
                logger.error(f"Error updating session heartbeat {session_id}: {e}")
                return False
                
    except Exception as e:
        logger.error(f"âŒ Failed to update session heartbeat {session_id}: {e}")
        return False


async def close_room_session(session_id: str) -> bool:
    """
    Close a room session by marking it as completed in the database.
    
    Args:
        session_id: The session ID to close
        
    Returns:
        True if successful, False otherwise
    """
    if not session_id:
        logger.warning("No session_id provided to close_room_session")
        return False
        
    try:
        session = await _pool.get_session()
        async with get_db_headers() as headers:
            # Call the cleanup_session_idempotent function
            url = f"{config.supabase.url}/rest/v1/rpc/cleanup_session_idempotent"
            data = {
                "p_session_id": session_id,
                "p_source": "agent_disconnect"
            }
            
            timeout = aiohttp.ClientTimeout(total=config.supabase.http_timeout)
            
            try:
                async with session.post(url, headers=headers, json=data, timeout=timeout) as response:
                    if response.status == 200:
                        result = await response.json()
                        logger.info(f"âœ… Session {session_id} closed successfully")
                        return True
                    else:
                        error_text = await response.text()
                        logger.error(f"Failed to close session: {response.status} - {error_text}")
                        return False
            except asyncio.TimeoutError:
                logger.warning(f"Timeout closing session {session_id}")
                return False
            except Exception as e:
                logger.error(f"Error closing session {session_id}: {e}")
                return False
                
    except Exception as e:
        logger.error(f"âŒ Failed to close session {session_id}: {e}")
        return False


async def close_database_connections():
    """Close all database connections. Call this on shutdown."""
    await _pool.close()
    logger.info("âœ… Database connections closed")


async def update_session_heartbeat_with_monitor(session_id: str) -> bool:
    """
    Update heartbeat with health monitoring.
    """
    if not session_id:
        return False
        
    try:
        session = await _pool.get_session()
        async with get_db_headers() as headers:
            # Use health monitor
            healthy = await _health_monitor.monitor_heartbeat(
                session_id, session, headers
            )
            
            if not healthy:
                if _health_monitor.should_force_cleanup(session_id):
                    logger.error(f"Session {session_id} needs force cleanup")
                    return False
                else:
                    _health_monitor.increment_recovery_attempt(session_id)
                    
            return healthy
    except Exception as e:
        logger.error(f"Heartbeat monitor error: {e}")
        return False


def get_health_monitor():
    """Export health monitor for use in main.py"""
    return _health_monitor


================================================
FILE: backup_20250728_212515/database_enhanced.py
================================================
"""
Enhanced Database operations with ghost session prevention.
This module should replace the ensure_active_session function in database.py
"""
import asyncio
import logging
from typing import Optional, Dict, Any
from datetime import datetime, timedelta
import aiohttp

from config import get_config

logger = logging.getLogger("transcriber.database_enhanced")
config = get_config()


async def ensure_active_session_atomic(
    room_id: int, 
    mosque_id: int,
    session: aiohttp.ClientSession,
    headers: Dict[str, str]
) -> Optional[str]:
    """
    Atomically ensure there's an active session for the room.
    Uses database-level locking to prevent ghost sessions.
    
    Args:
        room_id: The room ID
        mosque_id: The mosque ID
        session: Active aiohttp session
        headers: Supabase headers with auth
        
    Returns:
        Session ID or None on failure
    """
    try:
        # Use the atomic database function
        url = f"{config.supabase.url}/rest/v1/rpc/ensure_room_session_atomic"
        data = {
            "p_room_id": room_id,
            "p_mosque_id": mosque_id,
            "p_source": "livekit_agent"
        }
        
        timeout = aiohttp.ClientTimeout(total=config.supabase.http_timeout)
        
        async with session.post(
            url, 
            json=data, 
            headers=headers, 
            timeout=timeout
        ) as response:
            if response.status == 200:
                result = await response.json()
                if result and isinstance(result, list) and len(result) > 0:
                    result_data = result[0]
                elif isinstance(result, dict):
                    result_data = result
                else:
                    logger.error(f"Unexpected response format: {result}")
                    return None
                
                if 'error' in result_data:
                    logger.error(f"Database error: {result_data['error']}")
                    return None
                
                session_id = result_data.get('session_id')
                cleaned = result_data.get('cleaned_sessions', 0)
                
                if cleaned > 0:
                    logger.info(f"ðŸ§¹ Cleaned {cleaned} stale sessions before creating new one")
                
                if session_id:
                    logger.info(f"âœ… Active session ensured: {session_id}")
                    return session_id
                else:
                    logger.error("No session_id returned from atomic function")
                    return None
            else:
                error_text = await response.text()
                logger.error(f"Failed to ensure session: {response.status} - {error_text}")
                return None
                
    except asyncio.TimeoutError:
        logger.error("Timeout in ensure_active_session_atomic")
        return None
    except Exception as e:
        logger.error(f"Error in ensure_active_session_atomic: {e}")
        return None


async def update_session_heartbeat_enhanced(
    session_id: str,
    session: aiohttp.ClientSession,
    headers: Dict[str, str]
) -> bool:
    """
    Update session heartbeat with enhanced tracking.
    
    Args:
        session_id: The session ID to update
        session: Active aiohttp session
        headers: Supabase headers with auth
        
    Returns:
        True if successful, False otherwise
    """
    if not session_id:
        return False
        
    try:
        url = f"{config.supabase.url}/rest/v1/rpc/update_session_heartbeat_enhanced"
        data = {"p_session_id": session_id}
        
        timeout = aiohttp.ClientTimeout(total=5.0)  # Quick timeout for heartbeats
        
        async with session.post(
            url, 
            json=data, 
            headers=headers, 
            timeout=timeout
        ) as response:
            if response.status == 200:
                result = await response.json()
                if result and isinstance(result, list) and len(result) > 0:
                    result_data = result[0]
                elif isinstance(result, dict):
                    result_data = result
                else:
                    return False
                
                success = result_data.get('success', False)
                if success:
                    logger.debug(f"ðŸ’“ Heartbeat updated for session {session_id}")
                else:
                    reason = result_data.get('reason', 'unknown')
                    logger.warning(f"Heartbeat failed for {session_id}: {reason}")
                
                return success
            else:
                error_text = await response.text()
                logger.warning(f"Heartbeat update failed: {response.status} - {error_text}")
                return False
                
    except asyncio.TimeoutError:
        logger.warning(f"Heartbeat timeout for session {session_id}")
        return False
    except Exception as e:
        logger.error(f"Error updating heartbeat for {session_id}: {e}")
        return False


class SessionHealthMonitor:
    """
    Monitors session health and handles automatic recovery.
    """
    
    def __init__(self):
        self.missed_heartbeats: Dict[str, int] = {}
        self.recovery_attempts: Dict[str, int] = {}
        self.last_heartbeat: Dict[str, datetime] = {}
        
    async def monitor_heartbeat(
        self, 
        session_id: str,
        session: aiohttp.ClientSession,
        headers: Dict[str, str]
    ) -> bool:
        """
        Monitor heartbeat with automatic recovery on failure.
        
        Returns:
            True if healthy, False if recovery needed
        """
        try:
            # Update heartbeat
            success = await update_session_heartbeat_enhanced(
                session_id, session, headers
            )
            
            if success:
                self.missed_heartbeats[session_id] = 0
                self.recovery_attempts[session_id] = 0
                self.last_heartbeat[session_id] = datetime.utcnow()
                return True
            else:
                self.missed_heartbeats[session_id] = \
                    self.missed_heartbeats.get(session_id, 0) + 1
                
                # Check if we need recovery
                if self.missed_heartbeats[session_id] >= 3:
                    logger.warning(
                        f"Session {session_id} missed {self.missed_heartbeats[session_id]} heartbeats"
                    )
                    return False
                    
                return True
                
        except Exception as e:
            logger.error(f"Heartbeat monitoring error: {e}")
            return False
    
    def should_force_cleanup(self, session_id: str) -> bool:
        """
        Determine if a session should be forcefully cleaned up.
        """
        # Too many recovery attempts
        if self.recovery_attempts.get(session_id, 0) >= 3:
            return True
            
        # No heartbeat for too long
        last_beat = self.last_heartbeat.get(session_id)
        if last_beat and (datetime.utcnow() - last_beat) > timedelta(minutes=10):
            return True
            
        # Too many missed heartbeats
        if self.missed_heartbeats.get(session_id, 0) >= 10:
            return True
            
        return False
    
    def increment_recovery_attempt(self, session_id: str):
        """Track recovery attempts."""
        self.recovery_attempts[session_id] = \
            self.recovery_attempts.get(session_id, 0) + 1
    
    def cleanup_session_tracking(self, session_id: str):
        """Remove session from tracking."""
        self.missed_heartbeats.pop(session_id, None)
        self.recovery_attempts.pop(session_id, None)
        self.last_heartbeat.pop(session_id, None)


# Example integration in your main code:
"""
# In database.py, replace ensure_active_session with:
from database_enhanced import ensure_active_session_atomic

async def ensure_active_session(room_id: int, mosque_id: int) -> Optional[str]:
    session = await _pool.get_session()
    async with get_db_headers() as headers:
        return await ensure_active_session_atomic(
            room_id, mosque_id, session, headers
        )

# In main.py, add health monitoring:
from database_enhanced import SessionHealthMonitor

# Initialize monitor
health_monitor = SessionHealthMonitor()

# In your heartbeat periodic function:
async def update_session_heartbeat_periodic():
    while not stop_heartbeat:
        try:
            if tenant_context and tenant_context.get('session_id'):
                session_id = tenant_context['session_id']
                
                # Use health monitor
                healthy = await health_monitor.monitor_heartbeat(
                    session_id, session, headers
                )
                
                if not healthy:
                    if health_monitor.should_force_cleanup(session_id):
                        logger.error(f"Force cleanup needed for {session_id}")
                        await close_room_session(session_id)
                        break
                    else:
                        health_monitor.increment_recovery_attempt(session_id)
                        
            await asyncio.sleep(30)
        except Exception as e:
            logger.error(f"Heartbeat error: {e}")
            await asyncio.sleep(30)
"""


================================================
FILE: backup_20250728_212515/Dockerfile
================================================
# Use Python 3.10 as base image
FROM python:3.10-slim

# Set working directory
WORKDIR /app

# Install system dependencies for audio processing and LiveKit
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    portaudio19-dev \
    python3-dev \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements and install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir --root-user-action=ignore -r requirements.txt

# Copy application code
COPY . .

# Create non-root user for security
RUN useradd -m -u 1000 agent && chown -R agent:agent /app
USER agent

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
  CMD python -c "import asyncio; import sys; print('Agent is running')" || exit 1

# Default command to run the agent in production mode
CMD ["python", "main_production.py", "start"] 


================================================
FILE: backup_20250728_212515/main.py
================================================
import asyncio
import logging
import json
import time
import re
import os
import uuid
from typing import Set, Any, Dict, Optional
from collections import defaultdict, deque
from datetime import datetime, timedelta

from enum import Enum
from dataclasses import dataclass, asdict

from livekit import rtc
from livekit.agents import (
    AutoSubscribe,
    JobContext,
    JobProcess,
    JobRequest,
    WorkerOptions,
    cli,
    stt,
    utils,
)
from livekit.plugins import silero, speechmatics
from livekit.plugins.speechmatics.types import TranscriptionConfig

# Import configuration
from config import get_config, ApplicationConfig

# Import database operations
from database import (
    ensure_active_session,
    store_transcript_in_database,
    query_room_by_name,
    get_active_session_for_room,
    broadcast_to_channel,
    close_database_connections,
    get_health_monitor,
    update_session_heartbeat_with_monitor
)

# Import text processing and translation helpers
from text_processing import extract_complete_sentences
from translation_helpers import translate_sentences

# Import Translator class
from translator import Translator

# Import broadcasting function
from broadcasting import broadcast_to_displays

# Import resource management
from resource_management import ResourceManager, TaskManager, STTStreamManager

# Import webhook handler for room context
try:
    from webhook_handler import get_room_context as get_webhook_room_context
except ImportError:
    # Webhook handler not available, use empty context
    def get_webhook_room_context(room_name: str):
        return {}


# Load configuration
config = get_config()

logger = logging.getLogger("transcriber")


@dataclass
class Language:
    code: str
    name: str
    flag: str


# Build languages dictionary from config
languages = {}
for code, lang_info in config.translation.supported_languages.items():
    languages[code] = Language(
        code=code,
        name=lang_info["name"],
        flag=lang_info["flag"]
    )

LanguageCode = Enum(
    "LanguageCode",  # Name of the Enum
    {lang.name: code for code, lang in languages.items()},  # Enum entries: name -> code mapping
)


# Translator class has been moved to translator.py


def prewarm(proc: JobProcess):
    proc.userdata["vad"] = silero.VAD.load()


async def entrypoint(job: JobContext):
    # Configure source language - ARABIC as default
    # This will be the language that users are actually speaking (host/speaker language)
    source_language = config.translation.default_source_language
    
    # Initialize resource manager
    resource_manager = ResourceManager()
    
    # Initialize health monitor
    health_monitor = get_health_monitor()
    
    # Register heartbeat timeout callback
    async def on_participant_timeout(participant_id: str):
        logger.warning(f"ðŸ’” Participant {participant_id} timed out - initiating cleanup")
        # Could trigger cleanup here if needed
    
    resource_manager.heartbeat_monitor.register_callback(on_participant_timeout)
    
    # Extract tenant context from room metadata or webhook handler
    tenant_context = {}
    try:
        # Try to query Supabase directly for room information
        if job.room and job.room.name:
            logger.info(f"ðŸ” Looking up room context for: {job.room.name}")
            
            # Check if database is configured
            logger.info(f"ðŸ”‘ Supabase URL: {config.supabase.url}")
            logger.info(f"ðŸ”‘ Supabase key available: {'Yes' if config.supabase.service_role_key else 'No'}")
            
            if config.supabase.service_role_key:
                try:
                    # Query room by LiveKit room name using the new database module
                    logger.info(f"ðŸ” Querying database for room: {job.room.name}")
                    # Query room directly without task wrapper
                    room_data = await query_room_by_name(job.room.name)
                    
                    if room_data:
                        tenant_context = {
                            "room_id": room_data.get("id"),
                            "mosque_id": room_data.get("mosque_id"),
                            "room_title": room_data.get("Title"),
                            "transcription_language": room_data.get("transcription_language", "ar"),
                            "translation_language": room_data.get("translation__language", "nl"),
                            "context_window_size": room_data.get("context_window_size", 6),
                            "created_at": room_data.get("created_at")
                        }
                        # Also store the double underscore version for compatibility
                        if room_data.get("translation__language"):
                            tenant_context["translation__language"] = room_data.get("translation__language")
                        
                        logger.info(f"âœ… Found room in database: room_id={tenant_context.get('room_id')}, mosque_id={tenant_context.get('mosque_id')}")
                        logger.info(f"ðŸ—£ï¸ Languages: transcription={tenant_context.get('transcription_language')}, translation={tenant_context.get('translation_language')} (or {tenant_context.get('translation__language')})")
                        
                        # Try to get active session for this room
                        session_id = await get_active_session_for_room(tenant_context['room_id'])
                        if session_id:
                            tenant_context["session_id"] = session_id
                            logger.info(f"ðŸ“ Found active session: {tenant_context['session_id']}")
                except Exception as e:
                    logger.warning(f"âš ï¸ Could not query Supabase: {e}")
        
        # Fallback to webhook handler if available
        if not tenant_context:
            webhook_context = get_webhook_room_context(job.room.name if job.room else "")
            if webhook_context:
                tenant_context = {
                    "room_id": webhook_context.get("room_id"),
                    "mosque_id": webhook_context.get("mosque_id"),
                    "session_id": webhook_context.get("session_id"),
                    "room_title": webhook_context.get("room_title"),
                    "transcription_language": webhook_context.get("transcription_language", "ar"),
                    "translation_language": webhook_context.get("translation_language", "nl"),
                    "created_at": webhook_context.get("created_at")
                }
                logger.info(f"ðŸ¢ Tenant context from webhook handler: mosque_id={tenant_context.get('mosque_id')}, room_id={tenant_context.get('room_id')}")
        
        # Fallback to room metadata if available
        if not tenant_context and job.room and job.room.metadata:
            try:
                metadata = json.loads(job.room.metadata)
                tenant_context = {
                    "room_id": metadata.get("room_id"),
                    "mosque_id": metadata.get("mosque_id"),
                    "session_id": metadata.get("session_id"),
                    "room_title": metadata.get("room_title"),
                    "transcription_language": metadata.get("transcription_language", "ar"),
                    "translation_language": metadata.get("translation_language", "nl"),
                    "created_at": metadata.get("created_at")
                }
                logger.info(f"ðŸ¢ Tenant context from room metadata: mosque_id={tenant_context.get('mosque_id')}, room_id={tenant_context.get('room_id')}")
            except:
                pass
        
        # Final fallback to default context with hardcoded values for testing
        if not tenant_context:
            logger.warning(f"âš ï¸ No tenant context available for room: {job.room.name if job.room else 'unknown'}")
            # TEMPORARY: Use hardcoded values for mosque_546012 rooms
            if job.room and f"mosque_{config.test_mosque_id}" in job.room.name:
                tenant_context = {
                    "room_id": config.test_room_id,
                    "mosque_id": config.test_mosque_id,
                    "session_id": None,
                    "transcription_language": "ar",
                    "translation_language": "nl"
                }
                logger.info(f"ðŸ”§ Using hardcoded tenant context for testing: mosque_id={tenant_context['mosque_id']}, room_id={tenant_context['room_id']}")
            else:
                tenant_context = {
                    "room_id": None,
                    "mosque_id": int(os.getenv('DEFAULT_MOSQUE_ID', str(config.default_mosque_id))),
                    "session_id": None,
                    "transcription_language": "ar",
                    "translation_language": "nl"
                }
    except Exception as e:
        logger.warning(f"âš ï¸ Could not extract tenant context: {e}")
    
    # Configure Speechmatics STT with room-specific settings
    # Use tenant_context which already has room configuration
    room_config = None
    if tenant_context and tenant_context.get('room_id'):
        # We already have the room data in tenant_context from earlier query
        room_config = tenant_context
        logger.info(f"ðŸ“‹ Using room-specific configuration from context: "
                  f"lang={room_config.get('transcription_language', 'ar')}, "
                  f"target={room_config.get('translation_language', 'nl')}, "
                  f"delay={room_config.get('max_delay', 2.0)}, "
                  f"punct={room_config.get('punctuation_sensitivity', 0.5)}, "
                  f"context_window={room_config.get('context_window_size', 6)}")
        
        # If we need full room data and it's not in context, query it
        if not room_config.get('max_delay'):
            try:
                full_room_data = await query_room_by_name(job.room.name if job.room else None)
                if full_room_data:
                    # Merge the full room data with tenant context
                    room_config.update({
                        'max_delay': full_room_data.get('max_delay'),
                        'punctuation_sensitivity': full_room_data.get('punctuation_sensitivity'),
                        'translation__language': full_room_data.get('translation__language'),
                        'context_window_size': full_room_data.get('context_window_size', 6)
                    })
                    logger.info(f"ðŸ“‹ Fetched additional room config: delay={room_config.get('max_delay')}, punct={room_config.get('punctuation_sensitivity')}, context_window={room_config.get('context_window_size')}")
            except Exception as e:
                logger.warning(f"Failed to fetch additional room config: {e}")
    
    # Create STT configuration with room-specific overrides
    stt_config = config.speechmatics.with_room_settings(room_config)
    
    # Initialize STT provider with configured settings
    stt_provider = speechmatics.STT(
        transcription_config=TranscriptionConfig(
            language=stt_config.language,
            operating_point=stt_config.operating_point,
            enable_partials=stt_config.enable_partials,
            max_delay=stt_config.max_delay,
            punctuation_overrides={"sensitivity": stt_config.punctuation_sensitivity},
            diarization=stt_config.diarization
        )
    )
    
    # Update source language based on room config
    source_language = config.translation.get_source_language(room_config)
    logger.info(f"ðŸ—£ï¸ STT configured for {languages[source_language].name} speech recognition")
    
    translators = {}
    
    # Get target language from room config or use default
    target_language = config.translation.get_target_language(room_config)
    logger.info(f"ðŸŽ¯ Target language resolved to: '{target_language}' (from room_config: {room_config.get('translation_language') if room_config else 'None'} or {room_config.get('translation__language') if room_config else 'None'})")
    
    # Create translator for the configured target language
    if target_language in languages:
        # Get language enum dynamically
        lang_info = languages[target_language]
        lang_enum = getattr(LanguageCode, lang_info.name)
        translators[target_language] = Translator(job.room, lang_enum, tenant_context, broadcast_to_displays)
        logger.info(f"ðŸ“ Initialized {lang_info.name} translator ({target_language})")
    else:
        logger.warning(f"âš ï¸ Target language '{target_language}' not supported, falling back to Dutch")
        dutch_enum = getattr(LanguageCode, 'Dutch')
        translators["nl"] = Translator(job.room, dutch_enum, tenant_context, broadcast_to_displays)
    
    # Sentence accumulation for proper sentence-by-sentence translation
    accumulated_text = ""  # Accumulates text until we get a complete sentence
    last_final_transcript = ""  # Keep track of the last final transcript to avoid duplicates
    current_sentence_id = None  # Track current sentence being built
    
    logger.info(f"ðŸš€ Starting entrypoint for room: {job.room.name if job.room else 'unknown'}")
    logger.info(f"ðŸ” Translators dict ID: {id(translators)}")
    logger.info(f"ðŸŽ¯ Configuration: {languages[source_language].name} â†’ {languages.get(target_language, languages['nl']).name}")
    logger.info(f"âš™ï¸ STT Settings: delay={stt_config.max_delay}s, punctuation={stt_config.punctuation_sensitivity}")

    async def _forward_transcription(
        stt_stream: stt.SpeechStream,
        track: rtc.Track,
    ):
        """Forward the transcription and log the transcript in the console"""
        nonlocal accumulated_text, last_final_transcript, current_sentence_id
        
        try:
            async for ev in stt_stream:
                # Log to console for interim (word-by-word)
                if ev.type == stt.SpeechEventType.INTERIM_TRANSCRIPT:
                    print(ev.alternatives[0].text, end="", flush=True)
                    
                    # Publish interim transcription for real-time word-by-word display
                    interim_text = ev.alternatives[0].text.strip()
                    if interim_text:
                        try:
                            interim_segment = rtc.TranscriptionSegment(
                                id=utils.misc.shortuuid("SG_"),
                                text=interim_text,
                                start_time=0,
                                end_time=0,
                                language=source_language,  # Arabic
                                final=False,  # This is interim, not final
                            )
                            interim_transcription = rtc.Transcription(
                                job.room.local_participant.identity, "", [interim_segment]
                            )
                            await job.room.local_participant.publish_transcription(interim_transcription)
                        except Exception as e:
                            logger.debug(f"Failed to publish interim transcription: {str(e)}")
                    
                elif ev.type == stt.SpeechEventType.FINAL_TRANSCRIPT:
                    print("\n")
                    final_text = ev.alternatives[0].text.strip()
                    print(" -> ", final_text)
                    logger.info(f"Final Arabic transcript: {final_text}")

                    if final_text and final_text != last_final_transcript:
                        last_final_transcript = final_text
                        
                        # Publish final transcription for the original language (Arabic)
                        try:
                            final_segment = rtc.TranscriptionSegment(
                                id=utils.misc.shortuuid("SG_"),
                                text=final_text,
                                start_time=0,
                                end_time=0,
                                language=source_language,  # Arabic
                                final=True,
                            )
                            final_transcription = rtc.Transcription(
                                job.room.local_participant.identity, "", [final_segment]
                            )
                            await job.room.local_participant.publish_transcription(final_transcription)
                            
                            logger.info(f"âœ… Published final {languages[source_language].name} transcription: '{final_text}'")
                        except Exception as e:
                            logger.error(f"âŒ Failed to publish final transcription: {str(e)}")
                        
                        # Generate sentence ID if we don't have one for this sentence
                        if not current_sentence_id:
                            current_sentence_id = str(uuid.uuid4())
                        
                        # Broadcast final transcription text for real-time display
                        print(f"ðŸ“¡ Broadcasting Arabic text to frontend: '{final_text}'")
                        # Broadcast directly without task wrapper
                        await broadcast_to_displays(
                            "transcription", 
                            source_language, 
                            final_text, 
                            tenant_context,
                            sentence_context={
                                "sentence_id": current_sentence_id,
                                "is_complete": False,  # Will be marked complete when sentence ends
                                "is_fragment": True
                            }
                        )
                        
                        # Handle translation logic
                        if translators:
                            # SIMPLE ACCUMULATION LOGIC - ONLY APPEND, NEVER REPLACE
                            if accumulated_text:
                                # ALWAYS append new final transcript to existing accumulated text
                                accumulated_text = accumulated_text.strip() + " " + final_text
                            else:
                                # First transcript - start accumulation
                                accumulated_text = final_text
                            
                            logger.info(f"ðŸ“ Updated accumulated Arabic text: '{accumulated_text}'")
                            
                            # Extract complete sentences from accumulated text
                            complete_sentences, remaining_text = extract_complete_sentences(accumulated_text)
                            
                            # Handle special punctuation completion signal
                            if complete_sentences and complete_sentences[0] == "PUNCTUATION_COMPLETE":
                                if accumulated_text.strip():
                                    # Complete the accumulated sentence with this punctuation
                                    print(f"ðŸ“ PUNCTUATION SIGNAL: Completing accumulated text: '{accumulated_text}'")
                                    
                                    # Broadcast sentence completion
                                    if current_sentence_id:
                                        await broadcast_to_displays(
                                            "transcription", 
                                            source_language, 
                                            accumulated_text, 
                                            tenant_context,
                                            sentence_context={
                                                "sentence_id": current_sentence_id,
                                                "is_complete": True,
                                                "is_fragment": False
                                            }
                                        )
                                    
                                    # Translate the completed sentence (don't include the punctuation marker)
                                    await translate_sentences([accumulated_text], translators, source_language, current_sentence_id)
                                    
                                    # Clear accumulated text as sentence is now complete
                                    accumulated_text = ""
                                    current_sentence_id = None
                                    print(f"ðŸ“ Cleared accumulated text after punctuation completion")
                                else:
                                    print(f"âš ï¸ Received punctuation completion signal but no accumulated text")
                            elif complete_sentences:
                                # We have complete sentences - translate them immediately
                                print(f"ðŸŽ¯ Found {len(complete_sentences)} complete Arabic sentences: {complete_sentences}")
                                
                                # Broadcast each complete sentence
                                for sentence in complete_sentences:
                                    sentence_id = current_sentence_id if len(complete_sentences) == 1 else str(uuid.uuid4())
                                    await broadcast_to_displays(
                                        "transcription", 
                                        source_language, 
                                        sentence, 
                                        tenant_context,
                                        sentence_context={
                                            "sentence_id": sentence_id,
                                            "is_complete": True,
                                            "is_fragment": False
                                        }
                                    )
                                    # Translate complete sentences with sentence ID
                                    await translate_sentences([sentence], translators, source_language, sentence_id)
                                
                                # Update accumulated text to only remaining incomplete text
                                accumulated_text = remaining_text
                                # Generate new sentence ID for the remaining text
                                current_sentence_id = str(uuid.uuid4()) if remaining_text else None
                                print(f"ðŸ“ Updated accumulated Arabic text after sentence extraction: '{accumulated_text}'")
                            
                            # Log remaining incomplete text (no delayed translation)
                            if accumulated_text.strip():
                                logger.info(f"ðŸ“ Incomplete Arabic text remaining: '{accumulated_text}'")
                                # Note: Incomplete text will be translated when the next sentence completes
                        else:
                            logger.warning(f"âš ï¸ No translators available in room {job.room.name}, only {languages[source_language].name} transcription published")
                    else:
                        logger.debug("Empty or duplicate transcription, skipping")
        except Exception as e:
            logger.error(f"STT transcription error: {str(e)}")
            raise

    async def transcribe_track(participant: rtc.RemoteParticipant, track: rtc.Track):
        logger.info(f"ðŸŽ¤ Starting Arabic transcription for participant {participant.identity}, track {track.sid}")
        
        try:
            audio_stream = rtc.AudioStream(track)
            
            # Use context manager for STT stream
            async with resource_manager.stt_manager.create_stream(stt_provider, participant.identity) as stt_stream:
                # Create transcription task with tracking
                stt_task = resource_manager.task_manager.create_task(
                    _forward_transcription(stt_stream, track),
                    name=f"transcribe-{participant.identity}",
                    metadata={"participant": participant.identity, "track": track.sid}
                )
                
                frame_count = 0
                async for ev in audio_stream:
                    frame_count += 1
                    if frame_count % 100 == 0:  # Log every 100 frames to avoid spam
                        logger.debug(f"ðŸ”Š Received audio frame #{frame_count} from {participant.identity}")
                        # Update heartbeat every 100 frames
                        await resource_manager.heartbeat_monitor.update_heartbeat(
                            participant.identity, 
                            tenant_context.get('session_id')
                        )
                    stt_stream.push_frame(ev.frame)
                    
                logger.warning(f"ðŸ”‡ Audio stream ended for {participant.identity}")
                
                # Cancel the transcription task if still running
                if not stt_task.done():
                    stt_task.cancel()
                    try:
                        await stt_task
                    except asyncio.CancelledError:
                        logger.debug(f"STT task cancelled for {participant.identity}")
                        
        except Exception as e:
            logger.error(f"âŒ Transcription track error for {participant.identity}: {str(e)}")
        
        logger.info(f"ðŸ§¹ Transcription cleanup completed for {participant.identity}")

    @job.room.on("track_subscribed")
    def on_track_subscribed(
        track: rtc.Track,
        publication: rtc.TrackPublication,
        participant: rtc.RemoteParticipant,
    ):
        logger.info(f"ðŸŽµ Track subscribed: {track.kind} from {participant.identity} (track: {track.sid})")
        logger.info(f"Track details - muted: {publication.muted}")
        if track.kind == rtc.TrackKind.KIND_AUDIO:
            logger.info(f"âœ… Adding Arabic transcriber for participant: {participant.identity}")
            resource_manager.task_manager.create_task(
                transcribe_track(participant, track),
                name=f"track-handler-{participant.identity}",
                metadata={"participant": participant.identity, "track": track.sid}
            )
        else:
            logger.info(f"âŒ Ignoring non-audio track: {track.kind}")

    @job.room.on("track_published")
    def on_track_published(publication: rtc.TrackPublication, participant: rtc.RemoteParticipant):
        logger.info(f"ðŸ“¡ Track published: {publication.kind} from {participant.identity} (track: {publication.sid})")
        logger.info(f"Publication details - muted: {publication.muted}")

    @job.room.on("track_unpublished") 
    def on_track_unpublished(publication: rtc.TrackPublication, participant: rtc.RemoteParticipant):
        logger.info(f"ðŸ“¡ Track unpublished: {publication.kind} from {participant.identity}")

    @job.room.on("participant_connected")
    def on_participant_connected(participant: rtc.RemoteParticipant):
        logger.info(f"ðŸ‘¥ Participant connected: {participant.identity}")
        
        # Try to extract metadata from participant if available
        if hasattr(participant, 'metadata') and participant.metadata:
            try:
                participant_metadata = json.loads(participant.metadata)
                if participant_metadata:
                    # Update tenant context with participant metadata
                    tenant_context.update({
                        "room_id": participant_metadata.get("room_id", tenant_context.get("room_id")),
                        "mosque_id": participant_metadata.get("mosque_id", tenant_context.get("mosque_id")),
                        "session_id": participant_metadata.get("session_id", tenant_context.get("session_id")),
                        "room_title": participant_metadata.get("room_title", tenant_context.get("room_title"))
                    })
                    logger.info(f"ðŸ“‹ Updated tenant context from participant metadata: {tenant_context}")
                    
                    # Update all translators with new context
                    for translator in translators.values():
                        translator.tenant_context = tenant_context
            except Exception as e:
                logger.debug(f"Could not parse participant metadata: {e}")

    @job.room.on("participant_disconnected")
    def on_participant_disconnected(participant: rtc.RemoteParticipant):
        logger.info(f"ðŸ‘¥ Participant disconnected: {participant.identity}")
        
        # Remove from heartbeat monitoring
        resource_manager.heartbeat_monitor.remove_participant(participant.identity)
        
        # Immediately close any STT streams for this participant
        async def cleanup_participant_streams():
            try:
                await resource_manager.stt_manager.close_participant_stream(participant.identity)
                logger.info(f"âœ… STT stream closed for disconnected participant {participant.identity}")
            except Exception as e:
                logger.error(f"Error closing STT stream for {participant.identity}: {e}")
        
        # Schedule immediate cleanup
        resource_manager.task_manager.create_task(
            cleanup_participant_streams(),
            name=f"cleanup-stt-{participant.identity}"
        )
        
        # Log current resource statistics
        resource_manager.log_stats()
        logger.info(f"ðŸ§¹ Participant cleanup initiated for {participant.identity}")

    @job.room.on("participant_attributes_changed")
    def on_attributes_changed(
        changed_attributes: dict[str, str], participant: rtc.Participant
    ):
        """
        When participant attributes change, handle new translation requests.
        """
        logger.info(f"ðŸŒ Participant {participant.identity} attributes changed: {changed_attributes}")
        lang = changed_attributes.get("captions_language", None)
        if lang:
            if lang == source_language:
                logger.info(f"âœ… Participant {participant.identity} requested {languages[source_language].name} (source language - Arabic)")
            elif lang in translators:
                logger.info(f"âœ… Participant {participant.identity} requested existing language: {lang}")
                logger.info(f"ðŸ“Š Current translators for this room: {list(translators.keys())}")
            else:
                # Check if the language is supported and different from source language
                if lang in languages:
                    try:
                        # Create a translator for the requested language using the language enum
                        language_obj = languages[lang]
                        language_enum = getattr(LanguageCode, language_obj.name)
                        translators[lang] = Translator(job.room, language_enum, tenant_context, broadcast_to_displays)
                        logger.info(f"ðŸ†• Added translator for ROOM {job.room.name} (requested by {participant.identity}), language: {language_obj.name}")
                        logger.info(f"ðŸ¢ Translator created with tenant context: mosque_id={tenant_context.get('mosque_id')}")
                        logger.info(f"ðŸ“Š Total translators for room {job.room.name}: {len(translators)} -> {list(translators.keys())}")
                        logger.info(f"ðŸ” Translators dict ID: {id(translators)}")
                        
                        # Debug: Verify the translator was actually added
                        if lang in translators:
                            logger.info(f"âœ… Translator verification: {lang} successfully added to room translators")
                        else:
                            logger.error(f"âŒ Translator verification FAILED: {lang} not found in translators dict")
                            
                    except Exception as e:
                        logger.error(f"âŒ Error creating translator for {lang}: {str(e)}")
                else:
                    logger.warning(f"âŒ Unsupported language requested by {participant.identity}: {lang}")
                    logger.info(f"ðŸ’¡ Supported languages: {list(languages.keys())}")
        else:
            logger.debug(f"No caption language change for participant {participant.identity}")

    logger.info("Connecting to room...")
    await job.connect(auto_subscribe=AutoSubscribe.AUDIO_ONLY)
    logger.info(f"Successfully connected to room: {job.room.name}")
    logger.info(f"ðŸ“¡ Real-time transcription data will be sent via Supabase Broadcast")
    
    # Debug room state after connection
    logger.info(f"Room participants: {len(job.room.remote_participants)}")
    for participant in job.room.remote_participants.values():
        logger.info(f"Participant: {participant.identity}")
        logger.info(f"  Audio tracks: {len(participant.track_publications)}")
        for sid, pub in participant.track_publications.items():
            logger.info(f"    Track {sid}: {pub.kind}, muted: {pub.muted}")

    # Also check local participant
    logger.info(f"Local participant: {job.room.local_participant.identity}")
    logger.info(f"Local participant tracks: {len(job.room.local_participant.track_publications)}")

    @job.room.local_participant.register_rpc_method("get/languages")
    async def get_languages(data: rtc.RpcInvocationData):
        languages_list = [asdict(lang) for lang in languages.values()]
        return json.dumps(languages_list)
    
    @job.room.local_participant.register_rpc_method("request/cleanup")
    async def request_cleanup(data: rtc.RpcInvocationData):
        """Handle cleanup request from frontend"""
        try:
            payload = json.loads(data.payload)
            reason = payload.get('reason', 'unknown')
            session_id = payload.get('session_id')
            
            logger.info(f"ðŸ§¹ Cleanup requested by frontend: reason={reason}, session_id={session_id}")
            
            # Initiate graceful shutdown
            asyncio.create_task(perform_graceful_cleanup(reason, session_id))
            
            return json.dumps({
                "success": True,
                "message": "Cleanup initiated"
            })
        except Exception as e:
            logger.error(f"Error handling cleanup request: {e}")
            return json.dumps({
                "success": False,
                "error": str(e)
            })
    
    async def perform_graceful_cleanup(reason: str, session_id: Optional[str]):
        """Perform graceful cleanup when requested by frontend"""
        logger.info(f"ðŸ›‘ Starting graceful cleanup: {reason}")
        
        # Log current resource state
        resource_manager.log_stats()
        
        # If session_id provided, update it in database
        if session_id and tenant_context.get('room_id'):
            try:
                from database import query_database
                result = await query_database(
                    "SELECT cleanup_session_idempotent(%s, %s, %s)",
                    [session_id, f"frontend_{reason}", datetime.utcnow()]
                )
                logger.info(f"Session cleanup result: {result}")
            except Exception as e:
                logger.error(f"Error updating session: {e}")
        
        # Shutdown all resources
        await resource_manager.shutdown()
        
        # Verify cleanup is complete
        verification = await resource_manager.verify_cleanup_complete()
        logger.info(f"ðŸ” Cleanup verification: {verification}")
        
        # Disconnect from room (this will trigger on_room_disconnected)
        await job.room.disconnect()
        
        logger.info("âœ… Graceful cleanup completed")

    @job.room.on("disconnected")
    def on_room_disconnected():
        """Handle room disconnection - cleanup all resources"""
        logger.info("ðŸšª Room disconnected, starting cleanup...")
        
        # Create async task for cleanup
        async def cleanup():
            # Log final resource statistics
            resource_manager.log_stats()
            
            # Shutdown resource manager (cancels all tasks, closes all streams)
            await resource_manager.shutdown()
            
            # Close database connections
            try:
                await close_database_connections()
                logger.info("âœ… Database connections closed")
                
                # Force cleanup of any remaining sessions
                import gc
                gc.collect()  # Force garbage collection
                await asyncio.sleep(0.1)  # Give time for cleanup
            except Exception as e:
                logger.debug(f"Database cleanup error: {e}")
            
            # Final verification
            verification = await resource_manager.verify_cleanup_complete()
            logger.info(f"ðŸ” Final cleanup verification: {verification}")
            
            logger.info("âœ… Room cleanup completed")
        
        # Run cleanup in the event loop
        asyncio.create_task(cleanup())


async def request_fnc(req: JobRequest):
    logger.info(f"ðŸŽ¯ Received job request for room: {req.room.name if req.room else 'unknown'}")
    logger.info(f"ðŸ“‹ Request details: job_id={req.id}, room_name={req.room.name if req.room else 'unknown'}")
    await req.accept(
        name="agent",
        identity="agent",
    )
    logger.info(f"âœ… Accepted job request for room: {req.room.name if req.room else 'unknown'}")


if __name__ == "__main__":
    cli.run_app(
        WorkerOptions(
            entrypoint_fnc=entrypoint, prewarm_fnc=prewarm, request_fnc=request_fnc
        )
    )


================================================
FILE: backup_20250728_212515/main_production.py
================================================
#!/usr/bin/env python3
"""
Production-ready LiveKit Agent for Bayaan Translation Service
Optimized for Render deployment as a background worker
"""

import asyncio
import logging
import os
import sys
import signal
from datetime import datetime

# Setup production logging
def setup_production_logging():
    """Configure logging for production."""
    log_level = os.getenv('LOG_LEVEL', 'INFO').upper()
    
    logging.basicConfig(
        level=getattr(logging, log_level),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(sys.stdout)
        ]
    )

def health_check() -> bool:
    """Health check endpoint for Render."""
    try:
        # Basic health check - verify environment variables are set
        required_vars = [
            'LIVEKIT_URL', 'LIVEKIT_API_KEY', 'LIVEKIT_API_SECRET',
            'OPENAI_API_KEY', 'SUPABASE_URL', 'SUPABASE_SERVICE_ROLE_KEY'
        ]
        
        for var in required_vars:
            if not os.getenv(var):
                print(f"Missing required environment variable: {var}")
                return False
        
        print("Health check passed - all required environment variables are set")
        return True
        
    except Exception as e:
        print(f"Health check failed: {e}")
        return False

def main():
    """Main production entry point."""
    # Setup production logging
    setup_production_logging()
    
    logger = logging.getLogger(__name__)
    logger.info("Starting Bayaan LiveKit Agent for production deployment")
    
    # Set production environment
    os.environ['ENVIRONMENT'] = 'production'
    
    try:
        # Import and run the agent directly
        from livekit.agents import WorkerOptions, cli
        from main import entrypoint, prewarm, request_fnc
        
        # Production worker configuration
        worker_opts = WorkerOptions(
            entrypoint_fnc=entrypoint,
            prewarm_fnc=prewarm,
            request_fnc=request_fnc
        )
        
        logger.info("Starting LiveKit CLI with production configuration")
        
        # Run the agent - this handles its own event loop
        cli.run_app(worker_opts)
        
    except Exception as e:
        logger.error(f"Fatal error in production agent: {e}")
        sys.exit(1)

if __name__ == "__main__":
    # Handle different command line arguments
    if len(sys.argv) > 1:
        if sys.argv[1] == "health":
            # Health check endpoint
            is_healthy = health_check()
            sys.exit(0 if is_healthy else 1)
        elif sys.argv[1] == "start":
            # Start the agent
            main()
        else:
            # Default to original main.py behavior
            from main import *
            # Run with original CLI
            from livekit.agents import cli, WorkerOptions
            cli.run_app(WorkerOptions(
                entrypoint_fnc=entrypoint,
                prewarm_fnc=prewarm,
                request_fnc=request_fnc
            ))
    else:
        # Run the main agent
        main() 


================================================
FILE: backup_20250728_212515/prompt_builder.py
================================================
"""
Prompt Builder for customizable translation prompts.
Handles loading and formatting of prompt templates with variable substitution.
"""
import logging
from typing import Dict, Optional, Any
import json

from config import get_config
from database import query_prompt_template_for_room

logger = logging.getLogger("transcriber.prompt_builder")
config = get_config()


class PromptBuilder:
    """
    Builds customized translation prompts based on templates and room configuration.
    """
    
    # Default fallback prompt if no template is found
    DEFAULT_PROMPT = (
        "You are an expert simultaneous interpreter. Your task is to translate from {source_lang} to {target_lang}. "
        "Provide a direct and accurate translation of the user's input. "
        "Do not add any additional commentary, explanations, or introductory phrases. "
        "Be concise for real-time delivery."
    )
    
    def __init__(self):
        """Initialize the prompt builder."""
        logger.info("ðŸŽ¨ PromptBuilder initialized")
    
    async def get_prompt_for_room(
        self, 
        room_id: Optional[int],
        source_lang: str,
        target_lang: str,
        room_config: Optional[Dict[str, Any]] = None
    ) -> str:
        """
        Get the appropriate prompt for a room with variable substitution.
        
        Args:
            room_id: The room ID to get prompt for
            source_lang: Source language name (e.g., "Arabic")
            target_lang: Target language name (e.g., "Dutch")
            room_config: Optional room configuration with additional context
            
        Returns:
            Formatted prompt string ready for use
        """
        try:
            # Try to get template from database if room_id provided
            template = None
            if room_id:
                template = await self._fetch_template_for_room(room_id)
            
            if template:
                prompt = template.get('prompt_template', self.DEFAULT_PROMPT)
                # Ensure template_variables is always a dict, never None
                variables = template.get('template_variables') or {}
                if not isinstance(variables, dict):
                    logger.warning(f"template_variables is not a dict: {type(variables)}, using empty dict")
                    variables = {}
                logger.info(f"ðŸ“‹ Using prompt template: {template.get('name', 'Unknown')}")
                logger.info(f"ðŸ“‹ Template text: {prompt[:100]}...")
                logger.info(f"ðŸ“‹ Template variables from DB: {variables}")
            else:
                # Use default prompt
                prompt = self.DEFAULT_PROMPT
                variables = {}
                logger.info("ðŸ“‹ Using default prompt template")
            
            # Prepare substitution variables
            # Support both naming conventions for backwards compatibility
            substitutions = {
                'source_lang': source_lang,
                'source_language': source_lang,  # Also provide source_language for templates
                'target_lang': target_lang,
                'target_language': target_lang,  # Also provide target_language for templates
            }
            
            # Merge template variables, ensuring we handle nested values
            if variables:
                substitutions.update(variables)
                # Log what variables we're using
                logger.debug(f"Template variables: {variables}")
            
            # Add room-specific context if available
            if room_config:
                if room_config.get('mosque_name'):
                    substitutions['mosque_name'] = room_config['mosque_name']
                if room_config.get('speaker_role'):
                    substitutions['speaker_role'] = room_config['speaker_role']
            
            # Format the prompt with variables
            try:
                formatted_prompt = prompt.format(**substitutions)
            except KeyError as e:
                logger.warning(f"Missing variable in template: {e}. Using partial formatting.")
                # Use safe substitution that ignores missing keys
                from string import Template
                safe_template = Template(prompt.replace('{', '${'))
                formatted_prompt = safe_template.safe_substitute(**substitutions)
                # Replace any remaining ${var} with empty string
                import re
                formatted_prompt = re.sub(r'\$\{[^}]+\}', '', formatted_prompt)
            
            # Add critical translation instruction if missing
            if template and not any(word in formatted_prompt.lower() for word in ["translate", "translating", "translation", "translator"]):
                logger.warning("Template missing explicit translation instruction, adding prefix")
                formatted_prompt = f"IMPORTANT: You must ONLY translate the text, do not respond or react to it. {formatted_prompt}"
            
            # Log successful template usage
            if template:
                logger.info(f"âœ… Successfully formatted custom prompt template: {template.get('name', 'Unknown')}")
            
            # Log the generated prompt for debugging
            logger.info(f"Generated prompt: {formatted_prompt}")
            
            return formatted_prompt
            
        except Exception as e:
            logger.error(f"âŒ Error building prompt: {e}")
            # Fallback to basic default
            return self.DEFAULT_PROMPT.format(
                source_lang=source_lang,
                target_lang=target_lang
            )
    
    async def _fetch_template_for_room(self, room_id: int) -> Optional[Dict[str, Any]]:
        """
        Fetch prompt template for a specific room.
        
        Args:
            room_id: The room ID
            
        Returns:
            Template dictionary or None
        """
        try:
            # Always fetch fresh from database - no caching for production
            template = await query_prompt_template_for_room(room_id)
            
            if template:
                # Log what we received for debugging
                logger.info(f"ðŸ“‹ Fetched template for room {room_id}: {template.get('name', 'Unknown')}")
                logger.debug(f"Template data: {template}")
            else:
                logger.info(f"ðŸ“‹ No template found for room {room_id}")
                
            return template
            
        except Exception as e:
            logger.warning(f"Failed to fetch template for room {room_id}: {e}")
            return None
    
    def build_prompt_with_context(
        self,
        base_prompt: str,
        context_type: str,
        additional_context: Optional[Dict[str, str]] = None
    ) -> str:
        """
        Enhance a prompt with additional context based on content type.
        
        Args:
            base_prompt: The base prompt template
            context_type: Type of content (sermon, announcement, etc.)
            additional_context: Additional context variables
            
        Returns:
            Enhanced prompt with context
        """
        context_additions = {
            'sermon': (
                " Remember this is a religious sermon, so maintain appropriate "
                "reverence and formality. Preserve the spiritual tone."
            ),
            'announcement': (
                " This is a community announcement, so prioritize clarity and "
                "practical information over stylistic concerns."
            ),
            'dua': (
                " This is a prayer or supplication. Maintain the devotional "
                "atmosphere and emotional depth of the original."
            ),
            'lecture': (
                " This is an educational lecture. You may add brief clarifications "
                "in parentheses for complex religious terms if needed."
            )
        }
        
        # Add context-specific guidance
        addition = context_additions.get(context_type, "")
        
        # Add any additional context
        if additional_context:
            for key, value in additional_context.items():
                if value:
                    addition += f" {key}: {value}."
        
        return base_prompt + addition
    
    def get_preserved_terms_for_template(self, template_variables: Dict) -> list:
        """
        Extract list of terms to preserve from template variables.
        
        Args:
            template_variables: Template variables dictionary
            
        Returns:
            List of terms to preserve in original language
        """
        return template_variables.get('preserve_terms', [])


# Global instance
_prompt_builder: Optional[PromptBuilder] = None


def get_prompt_builder() -> PromptBuilder:
    """Get or create the global prompt builder instance."""
    global _prompt_builder
    if _prompt_builder is None:
        _prompt_builder = PromptBuilder()
    return _prompt_builder


================================================
FILE: backup_20250728_212515/render.yaml
================================================
services:
  - type: worker
    name: bayaan-livekit-agent
    env: docker
    dockerfilePath: ./Dockerfile
    plan: starter
    region: oregon
    scaling:
      minInstances: 1
      maxInstances: 3
      targetMemoryPercent: 80
      targetCPUPercent: 80
    envVars:
      - key: ENVIRONMENT
        value: production
      - key: LIVEKIT_URL
        fromSecret: livekit_url
      - key: LIVEKIT_API_KEY
        fromSecret: livekit_api_key
      - key: LIVEKIT_API_SECRET
        fromSecret: livekit_api_secret
      - key: OPENAI_API_KEY
        fromSecret: openai_api_key
      - key: SPEECHMATICS_API_KEY
        fromSecret: speechmatics_api_key
      - key: SUPABASE_URL
        fromSecret: supabase_url
      - key: SUPABASE_SERVICE_ROLE_KEY
        fromSecret: supabase_service_role_key
      - key: SUPABASE_ANON_KEY
        fromSecret: supabase_anon_key
      - key: PYTHONPATH
        value: /app
      - key: AGENT_NAME
        value: bayaan-transcriber
      - key: WORKER_TYPE
        value: background
      - key: LOG_LEVEL
        value: INFO
      - key: PERSISTENT_MODE
        value: "true"
      - key: MAX_WORKERS
        value: "3"
      - key: IDLE_TIMEOUT
        value: "300" 


================================================
FILE: backup_20250728_212515/requirements.txt
================================================
# LiveKit Core Dependencies
livekit-agents>=1.0.0
livekit-plugins-openai>=0.8.0
livekit-plugins-speechmatics>=0.6.0
livekit-plugins-silero>=0.6.0

# AI/ML Dependencies
openai>=1.0.0

# Web Framework (for health checks)
fastapi>=0.104.0
uvicorn[standard]>=0.24.0

# HTTP Client
aiohttp>=3.8.0

# Database
asyncpg>=0.29.0

# Environment & Configuration
python-dotenv>=1.0.0

# Logging & Monitoring
# structlog>=23.0.0  # Removed - using standard logging

# Production Dependencies
gunicorn>=21.0.0
psutil>=5.9.0

# Audio Processing
pyaudio>=0.2.11

# Async utilities
asyncio-throttle>=1.0.0


================================================
FILE: backup_20250728_212515/resource_management.py
================================================
"""
Resource management module for LiveKit AI Translation Server.
Handles tracking and cleanup of async tasks, STT streams, and other resources.
"""
import asyncio
import logging
from typing import Set, List, Dict, Any, Optional, Callable
from contextlib import asynccontextmanager
from dataclasses import dataclass, field
from datetime import datetime, timedelta
import time
import weakref

logger = logging.getLogger("transcriber.resources")


@dataclass
class ResourceStats:
    """Statistics about managed resources."""
    tasks_created: int = 0
    tasks_completed: int = 0
    tasks_failed: int = 0
    tasks_cancelled: int = 0
    streams_opened: int = 0
    streams_closed: int = 0
    active_tasks: int = 0
    active_streams: int = 0


class TaskManager:
    """
    Manages async tasks with proper tracking and cleanup.
    
    Features:
    - Automatic task tracking
    - Graceful cancellation
    - Resource leak prevention
    - Statistics tracking
    """
    
    def __init__(self, name: str = "default"):
        self.name = name
        self._tasks: Set[asyncio.Task] = set()
        self._task_metadata: Dict[asyncio.Task, Dict[str, Any]] = {}
        self._stats = ResourceStats()
        self._cleanup_interval = 30.0  # seconds
        self._cleanup_task: Optional[asyncio.Task] = None
        self._shutdown = False
        logger.info(f"ðŸ“‹ TaskManager '{self.name}' initialized")
    
    async def __aenter__(self):
        """Context manager entry."""
        self._cleanup_task = asyncio.create_task(self._periodic_cleanup())
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit with cleanup."""
        await self.shutdown()
    
    def create_task(
        self, 
        coro, 
        name: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None
    ) -> asyncio.Task:
        """
        Create and track an async task.
        
        Args:
            coro: Coroutine to run
            name: Optional task name
            metadata: Optional metadata for the task
            
        Returns:
            Created task
        """
        if self._shutdown:
            raise RuntimeError("TaskManager is shutting down")
        
        task = asyncio.create_task(coro, name=name)
        self._tasks.add(task)
        
        if metadata:
            self._task_metadata[task] = metadata
        
        # Add callback to clean up when done
        task.add_done_callback(self._task_done_callback)
        
        self._stats.tasks_created += 1
        self._stats.active_tasks = len(self._tasks)
        
        logger.debug(f"ðŸ“Œ Created task: {name or task.get_name()} (total: {len(self._tasks)})")
        return task
    
    def _task_done_callback(self, task: asyncio.Task):
        """Callback when a task completes."""
        self._tasks.discard(task)
        self._task_metadata.pop(task, None)
        
        try:
            if task.cancelled():
                self._stats.tasks_cancelled += 1
                logger.debug(f"ðŸš« Task cancelled: {task.get_name()}")
            elif task.exception():
                self._stats.tasks_failed += 1
                logger.error(f"âŒ Task failed: {task.get_name()}", exc_info=task.exception())
            else:
                self._stats.tasks_completed += 1
                logger.debug(f"âœ… Task completed: {task.get_name()}")
        except Exception as e:
            logger.debug(f"Error in task callback: {e}")
        
        self._stats.active_tasks = len(self._tasks)
    
    async def _periodic_cleanup(self):
        """Periodically clean up completed tasks."""
        while not self._shutdown:
            try:
                await asyncio.sleep(self._cleanup_interval)
                
                # Clean up any references to completed tasks
                completed = [t for t in self._tasks if t.done()]
                for task in completed:
                    self._tasks.discard(task)
                    self._task_metadata.pop(task, None)
                
                if completed:
                    logger.debug(f"ðŸ§¹ Cleaned up {len(completed)} completed tasks")
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"Error in periodic cleanup: {e}")
    
    async def cancel_all(self, timeout: float = 5.0) -> int:
        """
        Cancel all active tasks.
        
        Args:
            timeout: Maximum time to wait for cancellation
            
        Returns:
            Number of tasks cancelled
        """
        if not self._tasks:
            return 0
        
        tasks_to_cancel = list(self._tasks)
        cancelled_count = 0
        
        logger.info(f"ðŸš« Cancelling {len(tasks_to_cancel)} tasks...")
        
        # Cancel all tasks
        for task in tasks_to_cancel:
            if not task.done():
                task.cancel()
                cancelled_count += 1
        
        # Wait for cancellation with timeout
        if tasks_to_cancel:
            try:
                await asyncio.wait_for(
                    asyncio.gather(*tasks_to_cancel, return_exceptions=True),
                    timeout=timeout
                )
            except asyncio.TimeoutError:
                logger.warning(f"â° Timeout waiting for {len(tasks_to_cancel)} tasks to cancel")
        
        logger.info(f"âœ… Cancelled {cancelled_count} tasks")
        return cancelled_count
    
    async def shutdown(self):
        """Shutdown the task manager and cleanup all resources."""
        if self._shutdown:
            return
        
        self._shutdown = True
        logger.info(f"ðŸ›‘ Shutting down TaskManager '{self.name}'...")
        
        # Cancel cleanup task
        if self._cleanup_task and not self._cleanup_task.done():
            self._cleanup_task.cancel()
            try:
                await self._cleanup_task
            except asyncio.CancelledError:
                pass
        
        # Cancel all managed tasks
        await self.cancel_all()
        
        logger.info(f"âœ… TaskManager '{self.name}' shutdown complete")
    
    def get_stats(self) -> ResourceStats:
        """Get current statistics."""
        return self._stats
    
    def get_active_tasks(self) -> List[asyncio.Task]:
        """Get list of active tasks."""
        return list(self._tasks)


class STTStreamManager:
    """
    Manages STT (Speech-to-Text) streams with proper cleanup.
    """
    
    def __init__(self):
        self._streams: Set[Any] = set()
        self._stream_metadata: Dict[Any, Dict[str, Any]] = {}
        self._participant_streams: Dict[str, Any] = {}  # Track active streams per participant
        self._cleanup_locks: Dict[str, asyncio.Lock] = {}  # Prevent race conditions
        self._participant_disconnect_times: Dict[str, float] = {}  # Track disconnect times
        self._reconnect_grace_period = 3.0  # seconds to wait before allowing reconnect
        self._stats = ResourceStats()
        logger.info("ðŸŽ¤ STTStreamManager initialized")
    
    @asynccontextmanager
    async def create_stream(self, stt_provider, participant_id: str):
        """
        Create and manage an STT stream.
        
        Args:
            stt_provider: STT provider instance
            participant_id: ID of the participant
            
        Yields:
            STT stream instance
        """
        # Get or create lock for this participant
        if participant_id not in self._cleanup_locks:
            self._cleanup_locks[participant_id] = asyncio.Lock()
        
        async with self._cleanup_locks[participant_id]:
            # Check for recent disconnect - implement debouncing
            last_disconnect = self._participant_disconnect_times.get(participant_id, 0)
            time_since_disconnect = time.time() - last_disconnect
            
            if time_since_disconnect < self._reconnect_grace_period:
                wait_time = self._reconnect_grace_period - time_since_disconnect
                logger.warning(f"â³ Participant {participant_id} reconnecting too quickly. Waiting {wait_time:.1f}s")
                await asyncio.sleep(wait_time)
            
            # Check if there's already an active stream for this participant
            existing_stream = self._participant_streams.get(participant_id)
            if existing_stream:
                logger.warning(f"âš ï¸ Active STT stream already exists for {participant_id}, closing it first")
                try:
                    await existing_stream.aclose()
                except Exception as e:
                    logger.error(f"Error closing existing stream: {e}")
                finally:
                    self._streams.discard(existing_stream)
                    self._stream_metadata.pop(existing_stream, None)
                    self._participant_streams.pop(participant_id, None)
            
            stream = None
            try:
                # Create stream
                stream = stt_provider.stream()
                self._streams.add(stream)
                self._stream_metadata[stream] = {
                    "participant_id": participant_id,
                    "created_at": datetime.utcnow()
                }
                self._participant_streams[participant_id] = stream
                self._stats.streams_opened += 1
                self._stats.active_streams = len(self._streams)
                
                logger.info(f"ðŸŽ¤ Created STT stream for {participant_id}")
                yield stream
            
            finally:
                # Cleanup stream
                if stream:
                    try:
                        # Force close the stream
                        await stream.aclose()
                        logger.info(f"âœ… STT stream closed for {participant_id}")
                    except Exception as e:
                        logger.error(f"Error closing STT stream: {e}")
                    finally:
                        self._streams.discard(stream)
                        self._stream_metadata.pop(stream, None)
                        self._participant_streams.pop(participant_id, None)
                        # Record disconnect time for debouncing
                        self._participant_disconnect_times[participant_id] = time.time()
                        self._stats.streams_closed += 1
                        self._stats.active_streams = len(self._streams)
    
    async def close_all(self):
        """Close all active streams."""
        if not self._streams:
            return
        
        logger.info(f"ðŸš« Closing {len(self._streams)} STT streams...")
        
        streams_to_close = list(self._streams)
        for stream in streams_to_close:
            try:
                await stream.aclose()
            except Exception as e:
                logger.error(f"Error closing stream: {e}")
            finally:
                self._streams.discard(stream)
                self._stream_metadata.pop(stream, None)
        
        self._participant_streams.clear()
        self._cleanup_locks.clear()
        self._participant_disconnect_times.clear()
        self._stats.active_streams = 0
        logger.info("âœ… All STT streams closed")
    
    async def close_participant_stream(self, participant_id: str):
        """Close a specific participant's stream."""
        if participant_id not in self._participant_streams:
            return
        
        if participant_id not in self._cleanup_locks:
            self._cleanup_locks[participant_id] = asyncio.Lock()
        
        async with self._cleanup_locks[participant_id]:
            stream = self._participant_streams.get(participant_id)
            if stream:
                logger.info(f"ðŸš« Closing STT stream for participant {participant_id}")
                try:
                    await stream.aclose()
                except Exception as e:
                    logger.error(f"Error closing participant stream: {e}")
                finally:
                    self._streams.discard(stream)
                    self._stream_metadata.pop(stream, None)
                    self._participant_streams.pop(participant_id, None)
                    # Record disconnect time for debouncing
                    self._participant_disconnect_times[participant_id] = time.time()
                    self._stats.streams_closed += 1
                    self._stats.active_streams = len(self._streams)
    
    def get_stats(self) -> ResourceStats:
        """Get current statistics."""
        return self._stats


class HeartbeatMonitor:
    """
    Monitors participant activity and detects stuck sessions.
    """
    
    def __init__(self, timeout: float = 30.0):
        self.timeout = timeout
        self.participants: Dict[str, datetime] = {}
        self.session_info: Dict[str, Dict[str, Any]] = {}
        self._monitor_task: Optional[asyncio.Task] = None
        self._callbacks: List[Callable[[str], Any]] = []
        logger.info(f"ðŸ’“ HeartbeatMonitor initialized with {timeout}s timeout")
    
    def register_callback(self, callback: Callable[[str], Any]):
        """Register a callback to be called when a participant times out."""
        self._callbacks.append(callback)
    
    async def update_heartbeat(self, participant_id: str, session_id: Optional[str] = None):
        """Update the heartbeat timestamp for a participant."""
        self.participants[participant_id] = datetime.utcnow()
        if session_id:
            self.session_info[participant_id] = {
                "session_id": session_id,
                "last_seen": datetime.utcnow()
            }
        logger.debug(f"ðŸ’“ Heartbeat updated for {participant_id}")
    
    async def check_timeouts(self) -> List[str]:
        """Check for timed-out participants."""
        now = datetime.utcnow()
        timed_out = []
        
        for participant_id, last_seen in list(self.participants.items()):
            elapsed = (now - last_seen).total_seconds()
            if elapsed > self.timeout:
                timed_out.append(participant_id)
                logger.warning(f"â° Participant {participant_id} timed out (last seen {elapsed:.1f}s ago)")
                
                # Remove from tracking
                self.participants.pop(participant_id, None)
                session_info = self.session_info.pop(participant_id, None)
                
                # Call registered callbacks
                for callback in self._callbacks:
                    try:
                        if asyncio.iscoroutinefunction(callback):
                            await callback(participant_id)
                        else:
                            callback(participant_id)
                    except Exception as e:
                        logger.error(f"Error in heartbeat callback: {e}")
        
        return timed_out
    
    async def start_monitoring(self):
        """Start the heartbeat monitoring loop."""
        if self._monitor_task and not self._monitor_task.done():
            return
        
        async def monitor_loop():
            while True:
                try:
                    await asyncio.sleep(10)  # Check every 10 seconds
                    timed_out = await self.check_timeouts()
                    if timed_out:
                        logger.info(f"ðŸ’” {len(timed_out)} participants timed out")
                except asyncio.CancelledError:
                    break
                except Exception as e:
                    logger.error(f"Error in heartbeat monitor: {e}")
        
        self._monitor_task = asyncio.create_task(monitor_loop())
        logger.info("ðŸ’“ Heartbeat monitoring started")
    
    async def stop_monitoring(self):
        """Stop the heartbeat monitoring loop."""
        if self._monitor_task and not self._monitor_task.done():
            self._monitor_task.cancel()
            try:
                await self._monitor_task
            except asyncio.CancelledError:
                pass
        logger.info("ðŸ’” Heartbeat monitoring stopped")
    
    def remove_participant(self, participant_id: str):
        """Remove a participant from monitoring."""
        self.participants.pop(participant_id, None)
        self.session_info.pop(participant_id, None)
        logger.debug(f"ðŸ’” Participant {participant_id} removed from heartbeat monitoring")


class ResourceManager:
    """
    Central resource manager for the application.
    Coordinates TaskManager and STTStreamManager.
    """
    
    def __init__(self):
        self.task_manager = TaskManager("main")
        self.stt_manager = STTStreamManager()
        self.heartbeat_monitor = HeartbeatMonitor(timeout=45.0)  # 45 seconds timeout
        self._shutdown_handlers: List[Callable] = []
        logger.info("ðŸ—ï¸ ResourceManager initialized")
    
    async def __aenter__(self):
        """Context manager entry."""
        await self.task_manager.__aenter__()
        await self.heartbeat_monitor.start_monitoring()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit with cleanup."""
        await self.shutdown()
    
    def add_shutdown_handler(self, handler: Callable):
        """Add a handler to be called on shutdown."""
        self._shutdown_handlers.append(handler)
    
    async def shutdown(self):
        """Shutdown all managed resources."""
        logger.info("ðŸ›‘ Starting ResourceManager shutdown...")
        
        # Run shutdown handlers
        for handler in self._shutdown_handlers:
            try:
                if asyncio.iscoroutinefunction(handler):
                    await handler()
                else:
                    handler()
            except Exception as e:
                logger.error(f"Error in shutdown handler: {e}")
        
        # Shutdown managers
        await self.heartbeat_monitor.stop_monitoring()
        await self.task_manager.shutdown()
        await self.stt_manager.close_all()
        
        logger.info("âœ… ResourceManager shutdown complete")
    
    def get_all_stats(self) -> Dict[str, Any]:
        """Get statistics from all managers."""
        return {
            "tasks": self.task_manager.get_stats(),
            "stt_streams": self.stt_manager.get_stats(),
            "heartbeat": {
                "active_participants": len(self.heartbeat_monitor.participants),
                "timeout": self.heartbeat_monitor.timeout
            }
        }
    
    def log_stats(self):
        """Log current resource statistics."""
        stats = self.get_all_stats()
        logger.info(
            f"ðŸ“Š Resource Stats - "
            f"Tasks: {stats['tasks'].active_tasks} active "
            f"({stats['tasks'].tasks_completed} completed, "
            f"{stats['tasks'].tasks_failed} failed, "
            f"{stats['tasks'].tasks_cancelled} cancelled), "
            f"STT Streams: {stats['stt_streams'].active_streams} active, "
            f"Heartbeat: {stats['heartbeat']['active_participants']} participants"
        )
    
    async def verify_cleanup_complete(self) -> Dict[str, Any]:
        """Verify all resources are properly cleaned up."""
        active_tasks = self.task_manager.get_active_tasks()
        active_streams = len(self.stt_manager._streams)
        active_participants = len(self.heartbeat_monitor.participants)
        
        # Check if connection pool is closed
        db_closed = True
        try:
            from database import _pool
            if hasattr(_pool, '_local') and hasattr(_pool._local, 'session'):
                db_closed = _pool._local.session is None or _pool._local.session.closed
        except:
            pass
        
        cleanup_complete = (
            len(active_tasks) == 0 and 
            active_streams == 0 and 
            active_participants == 0 and
            db_closed
        )
        
        result = {
            "cleanup_complete": cleanup_complete,
            "tasks_remaining": len(active_tasks),
            "active_task_names": [t.get_name() for t in active_tasks],
            "streams_remaining": active_streams,
            "participants_remaining": active_participants,
            "participant_ids": list(self.heartbeat_monitor.participants.keys()),
            "db_connections_closed": db_closed,
            "timestamp": datetime.utcnow().isoformat()
        }
        
        if not cleanup_complete:
            logger.warning(f"âš ï¸ Cleanup verification failed: {result}")
        else:
            logger.info("âœ… Cleanup verification passed - all resources cleaned up")
        
        return result


================================================
FILE: backup_20250728_212515/sync_dev_to_production.sh
================================================
#!/bin/bash

# Dev Server to Production Server Update Script
# This script syncs changes from the development server to production
# while preserving production-specific optimizations
# 
# Author: Bayaan DevOps Team
# Date: $(date +%Y-%m-%d)
# Last Updated: 2025-01-28

set -euo pipefail  # Exit on error, undefined variables, pipe failures

# Configuration
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
DEV_DIR="${SCRIPT_DIR}/../server"
PROD_DIR="${SCRIPT_DIR}"
BACKUP_DIR="${PROD_DIR}/backup_$(date +%Y%m%d_%H%M%S)"
LOG_FILE="${PROD_DIR}/sync_dev_production_$(date +%Y%m%d_%H%M%S).log"
REPORT_FILE="${PROD_DIR}/sync_report_$(date +%Y%m%d_%H%M%S).md"

# Color codes for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color

# Logging function
log() {
    echo -e "$1" | tee -a "$LOG_FILE"
}

# Error handling
error_exit() {
    log "${RED}ERROR: $1${NC}"
    log "${YELLOW}Rolling back changes...${NC}"
    rollback
    exit 1
}

# Initialize sync report
init_report() {
    cat > "$REPORT_FILE" << EOF
# Dev Server â†’ Production Server Sync Report
**Date:** $(date)
**Source:** $DEV_DIR
**Target:** $PROD_DIR

## Summary of Changes

EOF
}

# Add to report
report() {
    echo "$1" >> "$REPORT_FILE"
}

# Rollback function
rollback() {
    if [ -d "$BACKUP_DIR" ]; then
        log "${YELLOW}Restoring from backup: $BACKUP_DIR${NC}"
        
        # Restore all backed up files
        for file in "$BACKUP_DIR"/*; do
            if [ -f "$file" ]; then
                filename=$(basename "$file")
                cp -f "$file" "$PROD_DIR/$filename" 2>/dev/null || true
                log "Restored: $filename"
            fi
        done
        
        log "${GREEN}Rollback completed${NC}"
    else
        log "${RED}No backup found for rollback${NC}"
    fi
}

# Pre-flight checks
preflight_checks() {
    log "${CYAN}=== Starting Pre-flight Checks ===${NC}"
    
    # Check if dev server directory exists
    if [ ! -d "$DEV_DIR" ]; then
        error_exit "Dev server directory not found: $DEV_DIR"
    fi
    
    # Check if we're in the production directory
    if [ ! -d "$PROD_DIR" ]; then
        error_exit "Production directory not found: $PROD_DIR"
    fi
    
    # Verify Python is available
    if ! command -v python3 &> /dev/null; then
        error_exit "Python3 is required but not installed"
    fi
    
    log "${GREEN}Pre-flight checks passed${NC}"
}

# Create backup
create_backup() {
    log "${CYAN}=== Creating Backup ===${NC}"
    
    # Create backup directory
    mkdir -p "$BACKUP_DIR" || error_exit "Cannot create backup directory"
    
    # Backup all Python files and critical configs
    local files_to_backup=(
        "*.py"
        ".env"
        "requirements.txt"
        "Dockerfile"
        "render.yaml"
        ".gitignore"
        "*.sh"
    )
    
    for pattern in "${files_to_backup[@]}"; do
        for file in $pattern; do
            if [ -f "$file" ]; then
                cp -p "$file" "$BACKUP_DIR/" 2>/dev/null || true
                log "Backed up: $file"
            fi
        done
    done
    
    log "${GREEN}Backup created at: $BACKUP_DIR${NC}"
    report "### Backup Location"
    report "\`$BACKUP_DIR\`"
    report ""
}

# Update core files that have been modified
update_core_files() {
    log "${CYAN}=== Updating Core Files ===${NC}"
    report "### Updated Files"
    report ""
    
    # Files that need to be updated (modified in both)
    local core_files=(
        "broadcasting.py"
        "config.py"
        "database.py"
        "database_enhanced.py"
        "main.py"
        "prompt_builder.py"
        "resource_management.py"
        "text_processing.py"
        "translation_helpers.py"
        "translator.py"
        "webhook_handler.py"
    )
    
    for file in "${core_files[@]}"; do
        if [ -f "$DEV_DIR/$file" ]; then
            if [ -f "$PROD_DIR/$file" ]; then
                # Check if files are different
                if ! diff -q "$DEV_DIR/$file" "$PROD_DIR/$file" > /dev/null 2>&1; then
                    cp -f "$DEV_DIR/$file" "$PROD_DIR/$file"
                    log "${GREEN}Updated: $file${NC}"
                    report "- **Updated:** \`$file\`"
                else
                    log "${BLUE}No changes: $file${NC}"
                fi
            else
                # File doesn't exist in production, copy it
                cp -f "$DEV_DIR/$file" "$PROD_DIR/$file"
                log "${GREEN}Added: $file${NC}"
                report "- **Added:** \`$file\` (was missing in production)"
            fi
        fi
    done
}

# Add new files from dev server
add_new_files() {
    log "${CYAN}=== Adding New Files from Dev Server ===${NC}"
    report ""
    report "### New Files Added"
    report ""
    
    # New files to add (exist in dev but not in production)
    local new_files=(
        "speechmatics_advanced.py"
        "speechmatics_domain_patch.py"
    )
    
    # Test/utility files to optionally add
    local optional_files=(
        "check_stt_params.py"
        "database_cleanup_fix.py"
        "simple_domain_test.py"
        "test_domain_patch.py"
        "test_domain_support.py"
        "test_room_domain.py"
        "verify_domain_config.py"
    )
    
    # Add essential new files
    for file in "${new_files[@]}"; do
        if [ -f "$DEV_DIR/$file" ] && [ ! -f "$PROD_DIR/$file" ]; then
            cp -f "$DEV_DIR/$file" "$PROD_DIR/$file"
            log "${GREEN}Added new file: $file${NC}"
            report "- **Added:** \`$file\` (Speechmatics domain support)"
        fi
    done
    
    # Report optional files (but don't copy automatically)
    report ""
    report "### Optional Files (Not Copied)"
    report "These files exist in dev but are test/utility files:"
    report ""
    
    for file in "${optional_files[@]}"; do
        if [ -f "$DEV_DIR/$file" ] && [ ! -f "$PROD_DIR/$file" ]; then
            log "${YELLOW}Optional file available: $file${NC}"
            report "- \`$file\` - $(get_file_purpose "$file")"
        fi
    done
}

# Get file purpose for documentation
get_file_purpose() {
    local file=$1
    case $file in
        "check_stt_params.py") echo "STT parameter verification utility" ;;
        "database_cleanup_fix.py") echo "Database cleanup script" ;;
        "simple_domain_test.py") echo "Domain testing utility" ;;
        "test_domain_patch.py") echo "Domain patch testing" ;;
        "test_domain_support.py") echo "Domain support testing" ;;
        "test_room_domain.py") echo "Room domain configuration test" ;;
        "verify_domain_config.py") echo "Domain configuration verification" ;;
        *) echo "Utility/test file" ;;
    esac
}

# Handle production-specific files
handle_production_files() {
    log "${CYAN}=== Checking Production-Specific Files ===${NC}"
    report ""
    report "### Production-Specific Files"
    report ""
    
    # main_production.py exists only in production
    if [ -f "$PROD_DIR/main_production.py" ]; then
        log "${YELLOW}Note: main_production.py is production-specific and was not modified${NC}"
        report "- \`main_production.py\` - Production entry point (preserved)"
    fi
    
    # Check if render.yaml or other deployment configs need updates
    if [ -f "$PROD_DIR/render.yaml" ]; then
        report "- \`render.yaml\` - Deployment configuration (preserved)"
    fi
}

# Update requirements.txt if needed
check_requirements() {
    log "${CYAN}=== Checking Requirements ===${NC}"
    report ""
    report "### Dependencies"
    report ""
    
    if [ -f "$DEV_DIR/requirements.txt" ] && [ -f "$PROD_DIR/requirements.txt" ]; then
        # Create sorted unique lists
        sort "$DEV_DIR/requirements.txt" | grep -v "^#" | grep -v "^$" > /tmp/dev_reqs_sorted.txt
        sort "$PROD_DIR/requirements.txt" | grep -v "^#" | grep -v "^$" > /tmp/prod_reqs_sorted.txt
        
        # Find differences
        if ! diff -q /tmp/dev_reqs_sorted.txt /tmp/prod_reqs_sorted.txt > /dev/null 2>&1; then
            log "${YELLOW}Requirements differ between dev and production${NC}"
            report "**Note:** requirements.txt files differ. Manual review recommended."
            
            # Show new requirements in dev
            comm -23 /tmp/dev_reqs_sorted.txt /tmp/prod_reqs_sorted.txt > /tmp/new_reqs.txt
            if [ -s /tmp/new_reqs.txt ]; then
                report ""
                report "New dependencies in dev:"
                while IFS= read -r req; do
                    report "- \`$req\`"
                done < /tmp/new_reqs.txt
            fi
        else
            log "${GREEN}Requirements are in sync${NC}"
            report "Requirements files are identical."
        fi
        
        # Cleanup
        rm -f /tmp/dev_reqs_sorted.txt /tmp/prod_reqs_sorted.txt /tmp/new_reqs.txt
    fi
}

# Verify Python syntax
verify_syntax() {
    log "${CYAN}=== Verifying Python Syntax ===${NC}"
    
    local all_good=true
    
    for file in "$PROD_DIR"/*.py; do
        if [ -f "$file" ]; then
            filename=$(basename "$file")
            if python3 -m py_compile "$file" 2>/dev/null; then
                log "${GREEN}âœ“ Syntax OK: $filename${NC}"
            else
                log "${RED}âœ— Syntax error in: $filename${NC}"
                all_good=false
            fi
        fi
    done
    
    # Clean up __pycache__
    rm -rf "$PROD_DIR/__pycache__" 2>/dev/null || true
    
    if [ "$all_good" = false ]; then
        error_exit "Python syntax verification failed"
    fi
    
    log "${GREEN}All Python files passed syntax check${NC}"
}

# Generate final recommendations
generate_recommendations() {
    log "${CYAN}=== Generating Recommendations ===${NC}"
    
    report ""
    report "## Post-Sync Recommendations"
    report ""
    report "### Important Notes"
    report ""
    report "1. **STT Stream Fix**: The recent STT stream reconnection fix has been applied to both environments"
    report "2. **Domain Support**: New Speechmatics domain support files have been added"
    report "3. **Resource Management**: Enhanced resource cleanup and debouncing implemented"
    report ""
    report "### Testing Checklist"
    report ""
    report "- [ ] Test STT stream reconnection scenarios"
    report "- [ ] Verify duplicate transcription prevention"
    report "- [ ] Test participant disconnect/reconnect within 3 seconds"
    report "- [ ] Verify resource cleanup on disconnect"
    report "- [ ] Test Speechmatics domain configuration (when enabled)"
    report ""
    report "### Deployment Steps"
    report ""
    report "1. Review this report and the sync log"
    report "2. Run local tests if possible"
    report "3. Commit changes: \`git add . && git commit -m \"Sync dev changes: STT fixes and domain support\"\`"
    report "4. Deploy to Render: \`git push\`"
    report "5. Monitor logs after deployment"
    report ""
    report "### Rollback Instructions"
    report ""
    report "If issues occur, run: \`bash $0 --rollback\`"
    report ""
    report "**Backup Location:** \`$BACKUP_DIR\`"
    report "**Log File:** \`$LOG_FILE\`"
}

# Main execution
main() {
    log "${GREEN}=== Dev Server â†’ Production Server Sync Script ===${NC}"
    log "Started at: $(date)"
    log ""
    
    # Initialize report
    init_report
    
    # Execute sync steps
    preflight_checks
    create_backup
    update_core_files
    add_new_files
    handle_production_files
    check_requirements
    verify_syntax
    generate_recommendations
    
    # Final summary
    log ""
    log "${GREEN}=== Sync Completed Successfully ===${NC}"
    log ""
    log "${CYAN}Important Files:${NC}"
    log "  ðŸ“„ Sync Report: ${YELLOW}$REPORT_FILE${NC}"
    log "  ðŸ“‹ Log File: ${YELLOW}$LOG_FILE${NC}"
    log "  ðŸ’¾ Backup: ${YELLOW}$BACKUP_DIR${NC}"
    log ""
    log "Review the sync report for detailed changes and recommendations."
    
    # Show the report
    echo ""
    echo "Opening sync report..."
    cat "$REPORT_FILE"
}

# Handle rollback option
if [ "${1:-}" == "--rollback" ]; then
    # Find the most recent backup
    LATEST_BACKUP=$(ls -td ${PROD_DIR}/backup_* 2>/dev/null | head -1)
    if [ -n "$LATEST_BACKUP" ]; then
        BACKUP_DIR="$LATEST_BACKUP"
        LOG_FILE="${PROD_DIR}/rollback_$(date +%Y%m%d_%H%M%S).log"
        log "${YELLOW}Rolling back to: $BACKUP_DIR${NC}"
        rollback
        log "${GREEN}Rollback completed successfully${NC}"
        exit 0
    else
        echo "${RED}No backup found for rollback${NC}"
        exit 1
    fi
fi

# Run main function
main "$@"


================================================
FILE: backup_20250728_212515/sync_ghost_session_fix.sh
================================================
#!/bin/bash

# Ghost Session Fix Sync Script for Bayaan Production Server
# This script syncs the ghost session fixes from server to production
# Author: SuperClaude DevOps
# Date: $(date +%Y-%m-%d)

set -euo pipefail

# Configuration
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
SERVER_DIR="${SCRIPT_DIR}/../server"
PROD_DIR="${SCRIPT_DIR}"
BACKUP_DIR="${PROD_DIR}/backup_ghost_fix_$(date +%Y%m%d_%H%M%S)"
LOG_FILE="${PROD_DIR}/ghost_fix_sync_$(date +%Y%m%d_%H%M%S).log"

# Color codes
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
CYAN='\033[0;36m'
NC='\033[0m'

# Logging function
log() {
    echo -e "$1" | tee -a "$LOG_FILE"
}

# Create backup
create_backup() {
    log "${CYAN}=== Creating Backup ===${NC}"
    mkdir -p "$BACKUP_DIR"
    
    # Backup critical files
    for file in database.py main.py requirements.txt; do
        if [ -f "$PROD_DIR/$file" ]; then
            cp -p "$PROD_DIR/$file" "$BACKUP_DIR/"
            log "Backed up: $file"
        fi
    done
    
    log "${GREEN}Backup created at: $BACKUP_DIR${NC}"
}

# Main sync function
main() {
    log "${GREEN}=== Ghost Session Fix Sync Script ===${NC}"
    log "Started at: $(date)"
    log ""
    
    # Create backup first
    create_backup
    
    # Step 1: Check if database_enhanced.py exists in production
    if [ -f "$PROD_DIR/database_enhanced.py" ]; then
        log "${GREEN}âœ… database_enhanced.py already in production${NC}"
    else
        log "${RED}âŒ database_enhanced.py missing - please run the main fix script first${NC}"
        exit 1
    fi
    
    # Step 2: Create integration patch for database.py
    log "${CYAN}=== Creating Database Integration Patch ===${NC}"
    
    cat > "$PROD_DIR/database_integration.patch" << 'EOF'
# Add these imports at the top of database.py after existing imports:
from database_enhanced import (
    ensure_active_session_atomic as _ensure_active_session_atomic,
    SessionHealthMonitor,
    update_session_heartbeat_enhanced
)

# Initialize health monitor (add after _pool initialization)
_health_monitor = SessionHealthMonitor()

# Replace the existing ensure_active_session function with:
async def ensure_active_session(room_id: int, mosque_id: int) -> Optional[str]:
    """
    Enhanced version with atomic session creation and ghost prevention.
    """
    try:
        session = await _pool.get_session()
        async with get_db_headers() as headers:
            return await _ensure_active_session_atomic(
                room_id, mosque_id, session, headers
            )
    except Exception as e:
        logger.error(f"Failed to ensure active session: {e}")
        return None

# Add this new function for enhanced heartbeat:
async def update_session_heartbeat_with_monitor(session_id: str) -> bool:
    """
    Update heartbeat with health monitoring.
    """
    if not session_id:
        return False
        
    try:
        session = await _pool.get_session()
        async with get_db_headers() as headers:
            # Use health monitor
            healthy = await _health_monitor.monitor_heartbeat(
                session_id, session, headers
            )
            
            if not healthy:
                if _health_monitor.should_force_cleanup(session_id):
                    logger.error(f"Session {session_id} needs force cleanup")
                    return False
                else:
                    _health_monitor.increment_recovery_attempt(session_id)
                    
            return healthy
    except Exception as e:
        logger.error(f"Heartbeat monitor error: {e}")
        return False

# Export health monitor for use in main.py
def get_health_monitor():
    return _health_monitor
EOF
    
    log "${GREEN}âœ… Integration patch created${NC}"
    
    # Step 3: Create main.py integration instructions
    log "${CYAN}=== Creating Main.py Integration Instructions ===${NC}"
    
    cat > "$PROD_DIR/main_integration.md" << 'EOF'
# Main.py Integration for Ghost Session Fix

## Required Changes:

### 1. Import the health monitor (at top of file):
```python
from database import get_health_monitor, update_session_heartbeat_with_monitor
```

### 2. Initialize health monitor (in entrypoint function):
```python
# After initializing resource_manager
health_monitor = get_health_monitor()
```

### 3. Update the heartbeat periodic function:
Replace the existing `update_session_heartbeat_periodic` function with:

```python
async def update_session_heartbeat_periodic():
    """Periodically update session heartbeat with health monitoring."""
    while not stop_heartbeat:
        try:
            if tenant_context and tenant_context.get('session_id'):
                session_id = tenant_context['session_id']
                
                # Use enhanced heartbeat with monitoring
                success = await update_session_heartbeat_with_monitor(session_id)
                
                if not success:
                    logger.error(f"Session {session_id} health check failed - may need cleanup")
                    # The monitor will handle cleanup if needed
                    
            await asyncio.sleep(30)
        except Exception as e:
            logger.error(f"Heartbeat task error: {e}")
            await asyncio.sleep(30)
```

### 4. Add cleanup on connect (in entrypoint, after room join):
```python
# Clean any stale sessions for this room on connect
if tenant_context.get('room_id'):
    logger.info("Checking for stale sessions on connect...")
    # The atomic session creation will handle cleanup automatically
```

## Testing the Integration:

1. Check logs for "ðŸ§¹ Cleaned X stale sessions" messages
2. Monitor for "Session health check failed" warnings
3. Verify no ghost sessions in database after disconnects
EOF
    
    log "${GREEN}âœ… Integration instructions created${NC}"
    
    # Step 4: Check for SQL migration status
    log "${CYAN}=== Checking SQL Migration Status ===${NC}"
    
    log "${YELLOW}âš ï¸  IMPORTANT: Make sure you've run the SQL migration:${NC}"
    log "  ${CYAN}20250128_fix_ghost_sessions_comprehensive.sql${NC}"
    log ""
    log "The migration adds:"
    log "  - ensure_room_session_atomic() function"
    log "  - update_session_heartbeat_enhanced() function"
    log "  - cleanup_ghost_sessions() function"
    log "  - ghost_session_monitor view"
    log ""
    
    # Step 5: Create deployment checklist
    log "${CYAN}=== Creating Deployment Checklist ===${NC}"
    
    cat > "$PROD_DIR/GHOST_FIX_DEPLOYMENT.md" << 'EOF'
# Ghost Session Fix Deployment Checklist

## Pre-Deployment:
- [ ] Run SQL migration in Supabase
- [ ] Backup production database.py
- [ ] Review database_integration.patch
- [ ] Test in development environment

## Integration Steps:
1. [ ] Apply database.py integration patch
2. [ ] Update main.py with health monitoring
3. [ ] Verify Python syntax: `python3 -m py_compile *.py`
4. [ ] Run local tests if available

## Deployment:
1. [ ] Commit changes to git
2. [ ] Push to production branch
3. [ ] Monitor Render deployment logs
4. [ ] Check for startup errors

## Post-Deployment Verification:
1. [ ] Check application logs for:
   - "âœ… Active session ensured" messages
   - "ðŸ§¹ Cleaned X stale sessions" messages
   - "ðŸ’“ Heartbeat updated" messages

2. [ ] Monitor database for ghost sessions:
   ```sql
   SELECT * FROM ghost_session_monitor;
   ```

3. [ ] Test session creation and cleanup:
   - Connect to a room
   - Verify session created
   - Disconnect
   - Verify session cleaned up

## Rollback Plan:
If issues occur:
1. Restore from backup: `backup_ghost_fix_*/`
2. Redeploy previous version on Render
3. Investigate logs for root cause
EOF
    
    log "${GREEN}âœ… Deployment checklist created${NC}"
    
    # Step 6: Summary
    log ""
    log "${GREEN}=== Sync Complete ===${NC}"
    log ""
    log "${CYAN}Files Created:${NC}"
    log "  ðŸ“„ database_integration.patch - Integration code for database.py"
    log "  ðŸ“„ main_integration.md - Instructions for main.py updates"
    log "  ðŸ“„ GHOST_FIX_DEPLOYMENT.md - Deployment checklist"
    log ""
    log "${YELLOW}Next Steps:${NC}"
    log "  1. Review the integration files"
    log "  2. Apply patches to database.py and main.py"
    log "  3. Test the changes locally"
    log "  4. Follow GHOST_FIX_DEPLOYMENT.md for deployment"
    log ""
    log "${CYAN}Backup Location:${NC} $BACKUP_DIR"
    log "${CYAN}Log File:${NC} $LOG_FILE"
    log ""
    log "Completed at: $(date)"
}

# Run main function
main "$@"


================================================
FILE: backup_20250728_212515/text_processing.py
================================================
"""
Text processing utilities for LiveKit AI Translation Server.
Handles sentence extraction and text manipulation for Arabic and other languages.
"""
import re
import logging
from typing import Tuple, List

logger = logging.getLogger("transcriber.text_processing")


def extract_complete_sentences(text: str) -> Tuple[List[str], str]:
    """
    Extract complete sentences from text and return them along with remaining incomplete text.
    
    This function is designed to work with Arabic text and handles Arabic punctuation marks.
    It identifies complete sentences based on punctuation and returns both the complete
    sentences and any remaining incomplete text.
    
    Args:
        text: The input text to process
        
    Returns:
        A tuple containing:
        - List of complete sentences
        - Remaining incomplete text
    """
    if not text.strip():
        return [], ""
    
    # Arabic sentence ending punctuation marks
    sentence_endings = ['.', '!', '?', 'ØŸ']  # Including Arabic question mark
    
    complete_sentences = []
    remaining_text = ""
    
    logger.debug(f"ðŸ” Processing text for sentence extraction: '{text}'")
    
    # Check if this is standalone punctuation
    if text.strip() in sentence_endings:
        logger.debug(f"ðŸ“ Detected standalone punctuation: '{text.strip()}'")
        # This is standalone punctuation - signal to complete any accumulated sentence
        return ["PUNCTUATION_COMPLETE"], ""
    
    # Split text into parts ending with punctuation
    # This regex splits on punctuation but keeps the punctuation in the result
    parts = re.split(r'([.!?ØŸ])', text)
    
    current_building = ""
    i = 0
    while i < len(parts):
        part = parts[i].strip()
        if not part:
            i += 1
            continue
            
        if part in sentence_endings:
            # Found punctuation - complete the current sentence
            if current_building.strip():
                complete_sentence = current_building.strip() + part
                complete_sentences.append(complete_sentence)
                logger.debug(f"âœ… Complete sentence found: '{complete_sentence}'")
                current_building = ""
            i += 1
        else:
            # Regular text - add to current building
            current_building += (" " + part if current_building else part)
            i += 1
    
    # Any remaining text becomes the incomplete part
    if current_building.strip():
        remaining_text = current_building.strip()
        logger.debug(f"ðŸ”„ Remaining incomplete text: '{remaining_text}'")
    
    logger.debug(f"ðŸ“Š Extracted {len(complete_sentences)} complete sentences, remaining: '{remaining_text}'")
    return complete_sentences, remaining_text


def is_sentence_ending(text: str) -> bool:
    """
    Check if the text ends with a sentence-ending punctuation mark.
    
    Args:
        text: The text to check
        
    Returns:
        True if the text ends with sentence-ending punctuation
    """
    if not text.strip():
        return False
    
    sentence_endings = ['.', '!', '?', 'ØŸ']
    return text.strip()[-1] in sentence_endings


def clean_text(text: str) -> str:
    """
    Clean and normalize text for processing.
    
    Args:
        text: The text to clean
        
    Returns:
        Cleaned text
    """
    # Remove extra whitespace
    text = ' '.join(text.split())
    # Remove leading/trailing whitespace
    text = text.strip()
    return text


def split_into_chunks(text: str, max_length: int = 500) -> List[str]:
    """
    Split text into chunks of maximum length, preferring to split at sentence boundaries.
    
    Args:
        text: The text to split
        max_length: Maximum length of each chunk
        
    Returns:
        List of text chunks
    """
    if len(text) <= max_length:
        return [text]
    
    chunks = []
    sentences, _ = extract_complete_sentences(text)
    
    current_chunk = ""
    for sentence in sentences:
        if len(current_chunk) + len(sentence) + 1 <= max_length:
            current_chunk += (" " + sentence if current_chunk else sentence)
        else:
            if current_chunk:
                chunks.append(current_chunk)
            current_chunk = sentence
    
    if current_chunk:
        chunks.append(current_chunk)
    
    return chunks


================================================
FILE: backup_20250728_212515/translation_helpers.py
================================================
"""
Translation helper functions for LiveKit AI Translation Server.
Handles translation orchestration and related utilities.
"""
import asyncio
import logging
from typing import List, Dict, Any, Optional

logger = logging.getLogger("transcriber.translation")


async def translate_sentences(
    sentences: List[str], 
    translators: Dict[str, Any],
    source_language: str = "ar",
    sentence_id: Optional[str] = None
) -> None:
    """
    Translate complete sentences to all target languages.
    
    This function takes a list of sentences and sends them to all available
    translators concurrently for better performance.
    
    Args:
        sentences: List of sentences to translate
        translators: Dictionary of language code to translator instances
        source_language: Source language code (default: "ar" for Arabic)
        sentence_id: Optional sentence ID for tracking
    """
    if not sentences or not translators:
        return
        
    for sentence in sentences:
        if sentence.strip():
            logger.info(f"ðŸŽ¯ TRANSLATING COMPLETE {source_language.upper()} SENTENCE: '{sentence}'")
            logger.info(f"ðŸ“Š Processing sentence for {len(translators)} translators")
            
            # Send to all translators concurrently for better performance
            translation_tasks = []
            for lang, translator in translators.items():
                logger.info(f"ðŸ“¤ Sending complete {source_language.upper()} sentence '{sentence}' to {lang} translator")
                translation_tasks.append(translator.translate(sentence, sentence_id))
            
            # Execute all translations concurrently
            if translation_tasks:
                results = await asyncio.gather(*translation_tasks, return_exceptions=True)
                # Check for any exceptions
                for i, result in enumerate(results):
                    if isinstance(result, Exception):
                        logger.error(f"âŒ Translation failed: {result}")


async def translate_single_sentence(
    sentence: str,
    translator: Any,
    target_language: str
) -> Optional[str]:
    """
    Translate a single sentence to a specific target language.
    
    Args:
        sentence: The sentence to translate
        translator: The translator instance to use
        target_language: Target language code
        
    Returns:
        Translated text or None if translation failed
    """
    try:
        if not sentence.strip():
            return None
            
        logger.debug(f"Translating to {target_language}: '{sentence}'")
        result = await translator.translate(sentence, None)
        return result
    except Exception as e:
        logger.error(f"Translation to {target_language} failed: {e}")
        return None


def should_translate_text(text: str, min_length: int = 3) -> bool:
    """
    Determine if text should be translated based on various criteria.
    
    Args:
        text: The text to evaluate
        min_length: Minimum length for translation (default: 3 characters)
        
    Returns:
        True if text should be translated
    """
    if not text or not text.strip():
        return False
    
    # Don't translate very short text
    if len(text.strip()) < min_length:
        return False
    
    # Don't translate if it's only punctuation
    if all(c in '.!?ØŸ,ØŒ;Ø›:' for c in text.strip()):
        return False
    
    return True


def format_translation_output(
    original_text: str,
    translated_text: str,
    source_lang: str,
    target_lang: str
) -> Dict[str, str]:
    """
    Format translation output for consistent structure.
    
    Args:
        original_text: Original text
        translated_text: Translated text
        source_lang: Source language code
        target_lang: Target language code
        
    Returns:
        Formatted translation dictionary
    """
    return {
        "original": original_text,
        "translated": translated_text,
        "source_language": source_lang,
        "target_language": target_lang,
        "type": "translation"
    }


async def batch_translate(
    texts: List[str],
    translators: Dict[str, Any],
    batch_size: int = 5
) -> Dict[str, List[str]]:
    """
    Translate multiple texts in batches for efficiency.
    
    Args:
        texts: List of texts to translate
        translators: Dictionary of language code to translator instances
        batch_size: Number of texts to process in each batch
        
    Returns:
        Dictionary mapping language codes to lists of translations
    """
    results = {lang: [] for lang in translators.keys()}
    
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]
        
        # Process each batch
        for text in batch:
            if should_translate_text(text):
                await translate_sentences([text], translators)
    
    return results


================================================
FILE: backup_20250728_212515/translator.py
================================================
"""
Translator module for LiveKit AI Translation Server.
Handles real-time translation with context management and error handling.
"""
import asyncio
import logging
from typing import Optional, Dict, Any, List, Callable
from collections import deque
from enum import Enum

from livekit import rtc
from livekit.agents import llm, utils
from livekit.plugins import openai

from config import get_config
from prompt_builder import get_prompt_builder

logger = logging.getLogger("transcriber.translator")
config = get_config()
prompt_builder = get_prompt_builder()


class TranslationError(Exception):
    """Custom exception for translation-related errors."""
    pass


class Translator:
    """
    Handles translation from source language to target language with context management.
    
    Features:
    - Sliding window context for better translation coherence
    - Automatic retry on failures
    - Comprehensive error handling
    - Real-time broadcasting to displays
    """
    
    # Class-level configuration from config module
    use_context = config.translation.use_context
    default_max_context_pairs = config.translation.max_context_pairs
    
    def __init__(self, room: rtc.Room, lang: Enum, tenant_context: Optional[Dict[str, Any]] = None, broadcast_callback: Optional[Callable] = None):
        """
        Initialize the Translator.
        
        Args:
            room: LiveKit room instance
            lang: Target language enum
            tenant_context: Optional context containing room_id, mosque_id, etc.
            broadcast_callback: Optional callback function for broadcasting translations
        """
        self.room = room
        self.lang = lang
        self.tenant_context = tenant_context or {}
        self.broadcast_callback = broadcast_callback
        self.llm = openai.LLM()
        
        # Initialize system prompt as None - will be built dynamically
        self.system_prompt = None
        self._prompt_template = None
        
        # Get context window size from room config or use default
        self.max_context_pairs = config.translation.get_context_window_size(tenant_context)
        
        # Use deque for automatic sliding window (old messages auto-removed)
        if self.use_context:
            self.message_history: deque = deque(maxlen=(self.max_context_pairs * 2))
        
        # Track translation statistics
        self.translation_count = 0
        self.error_count = 0
        
        # Log the context mode being used
        context_mode = f"TRUE SLIDING WINDOW ({self.max_context_pairs}-pair memory)" if self.use_context else "FRESH CONTEXT (no memory)"
        logger.info(f"ðŸ§  Translator initialized for {lang.value} with {context_mode} mode")
        
        # Initialize prompt asynchronously on first use
        self._prompt_initialized = False

    async def translate(self, message: str, sentence_id: Optional[str] = None, max_retries: int = 2) -> str:
        """
        Translate a message from source to target language.
        
        Args:
            message: Text to translate
            sentence_id: Optional sentence ID for tracking
            max_retries: Maximum number of retry attempts on failure
            
        Returns:
            Translated text (empty string on failure)
            
        Raises:
            TranslationError: If translation fails after all retries
        """
        if not message or not message.strip():
            logger.debug("Empty message, skipping translation")
            return ""
        
        retry_count = 0
        last_error = None
        
        while retry_count <= max_retries:
            try:
                translated_message = await self._perform_translation(message)
                
                if translated_message:
                    # Publish transcription to LiveKit room
                    await self._publish_transcription(translated_message, None)
                    
                    # Broadcast to displays
                    await self._broadcast_translation(translated_message, sentence_id)
                    
                    # Update statistics
                    self.translation_count += 1
                    
                    # Log successful translation
                    logger.info(f"âœ… Translated to {self.lang.value}: '{message}' â†’ '{translated_message}'")
                    
                    return translated_message
                else:
                    logger.warning(f"Empty translation result for: '{message}'")
                    return ""
                    
            except Exception as e:
                last_error = e
                retry_count += 1
                self.error_count += 1
                
                if retry_count <= max_retries:
                    wait_time = retry_count * 0.5  # Exponential backoff
                    logger.warning(
                        f"Translation attempt {retry_count} failed: {e}. "
                        f"Retrying in {wait_time}s..."
                    )
                    await asyncio.sleep(wait_time)
                else:
                    logger.error(
                        f"âŒ Translation failed after {max_retries} retries: {e}\n"
                        f"Message: '{message}'"
                    )
                    
        # If we get here, all retries failed
        error_msg = f"Translation failed for '{message}' after {max_retries} retries"
        if last_error:
            error_msg += f": {last_error}"
        
        # Don't raise exception - return empty string to keep stream going
        logger.error(error_msg)
        return ""

    async def _initialize_prompt(self):
        """Initialize the system prompt using the prompt builder."""
        if self._prompt_initialized:
            return
            
        try:
            # Get room ID from tenant context
            room_id = self.tenant_context.get('room_id')
            
            # Get source language from room config
            source_lang_code = self.tenant_context.get('transcription_language', 'ar')
            source_lang_name = config.translation.supported_languages.get(
                source_lang_code, 
                {"name": "Arabic"}
            )["name"]
            
            # Get target language name from the enum value
            target_lang_name = self.lang.name  # This should give us "Dutch" instead of "nl"
            
            # Build the prompt using prompt builder
            self.system_prompt = await prompt_builder.get_prompt_for_room(
                room_id=room_id,
                source_lang=source_lang_name,
                target_lang=target_lang_name,
                room_config=self.tenant_context
            )
            
            logger.info(f"ðŸ“ Initialized translation prompt for room {room_id}: {source_lang_name} â†’ {target_lang_name} (code: {self.lang.value})")
            self._prompt_initialized = True
            
        except Exception as e:
            logger.error(f"Failed to initialize prompt: {e}")
            # Fallback to default prompt with dynamic source language
            source_lang_code = self.tenant_context.get('transcription_language', 'ar')
            source_lang_name = config.translation.supported_languages.get(
                source_lang_code, 
                {"name": "Arabic"}
            )["name"]
            
            self.system_prompt = (
                f"You are an expert simultaneous interpreter. Your task is to translate from {source_lang_name} to {self.lang.value}. "
                f"Provide a direct and accurate translation of the user's input. Be concise and use natural-sounding language. "
                f"Do not add any additional commentary, explanations, or introductory phrases."
            )
            self._prompt_initialized = True
    
    async def _perform_translation(self, message: str) -> str:
        """
        Perform the actual translation using the LLM.
        
        Args:
            message: Text to translate
            
        Returns:
            Translated text
        """
        # Ensure prompt is initialized
        await self._initialize_prompt()
        # Build a fresh context for every translation (rebuild method)
        temp_context = llm.ChatContext()
        temp_context.add_message(role="system", content=self.system_prompt)
        
        # If using context, add the message history from our deque
        if self.use_context and hasattr(self, 'message_history'):
            logger.debug(f"ðŸ”„ Building context with {len(self.message_history)} historical messages")
            for msg in self.message_history:
                temp_context.add_message(role=msg['role'], content=msg['content'])
        
        # Add the current message to translate
        temp_context.add_message(content=message, role="user")
        
        # Get translation from LLM with the freshly built context
        stream = self.llm.chat(chat_ctx=temp_context)
        
        translated_message = ""
        async for chunk in stream:
            if chunk.delta is None:
                continue
            content = chunk.delta.content
            if content is None:
                break
            translated_message += content
        
        # If using context, update our history (deque will auto-remove old messages)
        if self.use_context and translated_message:
            self.message_history.append({"role": "user", "content": message})
            self.message_history.append({"role": "assistant", "content": translated_message})
            logger.debug(f"ðŸ’¾ History updated. Current size: {len(self.message_history)} messages")
        
        return translated_message

    async def _publish_transcription(self, translated_text: str, track: Optional[rtc.Track]) -> None:
        """
        Publish the translation as a transcription to the LiveKit room.
        
        Args:
            translated_text: The translated text to publish
            track: Optional track reference
        """
        try:
            segment = rtc.TranscriptionSegment(
                id=utils.misc.shortuuid("SG_"),
                text=translated_text,
                start_time=0,
                end_time=0,
                language=self.lang.value,
                final=True,
            )
            transcription = rtc.Transcription(
                self.room.local_participant.identity, 
                track.sid if track else "", 
                [segment]
            )
            await self.room.local_participant.publish_transcription(transcription)
            logger.debug(f"ðŸ“¤ Published {self.lang.value} transcription to LiveKit room")
        except Exception as e:
            logger.error(f"Failed to publish transcription: {e}")
            # Don't re-raise - translation was successful even if publishing failed

    async def _broadcast_translation(self, translated_text: str, sentence_id: Optional[str] = None) -> None:
        """
        Broadcast the translation to WebSocket displays.
        
        Args:
            translated_text: The translated text to broadcast
            sentence_id: Optional sentence ID for tracking
        """
        if self.broadcast_callback:
            try:
                # Use asyncio.create_task to avoid blocking
                # Include sentence context if provided
                sentence_context = None
                if sentence_id:
                    sentence_context = {
                        "sentence_id": sentence_id,
                        "is_complete": True,
                        "is_fragment": False
                    }
                
                asyncio.create_task(
                    self.broadcast_callback(
                        "translation", 
                        self.lang.value, 
                        translated_text, 
                        self.tenant_context,
                        sentence_context
                    )
                )
                logger.debug(f"ðŸ“¡ Broadcasted {self.lang.value} translation to displays")
            except Exception as e:
                logger.error(f"Failed to broadcast translation: {e}")
                # Don't re-raise - translation was successful even if broadcasting failed
        else:
            logger.debug("No broadcast callback provided, skipping broadcast")

    def get_statistics(self) -> Dict[str, Any]:
        """
        Get translation statistics.
        
        Returns:
            Dictionary containing translation stats
        """
        return {
            "language": self.lang.value,
            "translation_count": self.translation_count,
            "error_count": self.error_count,
            "error_rate": self.error_count / max(1, self.translation_count),
            "context_enabled": self.use_context,
            "context_size": len(self.message_history) if self.use_context else 0
        }

    def clear_context(self) -> None:
        """Clear the translation context history."""
        if self.use_context and hasattr(self, 'message_history'):
            self.message_history.clear()
            logger.info(f"ðŸ§¹ Cleared translation context for {self.lang.value}")

    def __repr__(self) -> str:
        """String representation of the Translator."""
        return (
            f"Translator(lang={self.lang.value}, "
            f"context={self.use_context}, "
            f"translations={self.translation_count}, "
            f"errors={self.error_count})"
        )


================================================
FILE: backup_20250728_212515/update_from_server_dev.sh
================================================
#!/bin/bash

# Server_Dev to Production Update Script for Bayaan Server
# This script intelligently syncs changes from server_dev while preserving production optimizations
# Author: Bayaan DevOps Team
# Date: $(date +%Y-%m-%d)

set -euo pipefail  # Exit on error, undefined variables, pipe failures

# Configuration
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
DEV_DIR="${SCRIPT_DIR}/server_dev"
PROD_DIR="${SCRIPT_DIR}"
BACKUP_DIR="${PROD_DIR}/backup_$(date +%Y%m%d_%H%M%S)"
LOG_FILE="${PROD_DIR}/update_server_dev_$(date +%Y%m%d_%H%M%S).log"
MERGE_REPORT="${PROD_DIR}/merge_report_$(date +%Y%m%d_%H%M%S).md"

# Color codes for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color

# Logging function
log() {
    echo -e "$1" | tee -a "$LOG_FILE"
}

# Error handling
error_exit() {
    log "${RED}ERROR: $1${NC}"
    log "${YELLOW}Rolling back changes...${NC}"
    rollback
    exit 1
}

# Initialize merge report
init_merge_report() {
    cat > "$MERGE_REPORT" << EOF
# Server_Dev â†’ Production Update Report
**Date:** $(date)
**Source:** $DEV_DIR
**Target:** $PROD_DIR

## Update Summary

EOF
}

# Add to merge report
report() {
    echo "$1" >> "$MERGE_REPORT"
}

# Rollback function
rollback() {
    if [ -d "$BACKUP_DIR" ]; then
        log "${YELLOW}Restoring from backup: $BACKUP_DIR${NC}"
        
        # Restore all backed up files
        for file in "$BACKUP_DIR"/*; do
            if [ -f "$file" ]; then
                filename=$(basename "$file")
                cp -f "$file" "$PROD_DIR/$filename" 2>/dev/null || true
                log "Restored: $filename"
            fi
        done
        
        log "${GREEN}Rollback completed${NC}"
    else
        log "${RED}No backup found for rollback${NC}"
    fi
}

# Pre-flight checks
preflight_checks() {
    log "${CYAN}=== Starting Pre-flight Checks ===${NC}"
    
    # Check if server_dev directory exists
    if [ ! -d "$DEV_DIR" ]; then
        error_exit "Server_dev directory not found: $DEV_DIR"
    fi
    
    # Check if we're in the production directory
    if [ "$(pwd)" != "$PROD_DIR" ]; then
        log "${YELLOW}Changing to production directory${NC}"
        cd "$PROD_DIR" || error_exit "Cannot change to production directory"
    fi
    
    # Check for critical production files
    local critical_files=("main_production.py" "render.yaml" "requirements.txt" "Dockerfile")
    for file in "${critical_files[@]}"; do
        if [ ! -f "$file" ]; then
            log "${RED}WARNING: Critical production file missing: $file${NC}"
        fi
    done
    
    # Verify Python is available
    if ! command -v python3 &> /dev/null; then
        error_exit "Python3 is required but not installed"
    fi
    
    log "${GREEN}Pre-flight checks passed${NC}"
}

# Create backup
create_backup() {
    log "${CYAN}=== Creating Backup ===${NC}"
    
    # Create backup directory
    mkdir -p "$BACKUP_DIR" || error_exit "Cannot create backup directory"
    
    # Backup all Python files and critical configs
    local files_to_backup=(
        "*.py"
        ".env"
        "requirements.txt"
        "Dockerfile"
        "render.yaml"
        ".gitignore"
        "*.sh"
    )
    
    for pattern in "${files_to_backup[@]}"; do
        for file in $pattern; do
            if [ -f "$file" ]; then
                cp -p "$file" "$BACKUP_DIR/" 2>/dev/null || true
                log "Backed up: $file"
            fi
        done
    done
    
    log "${GREEN}Backup created at: $BACKUP_DIR${NC}"
    report "### Backup Location\n\`$BACKUP_DIR\`\n"
}

# Compare files and determine update strategy
compare_and_update() {
    local file=$1
    local src_file="$DEV_DIR/$file"
    local dst_file="$PROD_DIR/$file"
    
    if [ ! -f "$src_file" ]; then
        log "${YELLOW}Source file not found, skipping: $file${NC}"
        return 1
    fi
    
    # If destination doesn't exist, it's a simple copy
    if [ ! -f "$dst_file" ]; then
        cp -f "$src_file" "$dst_file"
        log "${GREEN}Added new file: $file${NC}"
        report "- **Added:** \`$file\` (new file from server_dev)"
        return 0
    fi
    
    # Check if files are different
    if ! diff -q "$src_file" "$dst_file" > /dev/null 2>&1; then
        return 0  # Files are different, needs update
    else
        return 1  # Files are identical
    fi
}

# Update simple files (direct replacement)
update_simple_files() {
    log "${CYAN}=== Updating Simple Files ===${NC}"
    report "\n### Simple File Updates\n"
    
    # Files that can be safely replaced without merging
    local simple_files=(
        "config.py"
        "prompt_builder.py"
        "text_processing.py"
        "translation_helpers.py"
        "translator.py"
        "webhook_handler.py"
    )
    
    for file in "${simple_files[@]}"; do
        if compare_and_update "$file"; then
            cp -f "$DEV_DIR/$file" "$PROD_DIR/$file"
            log "${GREEN}Updated: $file${NC}"
            report "- **Updated:** \`$file\`"
        else
            log "${BLUE}No changes needed: $file${NC}"
        fi
    done
}

# Handle complex files with merge logic
update_complex_files() {
    log "${CYAN}=== Handling Complex Files ===${NC}"
    report "\n### Complex File Handling\n"
    
    # main.py - Preserve production optimizations
    if [ -f "$DEV_DIR/main.py" ]; then
        log "${YELLOW}Analyzing main.py differences...${NC}"
        
        # Create a comparison report
        if diff -u "$PROD_DIR/main.py" "$DEV_DIR/main.py" > /tmp/main_diff.txt 2>&1; then
            log "${BLUE}main.py is identical in both versions${NC}"
        else
            log "${YELLOW}main.py has differences - preserving production optimizations${NC}"
            report "- **main.py:** Differences detected - production optimizations preserved"
            report "  - Kept production's interim transcript handling"
            report "  - Kept production's simplified cleanup approach"
            report "  - Review \`/tmp/main_diff.txt\` for detailed differences"
            
            # Don't update main.py automatically - requires manual review
            log "${YELLOW}âš ï¸  main.py requires manual review due to production-specific optimizations${NC}"
        fi
    fi
    
    # database.py - Check for schema changes
    if [ -f "$DEV_DIR/database.py" ]; then
        if compare_and_update "database.py"; then
            log "${YELLOW}database.py has changes - reviewing for compatibility...${NC}"
            
            # Check if new functions are added that don't exist in production
            if grep -q "close_room_session\|update_session_heartbeat" "$DEV_DIR/database.py"; then
                log "${YELLOW}New database functions detected (heartbeat/session management)${NC}"
                report "- **database.py:** New session management functions detected"
                report "  - Contains heartbeat monitoring functions not used in production"
                report "  - Manual review recommended"
            fi
            
            # For now, skip automatic update of database.py
            log "${YELLOW}âš ï¸  database.py update skipped - manual review required${NC}"
        fi
    fi
    
    # broadcasting.py
    if compare_and_update "broadcasting.py"; then
        cp -f "$DEV_DIR/broadcasting.py" "$PROD_DIR/broadcasting.py"
        log "${GREEN}Updated: broadcasting.py${NC}"
        report "- **Updated:** \`broadcasting.py\`"
    fi
    
    # resource_management.py
    if compare_and_update "resource_management.py"; then
        cp -f "$DEV_DIR/resource_management.py" "$PROD_DIR/resource_management.py"
        log "${GREEN}Updated: resource_management.py${NC}"
        report "- **Updated:** \`resource_management.py\`"
    fi
}

# Handle new files from server_dev
handle_new_files() {
    log "${CYAN}=== Checking for New Files ===${NC}"
    report "\n### New Files Analysis\n"
    
    # Files to explicitly exclude
    local exclude_patterns=(
        "*_cleanup*.py"
        "*_fix.py"
        "*.sql"
        "start_server.sh"
        "production_deployment.md"
        "DEPLOYMENT.md"
        "CLEANUP_SUMMARY.md"
    )
    
    # Check for new Python files
    for file in "$DEV_DIR"/*.py; do
        if [ -f "$file" ]; then
            filename=$(basename "$file")
            
            # Skip if file exists in production
            if [ -f "$PROD_DIR/$filename" ]; then
                continue
            fi
            
            # Check exclusion patterns
            local skip=false
            for pattern in "${exclude_patterns[@]}"; do
                if [[ "$filename" == $pattern ]]; then
                    skip=true
                    break
                fi
            done
            
            if [ "$skip" = true ]; then
                log "${YELLOW}Excluded new file: $filename${NC}"
                report "- **Excluded:** \`$filename\` (development/cleanup file)"
            else
                log "${CYAN}Found new file: $filename - requires review${NC}"
                report "- **New file found:** \`$filename\` - manual review required"
            fi
        fi
    done
}

# Update requirements.txt intelligently
update_requirements() {
    log "${CYAN}=== Checking requirements.txt ===${NC}"
    report "\n### Dependencies Update\n"
    
    if [ -f "$DEV_DIR/requirements.txt" ] && [ -f "$PROD_DIR/requirements.txt" ]; then
        # Create sorted unique lists
        sort "$DEV_DIR/requirements.txt" | grep -v "^#" | grep -v "^$" > /tmp/dev_reqs.txt
        sort "$PROD_DIR/requirements.txt" | grep -v "^#" | grep -v "^$" > /tmp/prod_reqs.txt
        
        # Find new requirements in dev
        comm -23 /tmp/dev_reqs.txt /tmp/prod_reqs.txt > /tmp/new_reqs.txt
        
        if [ -s /tmp/new_reqs.txt ]; then
            log "${YELLOW}New dependencies found in server_dev:${NC}"
            cat /tmp/new_reqs.txt | while read -r req; do
                log "  + $req"
                report "- New dependency: \`$req\`"
            done
            report "\nâš ï¸  **Action Required:** Review and add new dependencies to production requirements.txt"
        else
            log "${GREEN}No new dependencies found${NC}"
            report "- No new dependencies detected"
        fi
        
        # Cleanup temp files
        rm -f /tmp/dev_reqs.txt /tmp/prod_reqs.txt /tmp/new_reqs.txt
    fi
}

# Verify Python syntax
verify_python_syntax() {
    log "${CYAN}=== Verifying Python Syntax ===${NC}"
    
    local all_good=true
    
    for file in *.py; do
        if [ -f "$file" ]; then
            if python3 -m py_compile "$file" 2>/dev/null; then
                log "${GREEN}âœ“ Syntax OK: $file${NC}"
            else
                log "${RED}âœ— Syntax error in: $file${NC}"
                all_good=false
            fi
        fi
    done
    
    # Clean up __pycache__
    rm -rf __pycache__ 2>/dev/null || true
    
    if [ "$all_good" = false ]; then
        error_exit "Python syntax verification failed"
    fi
    
    log "${GREEN}All Python files passed syntax check${NC}"
}

# Generate final recommendations
generate_recommendations() {
    log "${CYAN}=== Generating Recommendations ===${NC}"
    
    report "\n## Post-Update Recommendations\n"
    report "### Manual Review Required:"
    report "1. **main.py** - Review differences between dev and production versions"
    report "2. **database.py** - Check if new session management functions are needed"
    report "3. **New files** - Evaluate any new files from server_dev for inclusion"
    report "4. **Dependencies** - Review and update requirements.txt if needed"
    report ""
    report "### Testing Checklist:"
    report "- [ ] Run local tests with updated code"
    report "- [ ] Verify WebSocket connections work correctly"
    report "- [ ] Test transcript handling (both final and interim)"
    report "- [ ] Confirm database operations function properly"
    report "- [ ] Check resource cleanup on disconnection"
    report ""
    report "### Deployment Steps:"
    report "1. Review this report and the update log"
    report "2. Manually review complex files if needed"
    report "3. Run \`git status\` to see all changes"
    report "4. Test locally if possible"
    report "5. Commit changes with descriptive message"
    report "6. Deploy to Render following standard procedure"
    report ""
    report "### Rollback Instructions:"
    report "If issues occur, run: \`bash $0 --rollback\`"
    report ""
    report "**Backup Location:** \`$BACKUP_DIR\`"
    report "**Log File:** \`$LOG_FILE\`"
}

# Post-update summary
post_update_summary() {
    log ""
    log "${GREEN}=== Update Completed Successfully ===${NC}"
    log ""
    log "${CYAN}Important Files:${NC}"
    log "  ðŸ“„ Merge Report: ${YELLOW}$MERGE_REPORT${NC}"
    log "  ðŸ“‹ Log File: ${YELLOW}$LOG_FILE${NC}"
    log "  ðŸ’¾ Backup: ${YELLOW}$BACKUP_DIR${NC}"
    log ""
    log "${YELLOW}Next Steps:${NC}"
    log "  1. Review the merge report for detailed changes"
    log "  2. Manually review files marked for attention"
    log "  3. Run tests before deploying"
    log ""
    log "To rollback if needed: ${CYAN}bash $0 --rollback${NC}"
}

# Rollback functionality
if [ "${1:-}" == "--rollback" ]; then
    # Initialize log for rollback
    LOG_FILE="${PROD_DIR}/rollback_$(date +%Y%m%d_%H%M%S).log"
    
    # Find the most recent backup
    LATEST_BACKUP=$(ls -td ${PROD_DIR}/backup_* 2>/dev/null | head -1)
    if [ -n "$LATEST_BACKUP" ]; then
        BACKUP_DIR="$LATEST_BACKUP"
        log "${YELLOW}Rolling back to: $BACKUP_DIR${NC}"
        rollback
        log "${GREEN}Rollback completed successfully${NC}"
        exit 0
    else
        log "${RED}No backup found for rollback${NC}"
        exit 1
    fi
fi

# Main execution
main() {
    log "${GREEN}=== Bayaan Server_Dev â†’ Production Update Script ===${NC}"
    log "Started at: $(date)"
    log "Source: $DEV_DIR"
    log "Target: $PROD_DIR"
    log ""
    
    # Initialize merge report
    init_merge_report
    
    # Execute update steps
    preflight_checks
    create_backup
    update_simple_files
    update_complex_files
    handle_new_files
    update_requirements
    verify_python_syntax
    generate_recommendations
    post_update_summary
    
    log "Completed at: $(date)"
}

# Run main function
main "$@"


================================================
FILE: backup_20250728_212515/update_prod.sh
================================================
#!/bin/bash

# Production Update Script for Bayaan Server
# This script safely updates production files from development while preserving production-specific configurations
# Author: Senior DevOps Engineer
# Date: $(date +%Y-%m-%d)

set -euo pipefail  # Exit on error, undefined variables, pipe failures

# Configuration
DEV_DIR="/mnt/c/Users/hassa/OneDrive/Desktop/0.2 Bayan/bayan-platform-admin-login/Backend/LiveKit-ai-translation/server"
PROD_DIR="/mnt/c/Users/hassa/OneDrive/Desktop/0.2 Bayan/bayaan-server-production"
BACKUP_DIR="${PROD_DIR}/backup_$(date +%Y%m%d_%H%M%S)"
LOG_FILE="${PROD_DIR}/update_$(date +%Y%m%d_%H%M%S).log"

# Color codes for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

# Logging function
log() {
    echo -e "$1" | tee -a "$LOG_FILE"
}

# Error handling
error_exit() {
    log "${RED}ERROR: $1${NC}"
    log "${YELLOW}Rolling back changes...${NC}"
    rollback
    exit 1
}

# Rollback function
rollback() {
    if [ -d "$BACKUP_DIR" ]; then
        log "${YELLOW}Restoring from backup: $BACKUP_DIR${NC}"
        
        # Restore Python files
        for file in "$BACKUP_DIR"/*.py; do
            if [ -f "$file" ]; then
                filename=$(basename "$file")
                cp -f "$file" "$PROD_DIR/$filename" 2>/dev/null || true
                log "Restored: $filename"
            fi
        done
        
        log "${GREEN}Rollback completed${NC}"
    else
        log "${RED}No backup found for rollback${NC}"
    fi
}

# Pre-flight checks
preflight_checks() {
    log "${YELLOW}=== Starting Pre-flight Checks ===${NC}"
    
    # Check if dev directory exists
    if [ ! -d "$DEV_DIR" ]; then
        error_exit "Development directory not found: $DEV_DIR"
    fi
    
    # Check if we're in the production directory
    if [ "$(pwd)" != "$PROD_DIR" ]; then
        log "${YELLOW}Changing to production directory${NC}"
        cd "$PROD_DIR" || error_exit "Cannot change to production directory"
    fi
    
    # Check for critical production files
    local critical_files=("main_production.py" "render.yaml" "requirements.txt" "Dockerfile" ".env")
    for file in "${critical_files[@]}"; do
        if [ ! -f "$file" ]; then
            log "${RED}WARNING: Critical production file missing: $file${NC}"
        fi
    done
    
    log "${GREEN}Pre-flight checks passed${NC}"
}

# Create backup
create_backup() {
    log "${YELLOW}=== Creating Backup ===${NC}"
    
    # Create backup directory
    mkdir -p "$BACKUP_DIR" || error_exit "Cannot create backup directory"
    
    # Backup all Python files and critical configs
    local files_to_backup=(
        "*.py"
        ".env"
        "requirements.txt"
        "Dockerfile"
        "render.yaml"
        ".gitignore"
    )
    
    for pattern in "${files_to_backup[@]}"; do
        for file in $pattern; do
            if [ -f "$file" ]; then
                cp -p "$file" "$BACKUP_DIR/" 2>/dev/null || true
                log "Backed up: $file"
            fi
        done
    done
    
    log "${GREEN}Backup created at: $BACKUP_DIR${NC}"
}

# Update files from development
update_files() {
    log "${YELLOW}=== Updating Files from Development ===${NC}"
    
    # Core Python modules to update (excluding dev-specific files)
    local python_files=(
        "main.py"
        "prompt_builder.py"
        "broadcasting.py"
        "config.py"
        "database.py"
        "resource_management.py"
        "text_processing.py"
        "translation_helpers.py"
        "translator.py"
        "webhook_handler.py"
    )
    
    # Files to explicitly exclude
    local exclude_patterns=(
        "*_backup.py"
        "*_fixed.py"
        "*_cleanup*.py"
        "production_deployment.md"
    )
    
    # Update each Python file
    for file in "${python_files[@]}"; do
        local src_file="$DEV_DIR/$file"
        
        if [ -f "$src_file" ]; then
            # Check if file exists in exclude patterns
            local skip=false
            for pattern in "${exclude_patterns[@]}"; do
                if [[ "$file" == $pattern ]]; then
                    skip=true
                    break
                fi
            done
            
            if [ "$skip" = false ]; then
                cp -f "$src_file" "$PROD_DIR/$file" || error_exit "Failed to copy $file"
                log "${GREEN}Updated: $file${NC}"
            else
                log "${YELLOW}Skipped (excluded): $file${NC}"
            fi
        else
            log "${YELLOW}Not found in dev (skipping): $file${NC}"
        fi
    done
    
    # Handle .env.example if it exists and production doesn't have it
    if [ -f "$DEV_DIR/.env.example" ] && [ ! -f "$PROD_DIR/.env.example" ]; then
        cp -f "$DEV_DIR/.env.example" "$PROD_DIR/.env.example"
        log "${GREEN}Added: .env.example${NC}"
    fi
    
    # Update .gitignore if needed
    if [ -f "$DEV_DIR/.gitignore" ]; then
        cp -f "$DEV_DIR/.gitignore" "$PROD_DIR/.gitignore"
        log "${GREEN}Updated: .gitignore${NC}"
    fi
}

# Verify production integrity
verify_production() {
    log "${YELLOW}=== Verifying Production Integrity ===${NC}"
    
    # Check that production-specific files are still present
    local prod_files=("main_production.py" "render.yaml" "requirements.txt" "Dockerfile" ".env")
    local all_good=true
    
    for file in "${prod_files[@]}"; do
        if [ -f "$file" ]; then
            log "${GREEN}âœ“ Production file intact: $file${NC}"
        else
            log "${RED}âœ— Production file missing: $file${NC}"
            all_good=false
        fi
    done
    
    # Check Python syntax for all .py files
    log "${YELLOW}Checking Python syntax...${NC}"
    for file in *.py; do
        if [ -f "$file" ]; then
            if python3 -m py_compile "$file" 2>/dev/null; then
                log "${GREEN}âœ“ Syntax OK: $file${NC}"
                rm -f "__pycache__/${file%.py}.cpython-*.pyc" 2>/dev/null
            else
                log "${RED}âœ— Syntax error in: $file${NC}"
                all_good=false
            fi
        fi
    done
    
    # Clean up __pycache__
    rmdir __pycache__ 2>/dev/null || true
    
    if [ "$all_good" = false ]; then
        error_exit "Production integrity check failed"
    fi
    
    log "${GREEN}Production integrity verified${NC}"
}

# Post-update recommendations
post_update_recommendations() {
    log "${YELLOW}=== Post-Update Recommendations ===${NC}"
    log ""
    log "1. ${YELLOW}Test locally:${NC} Test the updated code in a staging environment if available"
    log "2. ${YELLOW}Review logs:${NC} Check $LOG_FILE for any warnings"
    log "3. ${YELLOW}Git status:${NC} Run 'git status' to review changes before committing"
    log "4. ${YELLOW}Deploy:${NC} Follow your standard Render deployment process"
    log ""
    log "${GREEN}Update completed successfully!${NC}"
    log ""
    log "Backup location: $BACKUP_DIR"
    log "To rollback if needed, run: ${YELLOW}bash $0 --rollback${NC}"
}

# Standalone rollback option
if [ "${1:-}" == "--rollback" ]; then
    # Find the most recent backup
    LATEST_BACKUP=$(ls -td ${PROD_DIR}/backup_* 2>/dev/null | head -1)
    if [ -n "$LATEST_BACKUP" ]; then
        BACKUP_DIR="$LATEST_BACKUP"
        log "${YELLOW}Rolling back to: $BACKUP_DIR${NC}"
        rollback
        exit 0
    else
        log "${RED}No backup found for rollback${NC}"
        exit 1
    fi
fi

# Main execution
main() {
    log "${GREEN}=== Bayaan Production Update Script ===${NC}"
    log "Started at: $(date)"
    log "Dev source: $DEV_DIR"
    log "Production: $PROD_DIR"
    log ""
    
    # Execute update steps
    preflight_checks
    create_backup
    update_files
    verify_production
    post_update_recommendations
    
    log "Completed at: $(date)"
}

# Run main function
main "$@"


================================================
FILE: backup_20250728_212515/update_server.sh
================================================
#!/bin/bash

# Bayaan Server Update Script
# Updates production server with development files

set -e  # Exit on error

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

# Paths
PROD_DIR="/mnt/c/Users/hassa/OneDrive/Desktop/0.2 Bayan/bayaan-server-production"
DEV_DIR="/mnt/c/Users/hassa/OneDrive/Desktop/0.2 Bayan/Dev/bayan-platform-admin-login/Backend/LiveKit-ai-translation/server"
BACKUP_DIR="$PROD_DIR/backup_$(date +%Y%m%d_%H%M%S)"

echo -e "${GREEN}=== Bayaan Server Update Script ===${NC}"
echo "Production Dir: $PROD_DIR"
echo "Development Dir: $DEV_DIR"
echo ""

# Check if DEV directory exists
if [ ! -d "$DEV_DIR" ]; then
    echo -e "${RED}Error: Development directory not found!${NC}"
    exit 1
fi

# Step 1: Create backup
echo -e "${YELLOW}Step 1: Creating backup...${NC}"
mkdir -p "$BACKUP_DIR"

# Backup files that will be updated
cp "$PROD_DIR/main.py" "$BACKUP_DIR/" 2>/dev/null || true
cp "$PROD_DIR/resource_management.py" "$BACKUP_DIR/" 2>/dev/null || true
cp "$PROD_DIR/translation_helpers.py" "$BACKUP_DIR/" 2>/dev/null || true
cp "$PROD_DIR/broadcasting.py" "$BACKUP_DIR/" 2>/dev/null || true

echo -e "${GREEN}âœ“ Backup created at: $BACKUP_DIR${NC}"

# Step 2: Copy development files to production
echo -e "${YELLOW}Step 2: Copying development files...${NC}"

# Copy the main files
cp "$DEV_DIR/main.py" "$PROD_DIR/"
echo "  âœ“ Copied main.py"

cp "$DEV_DIR/resource_management.py" "$PROD_DIR/"
echo "  âœ“ Copied resource_management.py"

cp "$DEV_DIR/translation_helpers.py" "$PROD_DIR/"
echo "  âœ“ Copied translation_helpers.py"

cp "$DEV_DIR/broadcasting.py" "$PROD_DIR/"
echo "  âœ“ Copied broadcasting.py"

# Check if other files need updating
echo -e "${YELLOW}Step 3: Checking other files...${NC}"

# List of other files that might need updating
OTHER_FILES=("config.py" "database.py" "text_processing.py" "translator.py" "webhook_handler.py" "prompt_builder.py")

for file in "${OTHER_FILES[@]}"; do
    if [ -f "$DEV_DIR/$file" ]; then
        # Check if files are different
        if ! cmp -s "$PROD_DIR/$file" "$DEV_DIR/$file"; then
            echo -e "  ${YELLOW}! $file differs between DEV and PROD${NC}"
            cp "$PROD_DIR/$file" "$BACKUP_DIR/" 2>/dev/null || true
            cp "$DEV_DIR/$file" "$PROD_DIR/"
            echo "    âœ“ Updated $file"
        else
            echo "  - $file is identical (no update needed)"
        fi
    fi
done

# Step 4: Log the update
echo -e "${YELLOW}Step 4: Creating update log...${NC}"
cat > "$PROD_DIR/update_$(date +%Y%m%d_%H%M%S).log" << EOF
Update performed at: $(date)
Files updated:
- main.py (added heartbeat monitoring and sentence tracking)
- resource_management.py (added HeartbeatMonitor class)
- translation_helpers.py (added sentence_id parameter)
- broadcasting.py (added sentence context support)

Key improvements:
1. Heartbeat monitoring - Detects stuck participant sessions (45s timeout)
2. Sentence tracking - Unique IDs for better UI synchronization
3. Fragment handling - Improved real-time display
4. Resource management - Better cleanup and monitoring

Backup location: $BACKUP_DIR
EOF

echo -e "${GREEN}âœ“ Update log created${NC}"

# Step 5: Verify installation
echo -e "${YELLOW}Step 5: Verifying installation...${NC}"

# Check if key imports work
python3 -c "
import sys
sys.path.insert(0, '$PROD_DIR')
try:
    from resource_management import HeartbeatMonitor
    print('  âœ“ HeartbeatMonitor class imported successfully')
except ImportError as e:
    print('  âœ— Failed to import HeartbeatMonitor:', e)
    sys.exit(1)
"

if [ $? -eq 0 ]; then
    echo -e "${GREEN}âœ“ Verification passed${NC}"
else
    echo -e "${RED}âœ— Verification failed${NC}"
    echo -e "${YELLOW}Rolling back...${NC}"
    # Rollback
    for file in "$BACKUP_DIR"/*.py; do
        if [ -f "$file" ]; then
            filename=$(basename "$file")
            cp "$file" "$PROD_DIR/$filename"
        fi
    done
    echo -e "${GREEN}âœ“ Rollback completed${NC}"
    exit 1
fi

echo ""
echo -e "${GREEN}=== Update Complete ===${NC}"
echo ""
echo "Next steps:"
echo "1. Review the changes in your version control system"
echo "2. Restart the Bayaan server:"
echo "   - If using systemd: sudo systemctl restart bayaan"
echo "   - If using PM2: pm2 restart bayaan"
echo "   - If running directly: restart the Python process"
echo "3. Monitor logs for any errors"
echo "4. Test the heartbeat monitoring feature"
echo ""
echo "To rollback if needed:"
echo "  cp $BACKUP_DIR/*.py $PROD_DIR/"
echo ""


================================================
FILE: backup_20250728_212515/webhook_handler.py
================================================
#!/usr/bin/env python3
"""
Webhook handler for Supabase integration
Receives notifications about room creation and management from the dashboard
"""

import asyncio
import json
import logging
import os
from aiohttp import web
from typing import Dict, Any

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Webhook secret for validation (should match Supabase webhook secret)
WEBHOOK_SECRET = os.environ.get("SUPABASE_WEBHOOK_SECRET", "")

class WebhookHandler:
    def __init__(self):
        self.active_sessions: Dict[str, Dict[str, Any]] = {}
        
    async def handle_room_created(self, payload: dict):
        """Handle room creation webhook from Supabase"""
        try:
            # Extract room information (aligning with your Supabase schema)
            room_data = payload.get("record", {})
            room_name = room_data.get("Livekit_room_name")  # Note: Capital L in your schema
            mosque_id = room_data.get("mosque_id")
            room_id = room_data.get("id")
            room_title = room_data.get("Title")
            transcription_language = room_data.get("transcription_language")
            translation_language = room_data.get("translation__language")  # Note: double underscore
            
            if not room_name or not mosque_id:
                logger.error(f"Missing required fields in room creation webhook: {payload}")
                return {"status": "error", "message": "Missing Livekit_room_name or mosque_id"}
            
            # Store session information
            self.active_sessions[room_name] = {
                "room_id": room_id,
                "mosque_id": mosque_id,
                "room_title": room_title,
                "transcription_language": transcription_language or "ar",  # Default to Arabic
                "translation_language": translation_language or "nl",     # Default to Dutch
                "created_at": room_data.get("created_at"),
                "status": "active"
            }
            
            logger.info(f"ðŸ›ï¸ Room created for mosque {mosque_id}: {room_name} (ID: {room_id})")
            logger.info(f"ðŸ—£ï¸ Transcription: {transcription_language}, Translation: {translation_language}")
            logger.info(f"ðŸ“Š Active sessions: {len(self.active_sessions)}")
            
            return {"status": "success", "room_name": room_name, "room_id": room_id}
            
        except Exception as e:
            logger.error(f"Error handling room creation webhook: {e}")
            return {"status": "error", "message": str(e)}
    
    async def handle_room_deleted(self, payload: dict):
        """Handle room deletion webhook from Supabase"""
        try:
            # Extract room information
            room_data = payload.get("old_record", {})
            room_name = room_data.get("livekit_room_name")
            
            if room_name and room_name in self.active_sessions:
                del self.active_sessions[room_name]
                logger.info(f"ðŸ—‘ï¸ Room deleted: {room_name}")
                logger.info(f"ðŸ“Š Active sessions: {len(self.active_sessions)}")
            
            return {"status": "success", "room_name": room_name}
            
        except Exception as e:
            logger.error(f"Error handling room deletion webhook: {e}")
            return {"status": "error", "message": str(e)}
    
    async def handle_session_started(self, payload: dict):
        """Handle session start webhook from Supabase"""
        try:
            session_data = payload.get("record", {})
            room_id = session_data.get("room_id")
            session_id = session_data.get("id")
            mosque_id = session_data.get("mosque_id")
            logging_enabled = session_data.get("logging_enabled", False)
            
            logger.info(f"ðŸŽ¤ Session started: {session_id} for room {room_id}, mosque {mosque_id}")
            logger.info(f"ðŸ“ Logging enabled: {logging_enabled}")
            
            # Find matching room by room_id and update with session info
            room_found = False
            for room_name, room_info in self.active_sessions.items():
                if room_info.get("room_id") == room_id:
                    room_info["session_id"] = session_id
                    room_info["session_started_at"] = session_data.get("started_at")
                    room_info["logging_enabled"] = logging_enabled
                    room_info["status"] = "recording" if logging_enabled else "active"
                    logger.info(f"ðŸ›ï¸ Updated room {room_name} with session {session_id}")
                    room_found = True
                    break
            
            if not room_found:
                # Create temporary session entry if room not found
                logger.warning(f"âš ï¸ Room not found for session {session_id}, creating temporary entry")
                temp_room_name = f"session_{session_id[:8]}"
                self.active_sessions[temp_room_name] = {
                    "room_id": room_id,
                    "mosque_id": mosque_id,
                    "session_id": session_id,
                    "session_started_at": session_data.get("started_at"),
                    "logging_enabled": logging_enabled,
                    "status": "recording" if logging_enabled else "active",
                    "transcription_language": "ar",  # Default
                    "translation_language": "nl"    # Default
                }
                    
            return {"status": "success", "session_id": session_id, "logging_enabled": logging_enabled}
            
        except Exception as e:
            logger.error(f"Error handling session start webhook: {e}")
            return {"status": "error", "message": str(e)}
    
    async def handle_session_ended(self, payload: dict):
        """Handle session end webhook from Supabase"""
        try:
            session_data = payload.get("record", {})
            session_id = session_data.get("id")
            
            # Update session status
            for room_name, room_info in self.active_sessions.items():
                if room_info.get("session_id") == session_id:
                    room_info["session_ended_at"] = session_data.get("ended_at")
                    room_info["status"] = "ended"
                    logger.info(f"ðŸ›‘ Session ended for room {room_name}: {session_id}")
                    break
                    
            return {"status": "success", "session_id": session_id}
            
        except Exception as e:
            logger.error(f"Error handling session end webhook: {e}")
            return {"status": "error", "message": str(e)}
    
    def get_room_context(self, room_name: str) -> Dict[str, Any]:
        """Get tenant context for a specific room"""
        return self.active_sessions.get(room_name, {})

# Global webhook handler instance
webhook_handler = WebhookHandler()

async def handle_webhook(request):
    """Main webhook endpoint handler"""
    try:
        # Validate webhook secret if configured
        if WEBHOOK_SECRET:
            webhook_signature = request.headers.get("X-Supabase-Signature", "")
            # TODO: Implement proper signature validation
            
        # Parse webhook payload
        payload = await request.json()
        webhook_type = payload.get("type")
        table = payload.get("table")
        
        logger.info(f"ðŸ“¨ Received webhook: type={webhook_type}, table={table}")
        
        # Route to appropriate handler
        result = {"status": "error", "message": "Unknown webhook type"}
        
        if table == "rooms":
            if webhook_type == "INSERT":
                result = await webhook_handler.handle_room_created(payload)
            elif webhook_type == "DELETE":
                result = await webhook_handler.handle_room_deleted(payload)
                
        elif table == "room_sessions":
            if webhook_type == "INSERT":
                result = await webhook_handler.handle_session_started(payload)
            elif webhook_type == "UPDATE":
                # Check if session is ending
                if payload.get("record", {}).get("ended_at"):
                    result = await webhook_handler.handle_session_ended(payload)
                    
        return web.json_response(result)
        
    except json.JSONDecodeError:
        return web.json_response({"status": "error", "message": "Invalid JSON"}, status=400)
    except Exception as e:
        logger.error(f"Error processing webhook: {e}")
        return web.json_response({"status": "error", "message": str(e)}, status=500)

async def handle_status(request):
    """Status endpoint to check webhook handler health"""
    return web.json_response({
        "status": "healthy",
        "active_sessions": len(webhook_handler.active_sessions),
        "sessions": list(webhook_handler.active_sessions.keys())
    })

async def start_webhook_server():
    """Start the webhook server"""
    app = web.Application()
    
    # Add routes
    app.router.add_post('/webhook', handle_webhook)
    app.router.add_get('/status', handle_status)
    
    # Add CORS middleware
    @web.middleware
    async def cors_middleware(request, handler):
        if request.method == 'OPTIONS':
            return web.Response(headers={
                'Access-Control-Allow-Origin': '*',
                'Access-Control-Allow-Methods': 'POST, GET, OPTIONS',
                'Access-Control-Allow-Headers': 'Content-Type, X-Supabase-Signature',
            })
        response = await handler(request)
        response.headers['Access-Control-Allow-Origin'] = '*'
        return response
    
    app.middlewares.append(cors_middleware)
    
    # Start server
    runner = web.AppRunner(app)
    await runner.setup()
    site = web.TCPSite(runner, '0.0.0.0', 8767)
    await site.start()
    
    logger.info("ðŸš€ Webhook server started on http://0.0.0.0:8767")
    logger.info("ðŸ“¨ Webhook endpoint: POST http://0.0.0.0:8767/webhook")
    logger.info("ðŸ“Š Status endpoint: GET http://0.0.0.0:8767/status")
    
    try:
        await asyncio.Future()  # Run forever
    except KeyboardInterrupt:
        logger.info("Shutting down webhook server...")
        await runner.cleanup()

# Export the handler for use in main.py
def get_room_context(room_name: str) -> Dict[str, Any]:
    """Get tenant context for a room from webhook handler"""
    return webhook_handler.get_room_context(room_name)

if __name__ == "__main__":
    try:
        asyncio.run(start_webhook_server())
    except KeyboardInterrupt:
        logger.info("Webhook server stopped by user")
    except Exception as e:
        logger.error(f"Webhook server error: {e}")


================================================
FILE: backup_ghost_fix_20250728_034924/database.py
================================================
"""
Database operations for LiveKit AI Translation Server.
Handles all Supabase database interactions with connection pooling and async support.
FIXED: Thread-safe connection pool that works with LiveKit's multi-process architecture.
"""
import asyncio
import logging
import uuid
from typing import Optional, Dict, Any
from datetime import datetime
import aiohttp
from contextlib import asynccontextmanager
import threading

from config import get_config

logger = logging.getLogger("transcriber.database")
config = get_config()


class ThreadSafeDatabasePool:
    """Thread-safe database connection pool that creates separate pools per thread/process."""
    
    def __init__(self, max_connections: int = 10):
        self.max_connections = max_connections
        self._local = threading.local()
        self._lock = threading.Lock()
        
    async def get_session(self) -> aiohttp.ClientSession:
        """Get or create a session for the current thread."""
        # Check if current thread has a session
        if not hasattr(self._local, 'session') or self._local.session is None or self._local.session.closed:
            # Create new session for this thread
            connector = aiohttp.TCPConnector(
                limit=self.max_connections,
                limit_per_host=self.max_connections,
                force_close=True  # Force close to avoid connection issues
            )
            self._local.session = aiohttp.ClientSession(
                connector=connector,
                trust_env=True  # Trust environment proxy settings
            )
            logger.debug(f"Created new connection pool for thread {threading.current_thread().ident}")
        
        return self._local.session
    
    async def close(self):
        """Close the session for current thread."""
        if hasattr(self._local, 'session') and self._local.session and not self._local.session.closed:
            await self._local.session.close()
            self._local.session = None
            logger.debug(f"Closed connection pool for thread {threading.current_thread().ident}")


# Use thread-safe pool
_pool = ThreadSafeDatabasePool()


@asynccontextmanager
async def get_db_headers():
    """Get headers for Supabase API requests."""
    if not config.supabase.service_role_key:
        raise ValueError("SUPABASE_SERVICE_ROLE_KEY not configured")
    
    yield {
        'apikey': config.supabase.service_role_key,
        'Authorization': f'Bearer {config.supabase.service_role_key}',
        'Content-Type': 'application/json'
    }


async def ensure_active_session(room_id: int, mosque_id: int) -> Optional[str]:
    """
    Ensure there's an active session for the room and return session_id.
    
    This function:
    1. Checks for existing active sessions
    2. Creates a new session if none exists
    3. Returns the session ID or None on failure
    """
    try:
        # Get session from thread-safe pool
        session = await _pool.get_session()
        
        async with get_db_headers() as headers:
            # Check for existing active session
            url = f"{config.supabase.url}/rest/v1/room_sessions"
            params = {
                "room_id": f"eq.{room_id}",
                "status": "eq.active",
                "select": "id,started_at",
                "order": "started_at.desc",
                "limit": "1"
            }
            
            timeout = aiohttp.ClientTimeout(total=config.supabase.http_timeout)
            
            try:
                async with session.get(url, headers=headers, params=params, timeout=timeout) as response:
                    if response.status == 200:
                        sessions = await response.json()
                        if sessions and len(sessions) > 0:
                            session_id = sessions[0]["id"]
                            logger.debug(f"ðŸ“ Using existing active session: {session_id}")
                            return session_id
                    else:
                        error_text = await response.text()
                        logger.warning(f"Failed to check existing sessions: {response.status} - {error_text}")
            except asyncio.TimeoutError:
                logger.warning("Timeout checking for existing sessions")
            except Exception as e:
                logger.error(f"Error checking sessions: {e}")
            
            # Create new session if none exists
            new_session_id = str(uuid.uuid4())
            session_data = {
                "id": new_session_id,
                "room_id": room_id,
                "mosque_id": mosque_id,
                "status": "active",
                "started_at": datetime.utcnow().isoformat() + "Z",
                "logging_enabled": True
            }
            
            try:
                async with session.post(
                    url,
                    json=session_data,
                    headers={**headers, 'Prefer': 'return=minimal'},
                    timeout=timeout
                ) as response:
                    if response.status in [200, 201]:
                        logger.info(f"ðŸ“ Created new session: {new_session_id}")
                        return new_session_id
                    else:
                        error_text = await response.text()
                        logger.error(f"âŒ Failed to create session: {response.status} - {error_text}")
                        return None
            except asyncio.TimeoutError:
                logger.error("Timeout creating new session")
                return None
            except Exception as e:
                logger.error(f"Error creating session: {e}")
                return None
                    
    except Exception as e:
        logger.error(f"âŒ Session management failed: {e}")
        return None


async def store_transcript_in_database(
    message_type: str, 
    language: str, 
    text: str, 
    tenant_context: Dict[str, Any],
    sentence_context: Optional[Dict[str, Any]] = None
) -> bool:
    """
    Store transcription/translation in Supabase database.
    
    Args:
        message_type: Either "transcription" or "translation"
        language: Language code (e.g., "ar", "nl")
        text: The text to store
        tenant_context: Context containing room_id, mosque_id, session_id
        sentence_context: Optional context containing sentence_id, is_complete, is_fragment
        
    Returns:
        bool: True if successful, False otherwise
    """
    try:
        if not config.supabase.service_role_key:
            logger.error("âŒ SUPABASE_SERVICE_ROLE_KEY not found - cannot store transcripts")
            return False
            
        room_id = tenant_context.get("room_id")
        mosque_id = tenant_context.get("mosque_id")
        session_id = tenant_context.get("session_id")
        
        if not room_id or not mosque_id:
            logger.warning(f"âš ï¸ Missing room context: room_id={room_id}, mosque_id={mosque_id}")
            return False
            
        # Ensure we have an active session
        if not session_id:
            session_id = await ensure_active_session(room_id, mosque_id)
            if session_id:
                tenant_context["session_id"] = session_id
            else:
                logger.error("âŒ Could not establish session - skipping database storage")
                return False
        
        # Prepare transcript data
        transcript_data = {
            "room_id": room_id,
            "session_id": session_id,
            "timestamp": datetime.utcnow().isoformat() + "Z",
        }
        
        # Add sentence context if provided
        if sentence_context:
            transcript_data["sentence_id"] = sentence_context.get("sentence_id")
            transcript_data["is_complete"] = sentence_context.get("is_complete", False)
            transcript_data["is_fragment"] = sentence_context.get("is_fragment", True)
        
        # Set appropriate field based on message type
        if message_type == "transcription":
            transcript_data["transcription_segment"] = text
        else:  # translation
            transcript_data["translation_segment"] = text
            
        # Store in database
        session = await _pool.get_session()
        async with get_db_headers() as headers:
            timeout = aiohttp.ClientTimeout(total=config.supabase.http_timeout)
            
            try:
                async with session.post(
                    f"{config.supabase.url}/rest/v1/transcripts",
                    json=transcript_data,
                    headers={**headers, 'Prefer': 'return=minimal'},
                    timeout=timeout
                ) as response:
                    if response.status in [200, 201]:
                        logger.debug(f"âœ… Stored {message_type} in database: room_id={room_id}, session_id={session_id[:8]}")
                        return True
                    else:
                        error_text = await response.text()
                        logger.warning(f"âš ï¸ Database storage failed with status {response.status}: {error_text}")
                        return False
            except asyncio.TimeoutError:
                logger.warning("Timeout storing transcript")
                return False
            except Exception as e:
                logger.error(f"Error storing transcript: {e}")
                return False
                    
    except Exception as e:
        logger.error(f"âŒ Database storage error: {e}")
        return False


async def query_room_by_name(room_name: str) -> Optional[Dict[str, Any]]:
    """
    Query room information by LiveKit room name.
    
    Args:
        room_name: The LiveKit room name
        
    Returns:
        Room data dictionary or None if not found
    """
    try:
        session = await _pool.get_session()
        async with get_db_headers() as headers:
            url = f"{config.supabase.url}/rest/v1/rooms"
            params = {"Livekit_room_name": f"eq.{room_name}"}
            
            timeout = aiohttp.ClientTimeout(total=config.supabase.http_timeout)
            
            try:
                async with session.get(url, headers=headers, params=params, timeout=timeout) as response:
                    if response.status == 200:
                        rooms = await response.json()
                        if rooms and len(rooms) > 0:
                            return rooms[0]
                    else:
                        error_text = await response.text()
                        logger.warning(f"Failed to query room: {response.status} - {error_text}")
            except asyncio.TimeoutError:
                logger.warning("Timeout querying room")
            except Exception as e:
                logger.error(f"Error querying room: {e}")
        
        return None
        
    except Exception as e:
        logger.error(f"âŒ Room query failed: {e}")
        return None


async def get_active_session_for_room(room_id: int) -> Optional[str]:
    """
    Get the active session ID for a room if one exists.
    
    Args:
        room_id: The room ID
        
    Returns:
        Session ID or None if no active session
    """
    try:
        session = await _pool.get_session()
        async with get_db_headers() as headers:
            url = f"{config.supabase.url}/rest/v1/room_sessions"
            params = {
                "room_id": f"eq.{room_id}",
                "status": "eq.active",
                "select": "id",
                "order": "started_at.desc",
                "limit": "1"
            }
            
            timeout = aiohttp.ClientTimeout(total=config.supabase.http_timeout)
            
            try:
                async with session.get(url, headers=headers, params=params, timeout=timeout) as response:
                    if response.status == 200:
                        sessions = await response.json()
                        if sessions and len(sessions) > 0:
                            return sessions[0].get("id")
            except asyncio.TimeoutError:
                logger.warning("Timeout getting active session")
            except Exception as e:
                logger.error(f"Error getting active session: {e}")
        
        return None
        
    except Exception as e:
        logger.error(f"âŒ Active session query failed: {e}")
        return None


async def broadcast_to_channel(
    channel_name: str,
    event_type: str,
    payload: Dict[str, Any]
) -> bool:
    """
    Broadcast a message to a Supabase channel.
    
    Args:
        channel_name: The channel to broadcast to
        event_type: The event type (e.g., "transcription", "translation")
        payload: The data to broadcast
        
    Returns:
        bool: True if successful, False otherwise
    """
    try:
        if not config.supabase.service_role_key:
            logger.warning("âš ï¸ SUPABASE_SERVICE_ROLE_KEY not found - skipping broadcast")
            return False
            
        session = await _pool.get_session()
        
        async with get_db_headers() as headers:
            # Use broadcast-specific timeout
            broadcast_timeout = aiohttp.ClientTimeout(total=config.supabase.broadcast_timeout)
            
            try:
                async with session.post(
                    f"{config.supabase.url}/functions/v1/broadcast",
                    json={
                        "channel": channel_name,
                        "event": event_type,
                        "payload": payload
                    },
                    headers=headers,
                    timeout=broadcast_timeout
                ) as response:
                    if response.status == 200:
                        return True
                    else:
                        error_text = await response.text()
                        logger.warning(f"âš ï¸ Broadcast failed: {response.status} - {error_text}")
                        return False
            except asyncio.TimeoutError:
                logger.warning(f"âš ï¸ Broadcast timeout for channel {channel_name}")
                return False
            except Exception as e:
                logger.error(f"Error broadcasting: {e}")
                return False
                    
    except Exception as e:
        logger.error(f"âŒ Broadcast error: {e}")
        return False


async def query_prompt_template_for_room(room_id: int) -> Optional[Dict[str, Any]]:
    """
    Query the prompt template for a specific room.
    
    Args:
        room_id: The room ID
        
    Returns:
        Template data dictionary or None if not found
    """
    try:
        session = await _pool.get_session()
        async with get_db_headers() as headers:
            # Use the database function to get the appropriate template
            url = f"{config.supabase.url}/rest/v1/rpc/get_room_prompt_template"
            data = {"room_id": room_id}
            
            timeout = aiohttp.ClientTimeout(total=config.supabase.http_timeout)
            
            try:
                async with session.post(url, headers=headers, json=data, timeout=timeout) as response:
                    if response.status == 200:
                        result = await response.json()
                        if result and len(result) > 0:
                            template = result[0]
                            # Parse template_variables if it's a string
                            if isinstance(template.get('template_variables'), str):
                                try:
                                    import json
                                    template['template_variables'] = json.loads(template['template_variables'])
                                except:
                                    template['template_variables'] = {}
                            return template
                    else:
                        error_text = await response.text()
                        logger.warning(f"Failed to query prompt template: {response.status} - {error_text}")
            except asyncio.TimeoutError:
                logger.warning("Timeout querying prompt template")
            except Exception as e:
                logger.error(f"Error querying prompt template: {e}")
        
        return None
        
    except Exception as e:
        logger.error(f"âŒ Prompt template query failed: {e}")
        return None


async def update_session_heartbeat(session_id: str) -> bool:
    """
    Update the last_active timestamp for a session to prevent it from being cleaned up.
    
    Args:
        session_id: The session ID to update
        
    Returns:
        True if successful, False otherwise
    """
    if not session_id:
        return False
        
    try:
        session = await _pool.get_session()
        async with get_db_headers() as headers:
            # Update the last_active timestamp
            url = f"{config.supabase.url}/rest/v1/room_sessions"
            params = {"id": f"eq.{session_id}"}
            data = {"last_active": datetime.utcnow().isoformat()}
            
            timeout = aiohttp.ClientTimeout(total=config.supabase.http_timeout)
            
            try:
                async with session.patch(url, headers=headers, params=params, json=data, timeout=timeout) as response:
                    if response.status in [200, 204]:
                        logger.debug(f"ðŸ’“ Session heartbeat updated for {session_id}")
                        return True
                    else:
                        error_text = await response.text()
                        logger.warning(f"Failed to update session heartbeat: {response.status} - {error_text}")
                        return False
            except asyncio.TimeoutError:
                logger.warning(f"Timeout updating session heartbeat {session_id}")
                return False
            except Exception as e:
                logger.error(f"Error updating session heartbeat {session_id}: {e}")
                return False
                
    except Exception as e:
        logger.error(f"âŒ Failed to update session heartbeat {session_id}: {e}")
        return False


async def close_room_session(session_id: str) -> bool:
    """
    Close a room session by marking it as completed in the database.
    
    Args:
        session_id: The session ID to close
        
    Returns:
        True if successful, False otherwise
    """
    if not session_id:
        logger.warning("No session_id provided to close_room_session")
        return False
        
    try:
        session = await _pool.get_session()
        async with get_db_headers() as headers:
            # Call the cleanup_session_idempotent function
            url = f"{config.supabase.url}/rest/v1/rpc/cleanup_session_idempotent"
            data = {
                "p_session_id": session_id,
                "p_source": "agent_disconnect"
            }
            
            timeout = aiohttp.ClientTimeout(total=config.supabase.http_timeout)
            
            try:
                async with session.post(url, headers=headers, json=data, timeout=timeout) as response:
                    if response.status == 200:
                        result = await response.json()
                        logger.info(f"âœ… Session {session_id} closed successfully")
                        return True
                    else:
                        error_text = await response.text()
                        logger.error(f"Failed to close session: {response.status} - {error_text}")
                        return False
            except asyncio.TimeoutError:
                logger.warning(f"Timeout closing session {session_id}")
                return False
            except Exception as e:
                logger.error(f"Error closing session {session_id}: {e}")
                return False
                
    except Exception as e:
        logger.error(f"âŒ Failed to close session {session_id}: {e}")
        return False


async def close_database_connections():
    """Close all database connections. Call this on shutdown."""
    await _pool.close()
    logger.info("âœ… Database connections closed")


================================================
FILE: backup_ghost_fix_20250728_034924/main.py
================================================
import asyncio
import logging
import json
import time
import re
import os
import uuid
from typing import Set, Any, Dict, Optional
from collections import defaultdict, deque
from datetime import datetime, timedelta

from enum import Enum
from dataclasses import dataclass, asdict

from livekit import rtc
from livekit.agents import (
    AutoSubscribe,
    JobContext,
    JobProcess,
    JobRequest,
    WorkerOptions,
    cli,
    stt,
    utils,
)
from livekit.plugins import silero, speechmatics
from livekit.plugins.speechmatics.types import TranscriptionConfig

# Import configuration
from config import get_config, ApplicationConfig

# Import database operations
from database import (
    ensure_active_session,
    store_transcript_in_database,
    query_room_by_name,
    get_active_session_for_room,
    broadcast_to_channel,
    close_database_connections
)

# Import text processing and translation helpers
from text_processing import extract_complete_sentences
from translation_helpers import translate_sentences

# Import Translator class
from translator import Translator

# Import broadcasting function
from broadcasting import broadcast_to_displays

# Import resource management
from resource_management import ResourceManager, TaskManager, STTStreamManager

# Import webhook handler for room context
try:
    from webhook_handler import get_room_context as get_webhook_room_context
except ImportError:
    # Webhook handler not available, use empty context
    def get_webhook_room_context(room_name: str):
        return {}


# Load configuration
config = get_config()

logger = logging.getLogger("transcriber")


@dataclass
class Language:
    code: str
    name: str
    flag: str


# Build languages dictionary from config
languages = {}
for code, lang_info in config.translation.supported_languages.items():
    languages[code] = Language(
        code=code,
        name=lang_info["name"],
        flag=lang_info["flag"]
    )

LanguageCode = Enum(
    "LanguageCode",  # Name of the Enum
    {lang.name: code for code, lang in languages.items()},  # Enum entries: name -> code mapping
)


# Translator class has been moved to translator.py


def prewarm(proc: JobProcess):
    proc.userdata["vad"] = silero.VAD.load()


async def entrypoint(job: JobContext):
    # Configure source language - ARABIC as default
    # This will be the language that users are actually speaking (host/speaker language)
    source_language = config.translation.default_source_language
    
    # Initialize resource manager
    resource_manager = ResourceManager()
    
    # Register heartbeat timeout callback
    async def on_participant_timeout(participant_id: str):
        logger.warning(f"ðŸ’” Participant {participant_id} timed out - initiating cleanup")
        # Could trigger cleanup here if needed
    
    resource_manager.heartbeat_monitor.register_callback(on_participant_timeout)
    
    # Extract tenant context from room metadata or webhook handler
    tenant_context = {}
    try:
        # Try to query Supabase directly for room information
        if job.room and job.room.name:
            logger.info(f"ðŸ” Looking up room context for: {job.room.name}")
            
            # Check if database is configured
            logger.info(f"ðŸ”‘ Supabase URL: {config.supabase.url}")
            logger.info(f"ðŸ”‘ Supabase key available: {'Yes' if config.supabase.service_role_key else 'No'}")
            
            if config.supabase.service_role_key:
                try:
                    # Query room by LiveKit room name using the new database module
                    logger.info(f"ðŸ” Querying database for room: {job.room.name}")
                    # Query room directly without task wrapper
                    room_data = await query_room_by_name(job.room.name)
                    
                    if room_data:
                        tenant_context = {
                            "room_id": room_data.get("id"),
                            "mosque_id": room_data.get("mosque_id"),
                            "room_title": room_data.get("Title"),
                            "transcription_language": room_data.get("transcription_language", "ar"),
                            "translation_language": room_data.get("translation__language", "nl"),
                            "created_at": room_data.get("created_at")
                        }
                        # Also store the double underscore version for compatibility
                        if room_data.get("translation__language"):
                            tenant_context["translation__language"] = room_data.get("translation__language")
                        
                        logger.info(f"âœ… Found room in database: room_id={tenant_context.get('room_id')}, mosque_id={tenant_context.get('mosque_id')}")
                        logger.info(f"ðŸ—£ï¸ Languages: transcription={tenant_context.get('transcription_language')}, translation={tenant_context.get('translation_language')} (or {tenant_context.get('translation__language')})")
                        
                        # Try to get active session for this room
                        session_id = await get_active_session_for_room(tenant_context['room_id'])
                        if session_id:
                            tenant_context["session_id"] = session_id
                            logger.info(f"ðŸ“ Found active session: {tenant_context['session_id']}")
                except Exception as e:
                    logger.warning(f"âš ï¸ Could not query Supabase: {e}")
        
        # Fallback to webhook handler if available
        if not tenant_context:
            webhook_context = get_webhook_room_context(job.room.name if job.room else "")
            if webhook_context:
                tenant_context = {
                    "room_id": webhook_context.get("room_id"),
                    "mosque_id": webhook_context.get("mosque_id"),
                    "session_id": webhook_context.get("session_id"),
                    "room_title": webhook_context.get("room_title"),
                    "transcription_language": webhook_context.get("transcription_language", "ar"),
                    "translation_language": webhook_context.get("translation_language", "nl"),
                    "created_at": webhook_context.get("created_at")
                }
                logger.info(f"ðŸ¢ Tenant context from webhook handler: mosque_id={tenant_context.get('mosque_id')}, room_id={tenant_context.get('room_id')}")
        
        # Fallback to room metadata if available
        if not tenant_context and job.room and job.room.metadata:
            try:
                metadata = json.loads(job.room.metadata)
                tenant_context = {
                    "room_id": metadata.get("room_id"),
                    "mosque_id": metadata.get("mosque_id"),
                    "session_id": metadata.get("session_id"),
                    "room_title": metadata.get("room_title"),
                    "transcription_language": metadata.get("transcription_language", "ar"),
                    "translation_language": metadata.get("translation_language", "nl"),
                    "created_at": metadata.get("created_at")
                }
                logger.info(f"ðŸ¢ Tenant context from room metadata: mosque_id={tenant_context.get('mosque_id')}, room_id={tenant_context.get('room_id')}")
            except:
                pass
        
        # Final fallback to default context with hardcoded values for testing
        if not tenant_context:
            logger.warning(f"âš ï¸ No tenant context available for room: {job.room.name if job.room else 'unknown'}")
            # TEMPORARY: Use hardcoded values for mosque_546012 rooms
            if job.room and f"mosque_{config.test_mosque_id}" in job.room.name:
                tenant_context = {
                    "room_id": config.test_room_id,
                    "mosque_id": config.test_mosque_id,
                    "session_id": None,
                    "transcription_language": "ar",
                    "translation_language": "nl"
                }
                logger.info(f"ðŸ”§ Using hardcoded tenant context for testing: mosque_id={tenant_context['mosque_id']}, room_id={tenant_context['room_id']}")
            else:
                tenant_context = {
                    "room_id": None,
                    "mosque_id": int(os.getenv('DEFAULT_MOSQUE_ID', str(config.default_mosque_id))),
                    "session_id": None,
                    "transcription_language": "ar",
                    "translation_language": "nl"
                }
    except Exception as e:
        logger.warning(f"âš ï¸ Could not extract tenant context: {e}")
    
    # Configure Speechmatics STT with room-specific settings
    # Use tenant_context which already has room configuration
    room_config = None
    if tenant_context and tenant_context.get('room_id'):
        # We already have the room data in tenant_context from earlier query
        room_config = tenant_context
        logger.info(f"ðŸ“‹ Using room-specific configuration from context: "
                  f"lang={room_config.get('transcription_language', 'ar')}, "
                  f"target={room_config.get('translation_language', 'nl')}, "
                  f"delay={room_config.get('max_delay', 2.0)}, "
                  f"punct={room_config.get('punctuation_sensitivity', 0.5)}")
        
        # If we need full room data and it's not in context, query it
        if not room_config.get('max_delay'):
            try:
                full_room_data = await query_room_by_name(job.room.name if job.room else None)
                if full_room_data:
                    # Merge the full room data with tenant context
                    room_config.update({
                        'max_delay': full_room_data.get('max_delay'),
                        'punctuation_sensitivity': full_room_data.get('punctuation_sensitivity'),
                        'translation__language': full_room_data.get('translation__language')
                    })
                    logger.info(f"ðŸ“‹ Fetched additional room config: delay={room_config.get('max_delay')}, punct={room_config.get('punctuation_sensitivity')}")
            except Exception as e:
                logger.warning(f"Failed to fetch additional room config: {e}")
    
    # Create STT configuration with room-specific overrides
    stt_config = config.speechmatics.with_room_settings(room_config)
    
    # Initialize STT provider with configured settings
    stt_provider = speechmatics.STT(
        transcription_config=TranscriptionConfig(
            language=stt_config.language,
            operating_point=stt_config.operating_point,
            enable_partials=stt_config.enable_partials,
            max_delay=stt_config.max_delay,
            punctuation_overrides={"sensitivity": stt_config.punctuation_sensitivity},
            diarization=stt_config.diarization
        )
    )
    
    # Update source language based on room config
    source_language = config.translation.get_source_language(room_config)
    logger.info(f"ðŸ—£ï¸ STT configured for {languages[source_language].name} speech recognition")
    
    translators = {}
    
    # Get target language from room config or use default
    target_language = config.translation.get_target_language(room_config)
    logger.info(f"ðŸŽ¯ Target language resolved to: '{target_language}' (from room_config: {room_config.get('translation_language') if room_config else 'None'} or {room_config.get('translation__language') if room_config else 'None'})")
    
    # Create translator for the configured target language
    if target_language in languages:
        # Get language enum dynamically
        lang_info = languages[target_language]
        lang_enum = getattr(LanguageCode, lang_info.name)
        translators[target_language] = Translator(job.room, lang_enum, tenant_context, broadcast_to_displays)
        logger.info(f"ðŸ“ Initialized {lang_info.name} translator ({target_language})")
    else:
        logger.warning(f"âš ï¸ Target language '{target_language}' not supported, falling back to Dutch")
        dutch_enum = getattr(LanguageCode, 'Dutch')
        translators["nl"] = Translator(job.room, dutch_enum, tenant_context, broadcast_to_displays)
    
    # Sentence accumulation for proper sentence-by-sentence translation
    accumulated_text = ""  # Accumulates text until we get a complete sentence
    last_final_transcript = ""  # Keep track of the last final transcript to avoid duplicates
    current_sentence_id = None  # Track current sentence being built
    
    logger.info(f"ðŸš€ Starting entrypoint for room: {job.room.name if job.room else 'unknown'}")
    logger.info(f"ðŸ” Translators dict ID: {id(translators)}")
    logger.info(f"ðŸŽ¯ Configuration: {languages[source_language].name} â†’ {languages.get(target_language, languages['nl']).name}")
    logger.info(f"âš™ï¸ STT Settings: delay={stt_config.max_delay}s, punctuation={stt_config.punctuation_sensitivity}")

    async def _forward_transcription(
        stt_stream: stt.SpeechStream,
        track: rtc.Track,
    ):
        """Forward the transcription and log the transcript in the console"""
        nonlocal accumulated_text, last_final_transcript, current_sentence_id
        
        try:
            async for ev in stt_stream:
                # Log to console for interim (word-by-word)
                if ev.type == stt.SpeechEventType.INTERIM_TRANSCRIPT:
                    print(ev.alternatives[0].text, end="", flush=True)
                    
                    # Publish interim transcription for real-time word-by-word display
                    interim_text = ev.alternatives[0].text.strip()
                    if interim_text:
                        try:
                            interim_segment = rtc.TranscriptionSegment(
                                id=utils.misc.shortuuid("SG_"),
                                text=interim_text,
                                start_time=0,
                                end_time=0,
                                language=source_language,  # Arabic
                                final=False,  # This is interim, not final
                            )
                            interim_transcription = rtc.Transcription(
                                job.room.local_participant.identity, "", [interim_segment]
                            )
                            await job.room.local_participant.publish_transcription(interim_transcription)
                        except Exception as e:
                            logger.debug(f"Failed to publish interim transcription: {str(e)}")
                    
                elif ev.type == stt.SpeechEventType.FINAL_TRANSCRIPT:
                    print("\n")
                    final_text = ev.alternatives[0].text.strip()
                    print(" -> ", final_text)
                    logger.info(f"Final Arabic transcript: {final_text}")

                    if final_text and final_text != last_final_transcript:
                        last_final_transcript = final_text
                        
                        # Publish final transcription for the original language (Arabic)
                        try:
                            final_segment = rtc.TranscriptionSegment(
                                id=utils.misc.shortuuid("SG_"),
                                text=final_text,
                                start_time=0,
                                end_time=0,
                                language=source_language,  # Arabic
                                final=True,
                            )
                            final_transcription = rtc.Transcription(
                                job.room.local_participant.identity, "", [final_segment]
                            )
                            await job.room.local_participant.publish_transcription(final_transcription)
                            
                            logger.info(f"âœ… Published final {languages[source_language].name} transcription: '{final_text}'")
                        except Exception as e:
                            logger.error(f"âŒ Failed to publish final transcription: {str(e)}")
                        
                        # Generate sentence ID if we don't have one for this sentence
                        if not current_sentence_id:
                            current_sentence_id = str(uuid.uuid4())
                        
                        # Broadcast final transcription text for real-time display
                        print(f"ðŸ“¡ Broadcasting Arabic text to frontend: '{final_text}'")
                        # Broadcast directly without task wrapper
                        await broadcast_to_displays(
                            "transcription", 
                            source_language, 
                            final_text, 
                            tenant_context,
                            sentence_context={
                                "sentence_id": current_sentence_id,
                                "is_complete": False,  # Will be marked complete when sentence ends
                                "is_fragment": True
                            }
                        )
                        
                        # Handle translation logic
                        if translators:
                            # SIMPLE ACCUMULATION LOGIC - ONLY APPEND, NEVER REPLACE
                            if accumulated_text:
                                # ALWAYS append new final transcript to existing accumulated text
                                accumulated_text = accumulated_text.strip() + " " + final_text
                            else:
                                # First transcript - start accumulation
                                accumulated_text = final_text
                            
                            logger.info(f"ðŸ“ Updated accumulated Arabic text: '{accumulated_text}'")
                            
                            # Extract complete sentences from accumulated text
                            complete_sentences, remaining_text = extract_complete_sentences(accumulated_text)
                            
                            # Handle special punctuation completion signal
                            if complete_sentences and complete_sentences[0] == "PUNCTUATION_COMPLETE":
                                if accumulated_text.strip():
                                    # Complete the accumulated sentence with this punctuation
                                    print(f"ðŸ“ PUNCTUATION SIGNAL: Completing accumulated text: '{accumulated_text}'")
                                    
                                    # Broadcast sentence completion
                                    if current_sentence_id:
                                        await broadcast_to_displays(
                                            "transcription", 
                                            source_language, 
                                            accumulated_text, 
                                            tenant_context,
                                            sentence_context={
                                                "sentence_id": current_sentence_id,
                                                "is_complete": True,
                                                "is_fragment": False
                                            }
                                        )
                                    
                                    # Translate the completed sentence (don't include the punctuation marker)
                                    await translate_sentences([accumulated_text], translators, source_language, current_sentence_id)
                                    
                                    # Clear accumulated text as sentence is now complete
                                    accumulated_text = ""
                                    current_sentence_id = None
                                    print(f"ðŸ“ Cleared accumulated text after punctuation completion")
                                else:
                                    print(f"âš ï¸ Received punctuation completion signal but no accumulated text")
                            elif complete_sentences:
                                # We have complete sentences - translate them immediately
                                print(f"ðŸŽ¯ Found {len(complete_sentences)} complete Arabic sentences: {complete_sentences}")
                                
                                # Broadcast each complete sentence
                                for sentence in complete_sentences:
                                    sentence_id = current_sentence_id if len(complete_sentences) == 1 else str(uuid.uuid4())
                                    await broadcast_to_displays(
                                        "transcription", 
                                        source_language, 
                                        sentence, 
                                        tenant_context,
                                        sentence_context={
                                            "sentence_id": sentence_id,
                                            "is_complete": True,
                                            "is_fragment": False
                                        }
                                    )
                                    # Translate complete sentences with sentence ID
                                    await translate_sentences([sentence], translators, source_language, sentence_id)
                                
                                # Update accumulated text to only remaining incomplete text
                                accumulated_text = remaining_text
                                # Generate new sentence ID for the remaining text
                                current_sentence_id = str(uuid.uuid4()) if remaining_text else None
                                print(f"ðŸ“ Updated accumulated Arabic text after sentence extraction: '{accumulated_text}'")
                            
                            # Log remaining incomplete text (no delayed translation)
                            if accumulated_text.strip():
                                logger.info(f"ðŸ“ Incomplete Arabic text remaining: '{accumulated_text}'")
                                # Note: Incomplete text will be translated when the next sentence completes
                        else:
                            logger.warning(f"âš ï¸ No translators available in room {job.room.name}, only {languages[source_language].name} transcription published")
                    else:
                        logger.debug("Empty or duplicate transcription, skipping")
        except Exception as e:
            logger.error(f"STT transcription error: {str(e)}")
            raise

    async def transcribe_track(participant: rtc.RemoteParticipant, track: rtc.Track):
        logger.info(f"ðŸŽ¤ Starting Arabic transcription for participant {participant.identity}, track {track.sid}")
        
        try:
            audio_stream = rtc.AudioStream(track)
            
            # Use context manager for STT stream
            async with resource_manager.stt_manager.create_stream(stt_provider, participant.identity) as stt_stream:
                # Create transcription task with tracking
                stt_task = resource_manager.task_manager.create_task(
                    _forward_transcription(stt_stream, track),
                    name=f"transcribe-{participant.identity}",
                    metadata={"participant": participant.identity, "track": track.sid}
                )
                
                frame_count = 0
                async for ev in audio_stream:
                    frame_count += 1
                    if frame_count % 100 == 0:  # Log every 100 frames to avoid spam
                        logger.debug(f"ðŸ”Š Received audio frame #{frame_count} from {participant.identity}")
                        # Update heartbeat every 100 frames
                        await resource_manager.heartbeat_monitor.update_heartbeat(
                            participant.identity, 
                            tenant_context.get('session_id')
                        )
                    stt_stream.push_frame(ev.frame)
                    
                logger.warning(f"ðŸ”‡ Audio stream ended for {participant.identity}")
                
                # Cancel the transcription task if still running
                if not stt_task.done():
                    stt_task.cancel()
                    try:
                        await stt_task
                    except asyncio.CancelledError:
                        logger.debug(f"STT task cancelled for {participant.identity}")
                        
        except Exception as e:
            logger.error(f"âŒ Transcription track error for {participant.identity}: {str(e)}")
        
        logger.info(f"ðŸ§¹ Transcription cleanup completed for {participant.identity}")

    @job.room.on("track_subscribed")
    def on_track_subscribed(
        track: rtc.Track,
        publication: rtc.TrackPublication,
        participant: rtc.RemoteParticipant,
    ):
        logger.info(f"ðŸŽµ Track subscribed: {track.kind} from {participant.identity} (track: {track.sid})")
        logger.info(f"Track details - muted: {publication.muted}")
        if track.kind == rtc.TrackKind.KIND_AUDIO:
            logger.info(f"âœ… Adding Arabic transcriber for participant: {participant.identity}")
            resource_manager.task_manager.create_task(
                transcribe_track(participant, track),
                name=f"track-handler-{participant.identity}",
                metadata={"participant": participant.identity, "track": track.sid}
            )
        else:
            logger.info(f"âŒ Ignoring non-audio track: {track.kind}")

    @job.room.on("track_published")
    def on_track_published(publication: rtc.TrackPublication, participant: rtc.RemoteParticipant):
        logger.info(f"ðŸ“¡ Track published: {publication.kind} from {participant.identity} (track: {publication.sid})")
        logger.info(f"Publication details - muted: {publication.muted}")

    @job.room.on("track_unpublished") 
    def on_track_unpublished(publication: rtc.TrackPublication, participant: rtc.RemoteParticipant):
        logger.info(f"ðŸ“¡ Track unpublished: {publication.kind} from {participant.identity}")

    @job.room.on("participant_connected")
    def on_participant_connected(participant: rtc.RemoteParticipant):
        logger.info(f"ðŸ‘¥ Participant connected: {participant.identity}")
        
        # Try to extract metadata from participant if available
        if hasattr(participant, 'metadata') and participant.metadata:
            try:
                participant_metadata = json.loads(participant.metadata)
                if participant_metadata:
                    # Update tenant context with participant metadata
                    tenant_context.update({
                        "room_id": participant_metadata.get("room_id", tenant_context.get("room_id")),
                        "mosque_id": participant_metadata.get("mosque_id", tenant_context.get("mosque_id")),
                        "session_id": participant_metadata.get("session_id", tenant_context.get("session_id")),
                        "room_title": participant_metadata.get("room_title", tenant_context.get("room_title"))
                    })
                    logger.info(f"ðŸ“‹ Updated tenant context from participant metadata: {tenant_context}")
                    
                    # Update all translators with new context
                    for translator in translators.values():
                        translator.tenant_context = tenant_context
            except Exception as e:
                logger.debug(f"Could not parse participant metadata: {e}")

    @job.room.on("participant_disconnected")
    def on_participant_disconnected(participant: rtc.RemoteParticipant):
        logger.info(f"ðŸ‘¥ Participant disconnected: {participant.identity}")
        
        # Remove from heartbeat monitoring
        resource_manager.heartbeat_monitor.remove_participant(participant.identity)
        
        # Resource cleanup is now handled by ResourceManager
        # Log current resource statistics
        resource_manager.log_stats()
        logger.info(f"ðŸ§¹ Participant cleanup completed for {participant.identity}")

    @job.room.on("participant_attributes_changed")
    def on_attributes_changed(
        changed_attributes: dict[str, str], participant: rtc.Participant
    ):
        """
        When participant attributes change, handle new translation requests.
        """
        logger.info(f"ðŸŒ Participant {participant.identity} attributes changed: {changed_attributes}")
        lang = changed_attributes.get("captions_language", None)
        if lang:
            if lang == source_language:
                logger.info(f"âœ… Participant {participant.identity} requested {languages[source_language].name} (source language - Arabic)")
            elif lang in translators:
                logger.info(f"âœ… Participant {participant.identity} requested existing language: {lang}")
                logger.info(f"ðŸ“Š Current translators for this room: {list(translators.keys())}")
            else:
                # Check if the language is supported and different from source language
                if lang in languages:
                    try:
                        # Create a translator for the requested language using the language enum
                        language_obj = languages[lang]
                        language_enum = getattr(LanguageCode, language_obj.name)
                        translators[lang] = Translator(job.room, language_enum, tenant_context, broadcast_to_displays)
                        logger.info(f"ðŸ†• Added translator for ROOM {job.room.name} (requested by {participant.identity}), language: {language_obj.name}")
                        logger.info(f"ðŸ¢ Translator created with tenant context: mosque_id={tenant_context.get('mosque_id')}")
                        logger.info(f"ðŸ“Š Total translators for room {job.room.name}: {len(translators)} -> {list(translators.keys())}")
                        logger.info(f"ðŸ” Translators dict ID: {id(translators)}")
                        
                        # Debug: Verify the translator was actually added
                        if lang in translators:
                            logger.info(f"âœ… Translator verification: {lang} successfully added to room translators")
                        else:
                            logger.error(f"âŒ Translator verification FAILED: {lang} not found in translators dict")
                            
                    except Exception as e:
                        logger.error(f"âŒ Error creating translator for {lang}: {str(e)}")
                else:
                    logger.warning(f"âŒ Unsupported language requested by {participant.identity}: {lang}")
                    logger.info(f"ðŸ’¡ Supported languages: {list(languages.keys())}")
        else:
            logger.debug(f"No caption language change for participant {participant.identity}")

    logger.info("Connecting to room...")
    await job.connect(auto_subscribe=AutoSubscribe.AUDIO_ONLY)
    logger.info(f"Successfully connected to room: {job.room.name}")
    logger.info(f"ðŸ“¡ Real-time transcription data will be sent via Supabase Broadcast")
    
    # Debug room state after connection
    logger.info(f"Room participants: {len(job.room.remote_participants)}")
    for participant in job.room.remote_participants.values():
        logger.info(f"Participant: {participant.identity}")
        logger.info(f"  Audio tracks: {len(participant.track_publications)}")
        for sid, pub in participant.track_publications.items():
            logger.info(f"    Track {sid}: {pub.kind}, muted: {pub.muted}")

    # Also check local participant
    logger.info(f"Local participant: {job.room.local_participant.identity}")
    logger.info(f"Local participant tracks: {len(job.room.local_participant.track_publications)}")

    @job.room.local_participant.register_rpc_method("get/languages")
    async def get_languages(data: rtc.RpcInvocationData):
        languages_list = [asdict(lang) for lang in languages.values()]
        return json.dumps(languages_list)
    
    @job.room.local_participant.register_rpc_method("request/cleanup")
    async def request_cleanup(data: rtc.RpcInvocationData):
        """Handle cleanup request from frontend"""
        try:
            payload = json.loads(data.payload)
            reason = payload.get('reason', 'unknown')
            session_id = payload.get('session_id')
            
            logger.info(f"ðŸ§¹ Cleanup requested by frontend: reason={reason}, session_id={session_id}")
            
            # Initiate graceful shutdown
            asyncio.create_task(perform_graceful_cleanup(reason, session_id))
            
            return json.dumps({
                "success": True,
                "message": "Cleanup initiated"
            })
        except Exception as e:
            logger.error(f"Error handling cleanup request: {e}")
            return json.dumps({
                "success": False,
                "error": str(e)
            })
    
    async def perform_graceful_cleanup(reason: str, session_id: Optional[str]):
        """Perform graceful cleanup when requested by frontend"""
        logger.info(f"ðŸ›‘ Starting graceful cleanup: {reason}")
        
        # Log current resource state
        resource_manager.log_stats()
        
        # If session_id provided, update it in database
        if session_id and tenant_context.get('room_id'):
            try:
                from database import query_database
                result = await query_database(
                    "SELECT cleanup_session_idempotent(%s, %s, %s)",
                    [session_id, f"frontend_{reason}", datetime.utcnow()]
                )
                logger.info(f"Session cleanup result: {result}")
            except Exception as e:
                logger.error(f"Error updating session: {e}")
        
        # Shutdown all resources
        await resource_manager.shutdown()
        
        # Verify cleanup is complete
        verification = await resource_manager.verify_cleanup_complete()
        logger.info(f"ðŸ” Cleanup verification: {verification}")
        
        # Disconnect from room (this will trigger on_room_disconnected)
        await job.room.disconnect()
        
        logger.info("âœ… Graceful cleanup completed")

    @job.room.on("disconnected")
    def on_room_disconnected():
        """Handle room disconnection - cleanup all resources"""
        logger.info("ðŸšª Room disconnected, starting cleanup...")
        
        # Create async task for cleanup
        async def cleanup():
            # Log final resource statistics
            resource_manager.log_stats()
            
            # Shutdown resource manager (cancels all tasks, closes all streams)
            await resource_manager.shutdown()
            
            # Close database connections
            try:
                await close_database_connections()
                logger.info("âœ… Database connections closed")
                
                # Force cleanup of any remaining sessions
                import gc
                gc.collect()  # Force garbage collection
                await asyncio.sleep(0.1)  # Give time for cleanup
            except Exception as e:
                logger.debug(f"Database cleanup error: {e}")
            
            # Final verification
            verification = await resource_manager.verify_cleanup_complete()
            logger.info(f"ðŸ” Final cleanup verification: {verification}")
            
            logger.info("âœ… Room cleanup completed")
        
        # Run cleanup in the event loop
        asyncio.create_task(cleanup())


async def request_fnc(req: JobRequest):
    logger.info(f"ðŸŽ¯ Received job request for room: {req.room.name if req.room else 'unknown'}")
    logger.info(f"ðŸ“‹ Request details: job_id={req.id}, room_name={req.room.name if req.room else 'unknown'}")
    await req.accept(
        name="agent",
        identity="agent",
    )
    logger.info(f"âœ… Accepted job request for room: {req.room.name if req.room else 'unknown'}")


if __name__ == "__main__":
    cli.run_app(
        WorkerOptions(
            entrypoint_fnc=entrypoint, prewarm_fnc=prewarm, request_fnc=request_fnc
        )
    )


================================================
FILE: backup_ghost_fix_20250728_034924/requirements.txt
================================================
# LiveKit Core Dependencies
livekit-agents>=1.0.0
livekit-plugins-openai>=0.8.0
livekit-plugins-speechmatics>=0.6.0
livekit-plugins-silero>=0.6.0

# AI/ML Dependencies
openai>=1.0.0

# Web Framework (for health checks)
fastapi>=0.104.0
uvicorn[standard]>=0.24.0

# HTTP Client
aiohttp>=3.8.0

# Database
asyncpg>=0.29.0

# Environment & Configuration
python-dotenv>=1.0.0

# Logging & Monitoring
# structlog>=23.0.0  # Removed - using standard logging

# Production Dependencies
gunicorn>=21.0.0
psutil>=5.9.0

# Audio Processing
pyaudio>=0.2.11

# Async utilities
asyncio-throttle>=1.0.0


================================================
FILE: backup_sentence_context_20250728_031206/broadcasting.py
================================================
"""
Broadcasting module for LiveKit AI Translation Server.
Handles real-time broadcasting of transcriptions and translations to displays.
"""
import asyncio
import logging
import hashlib
import uuid
from typing import Optional, Dict, Any
from datetime import datetime

from config import get_config
from database import broadcast_to_channel, store_transcript_in_database

logger = logging.getLogger("transcriber.broadcasting")
config = get_config()


class BroadcastError(Exception):
    """Custom exception for broadcasting-related errors."""
    pass


async def broadcast_to_displays(
    message_type: str, 
    language: str, 
    text: str, 
    tenant_context: Optional[Dict[str, Any]] = None,
    sentence_context: Optional[Dict[str, Any]] = None
) -> bool:
    """
    Send transcription/translation to frontend via Supabase Broadcast and store in database.
    
    This function handles both real-time broadcasting and database storage of
    transcriptions and translations. It uses Supabase's broadcast feature for
    real-time updates and stores the data for persistence.
    
    Args:
        message_type: Type of message ("transcription" or "translation")
        language: Language code (e.g., "ar", "nl")
        text: The text content to broadcast
        tenant_context: Optional context containing room_id, mosque_id, etc.
        sentence_context: Optional context for sentence tracking (sentence_id, is_complete, etc.)
        
    Returns:
        bool: True if broadcast was successful, False otherwise
    """
    if not text or not text.strip():
        logger.debug("Empty text provided, skipping broadcast")
        return False
    
    success = False
    
    # Phase 1: Immediate broadcast via Supabase for real-time display
    if tenant_context and tenant_context.get("room_id") and tenant_context.get("mosque_id"):
        try:
            channel_name = f"live-transcription-{tenant_context['room_id']}-{tenant_context['mosque_id']}"
            
            # Generate unique message ID based on timestamp and content hash
            timestamp = datetime.utcnow().isoformat() + "Z"
            text_hash = hashlib.md5(text.encode('utf-8')).hexdigest()[:8]
            msg_id = f"{timestamp}_{text_hash}"
            
            # Build payload with optional sentence context
            data_payload = {
                "text": text,
                "language": language,
                "timestamp": timestamp,
                "msg_id": msg_id
            }
            
            # Add sentence context if provided
            if sentence_context:
                data_payload.update({
                    "sentence_id": sentence_context.get("sentence_id"),
                    "is_complete": sentence_context.get("is_complete", False),
                    "is_fragment": sentence_context.get("is_fragment", True)
                })
            
            payload = {
                "type": message_type,
                "room_id": tenant_context["room_id"],
                "mosque_id": tenant_context["mosque_id"],
                "data": data_payload
            }
            
            # Use the broadcast_to_channel function from database module
            success = await broadcast_to_channel(channel_name, message_type, payload)
            
            if success:
                logger.info(
                    f"ðŸ“¡ LIVE: Sent {message_type} ({language}) via Supabase broadcast: "
                    f"{text[:50]}{'...' if len(text) > 50 else ''}"
                )
            else:
                logger.warning(f"âš ï¸ Failed to broadcast {message_type} to Supabase")
                
        except Exception as e:
            logger.error(f"âŒ Broadcast error: {e}")
            success = False
    else:
        logger.warning("âš ï¸ Missing tenant context for Supabase broadcast")
    
    # Phase 2: Direct database storage (no batching)
    if tenant_context and tenant_context.get("room_id") and tenant_context.get("mosque_id"):
        try:
            # Store directly in database using existing function
            # Use create_task to avoid blocking the broadcast with proper error handling
            task = asyncio.create_task(
                _store_with_error_handling(message_type, language, text, tenant_context)
            )
            task.add_done_callback(lambda t: None if not t.exception() else logger.error(f"Storage task failed: {t.exception()}"))
            logger.debug(
                f"ðŸ’¾ DIRECT: Storing {message_type} directly to database "
                f"for room {tenant_context['room_id']}"
            )
        except Exception as e:
            logger.error(f"âŒ Failed to initiate database storage: {e}")
    else:
        logger.warning("âš ï¸ Missing tenant context for database storage")
    
    return success


async def _store_with_error_handling(
    message_type: str, 
    language: str, 
    text: str, 
    tenant_context: Dict[str, Any]
) -> None:
    """
    Store transcript with proper error handling.
    
    This is a wrapper around store_transcript_in_database that ensures
    errors don't propagate and crash the application.
    
    Args:
        message_type: Type of message ("transcription" or "translation")
        language: Language code
        text: The text content to store
        tenant_context: Context containing room_id, mosque_id, etc.
    """
    try:
        success = await store_transcript_in_database(
            message_type, language, text, tenant_context
        )
        if not success:
            logger.warning(
                f"âš ï¸ Failed to store {message_type} in database for "
                f"room {tenant_context.get('room_id')}"
            )
    except Exception as e:
        logger.error(
            f"âŒ Database storage error for {message_type}: {e}\n"
            f"Room: {tenant_context.get('room_id')}, "
            f"Language: {language}"
        )


async def broadcast_batch(
    messages: list[tuple[str, str, str, Dict[str, Any]]]
) -> Dict[str, int]:
    """
    Broadcast multiple messages in batch for efficiency.
    
    Args:
        messages: List of tuples (message_type, language, text, tenant_context)
        
    Returns:
        Dictionary with counts of successful and failed broadcasts
    """
    results = {"success": 0, "failed": 0}
    
    # Process all broadcasts concurrently
    tasks = []
    for message_type, language, text, tenant_context in messages:
        task = broadcast_to_displays(message_type, language, text, tenant_context)
        tasks.append(task)
    
    # Wait for all broadcasts to complete
    broadcast_results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # Count results
    for result in broadcast_results:
        if isinstance(result, Exception):
            results["failed"] += 1
            logger.error(f"Batch broadcast error: {result}")
        elif result:
            results["success"] += 1
        else:
            results["failed"] += 1
    
    logger.info(
        f"ðŸ“Š Batch broadcast complete: "
        f"{results['success']} successful, {results['failed']} failed"
    )
    
    return results


def create_broadcast_payload(
    message_type: str,
    language: str,
    text: str,
    room_id: int,
    mosque_id: int,
    additional_data: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """
    Create a standardized broadcast payload.
    
    Args:
        message_type: Type of message
        language: Language code
        text: The text content
        room_id: Room ID
        mosque_id: Mosque ID
        additional_data: Optional additional data to include
        
    Returns:
        Formatted payload dictionary
    """
    # Generate unique message ID
    timestamp = datetime.utcnow().isoformat() + "Z"
    text_hash = hashlib.md5(text.encode('utf-8')).hexdigest()[:8]
    msg_id = f"{timestamp}_{text_hash}"
    
    payload = {
        "type": message_type,
        "room_id": room_id,
        "mosque_id": mosque_id,
        "data": {
            "text": text,
            "language": language,
            "timestamp": timestamp,
            "msg_id": msg_id
        }
    }
    
    if additional_data:
        payload["data"].update(additional_data)
    
    return payload


def get_channel_name(room_id: int, mosque_id: int) -> str:
    """
    Generate the channel name for a room.
    
    Args:
        room_id: Room ID
        mosque_id: Mosque ID
        
    Returns:
        Channel name string
    """
    return f"live-transcription-{room_id}-{mosque_id}"


================================================
FILE: backup_sentence_context_20250728_031206/database.py
================================================
"""
Database operations for LiveKit AI Translation Server.
Handles all Supabase database interactions with connection pooling and async support.
FIXED: Thread-safe connection pool that works with LiveKit's multi-process architecture.
"""
import asyncio
import logging
import uuid
from typing import Optional, Dict, Any
from datetime import datetime
import aiohttp
from contextlib import asynccontextmanager
import threading

from config import get_config

logger = logging.getLogger("transcriber.database")
config = get_config()


class ThreadSafeDatabasePool:
    """Thread-safe database connection pool that creates separate pools per thread/process."""
    
    def __init__(self, max_connections: int = 10):
        self.max_connections = max_connections
        self._local = threading.local()
        self._lock = threading.Lock()
        
    async def get_session(self) -> aiohttp.ClientSession:
        """Get or create a session for the current thread."""
        # Check if current thread has a session
        if not hasattr(self._local, 'session') or self._local.session is None or self._local.session.closed:
            # Create new session for this thread
            connector = aiohttp.TCPConnector(
                limit=self.max_connections,
                limit_per_host=self.max_connections,
                force_close=True  # Force close to avoid connection issues
            )
            self._local.session = aiohttp.ClientSession(
                connector=connector,
                trust_env=True  # Trust environment proxy settings
            )
            logger.debug(f"Created new connection pool for thread {threading.current_thread().ident}")
        
        return self._local.session
    
    async def close(self):
        """Close the session for current thread."""
        if hasattr(self._local, 'session') and self._local.session and not self._local.session.closed:
            await self._local.session.close()
            self._local.session = None
            logger.debug(f"Closed connection pool for thread {threading.current_thread().ident}")


# Use thread-safe pool
_pool = ThreadSafeDatabasePool()


@asynccontextmanager
async def get_db_headers():
    """Get headers for Supabase API requests."""
    if not config.supabase.service_role_key:
        raise ValueError("SUPABASE_SERVICE_ROLE_KEY not configured")
    
    yield {
        'apikey': config.supabase.service_role_key,
        'Authorization': f'Bearer {config.supabase.service_role_key}',
        'Content-Type': 'application/json'
    }


async def ensure_active_session(room_id: int, mosque_id: int) -> Optional[str]:
    """
    Ensure there's an active session for the room and return session_id.
    
    This function:
    1. Checks for existing active sessions
    2. Creates a new session if none exists
    3. Returns the session ID or None on failure
    """
    try:
        # Get session from thread-safe pool
        session = await _pool.get_session()
        
        async with get_db_headers() as headers:
            # Check for existing active session
            url = f"{config.supabase.url}/rest/v1/room_sessions"
            params = {
                "room_id": f"eq.{room_id}",
                "status": "eq.active",
                "select": "id,started_at",
                "order": "started_at.desc",
                "limit": "1"
            }
            
            timeout = aiohttp.ClientTimeout(total=config.supabase.http_timeout)
            
            try:
                async with session.get(url, headers=headers, params=params, timeout=timeout) as response:
                    if response.status == 200:
                        sessions = await response.json()
                        if sessions and len(sessions) > 0:
                            session_id = sessions[0]["id"]
                            logger.debug(f"ðŸ“ Using existing active session: {session_id}")
                            return session_id
                    else:
                        error_text = await response.text()
                        logger.warning(f"Failed to check existing sessions: {response.status} - {error_text}")
            except asyncio.TimeoutError:
                logger.warning("Timeout checking for existing sessions")
            except Exception as e:
                logger.error(f"Error checking sessions: {e}")
            
            # Create new session if none exists
            new_session_id = str(uuid.uuid4())
            session_data = {
                "id": new_session_id,
                "room_id": room_id,
                "mosque_id": mosque_id,
                "status": "active",
                "started_at": datetime.utcnow().isoformat() + "Z",
                "logging_enabled": True
            }
            
            try:
                async with session.post(
                    url,
                    json=session_data,
                    headers={**headers, 'Prefer': 'return=minimal'},
                    timeout=timeout
                ) as response:
                    if response.status in [200, 201]:
                        logger.info(f"ðŸ“ Created new session: {new_session_id}")
                        return new_session_id
                    else:
                        error_text = await response.text()
                        logger.error(f"âŒ Failed to create session: {response.status} - {error_text}")
                        return None
            except asyncio.TimeoutError:
                logger.error("Timeout creating new session")
                return None
            except Exception as e:
                logger.error(f"Error creating session: {e}")
                return None
                    
    except Exception as e:
        logger.error(f"âŒ Session management failed: {e}")
        return None


async def store_transcript_in_database(
    message_type: str, 
    language: str, 
    text: str, 
    tenant_context: Dict[str, Any]
) -> bool:
    """
    Store transcription/translation in Supabase database.
    
    Args:
        message_type: Either "transcription" or "translation"
        language: Language code (e.g., "ar", "nl")
        text: The text to store
        tenant_context: Context containing room_id, mosque_id, session_id
        
    Returns:
        bool: True if successful, False otherwise
    """
    try:
        if not config.supabase.service_role_key:
            logger.error("âŒ SUPABASE_SERVICE_ROLE_KEY not found - cannot store transcripts")
            return False
            
        room_id = tenant_context.get("room_id")
        mosque_id = tenant_context.get("mosque_id")
        session_id = tenant_context.get("session_id")
        
        if not room_id or not mosque_id:
            logger.warning(f"âš ï¸ Missing room context: room_id={room_id}, mosque_id={mosque_id}")
            return False
            
        # Ensure we have an active session
        if not session_id:
            session_id = await ensure_active_session(room_id, mosque_id)
            if session_id:
                tenant_context["session_id"] = session_id
            else:
                logger.error("âŒ Could not establish session - skipping database storage")
                return False
        
        # Prepare transcript data
        transcript_data = {
            "room_id": room_id,
            "session_id": session_id,
            "timestamp": datetime.utcnow().isoformat() + "Z",
        }
        
        # Set appropriate field based on message type
        if message_type == "transcription":
            transcript_data["transcription_segment"] = text
        else:  # translation
            transcript_data["translation_segment"] = text
            
        # Store in database
        session = await _pool.get_session()
        async with get_db_headers() as headers:
            timeout = aiohttp.ClientTimeout(total=config.supabase.http_timeout)
            
            try:
                async with session.post(
                    f"{config.supabase.url}/rest/v1/transcripts",
                    json=transcript_data,
                    headers={**headers, 'Prefer': 'return=minimal'},
                    timeout=timeout
                ) as response:
                    if response.status in [200, 201]:
                        logger.debug(f"âœ… Stored {message_type} in database: room_id={room_id}, session_id={session_id[:8]}")
                        return True
                    else:
                        error_text = await response.text()
                        logger.warning(f"âš ï¸ Database storage failed with status {response.status}: {error_text}")
                        return False
            except asyncio.TimeoutError:
                logger.warning("Timeout storing transcript")
                return False
            except Exception as e:
                logger.error(f"Error storing transcript: {e}")
                return False
                    
    except Exception as e:
        logger.error(f"âŒ Database storage error: {e}")
        return False


async def query_room_by_name(room_name: str) -> Optional[Dict[str, Any]]:
    """
    Query room information by LiveKit room name.
    
    Args:
        room_name: The LiveKit room name
        
    Returns:
        Room data dictionary or None if not found
    """
    try:
        session = await _pool.get_session()
        async with get_db_headers() as headers:
            url = f"{config.supabase.url}/rest/v1/rooms"
            params = {"Livekit_room_name": f"eq.{room_name}"}
            
            timeout = aiohttp.ClientTimeout(total=config.supabase.http_timeout)
            
            try:
                async with session.get(url, headers=headers, params=params, timeout=timeout) as response:
                    if response.status == 200:
                        rooms = await response.json()
                        if rooms and len(rooms) > 0:
                            return rooms[0]
                    else:
                        error_text = await response.text()
                        logger.warning(f"Failed to query room: {response.status} - {error_text}")
            except asyncio.TimeoutError:
                logger.warning("Timeout querying room")
            except Exception as e:
                logger.error(f"Error querying room: {e}")
        
        return None
        
    except Exception as e:
        logger.error(f"âŒ Room query failed: {e}")
        return None


async def get_active_session_for_room(room_id: int) -> Optional[str]:
    """
    Get the active session ID for a room if one exists.
    
    Args:
        room_id: The room ID
        
    Returns:
        Session ID or None if no active session
    """
    try:
        session = await _pool.get_session()
        async with get_db_headers() as headers:
            url = f"{config.supabase.url}/rest/v1/room_sessions"
            params = {
                "room_id": f"eq.{room_id}",
                "status": "eq.active",
                "select": "id",
                "order": "started_at.desc",
                "limit": "1"
            }
            
            timeout = aiohttp.ClientTimeout(total=config.supabase.http_timeout)
            
            try:
                async with session.get(url, headers=headers, params=params, timeout=timeout) as response:
                    if response.status == 200:
                        sessions = await response.json()
                        if sessions and len(sessions) > 0:
                            return sessions[0].get("id")
            except asyncio.TimeoutError:
                logger.warning("Timeout getting active session")
            except Exception as e:
                logger.error(f"Error getting active session: {e}")
        
        return None
        
    except Exception as e:
        logger.error(f"âŒ Active session query failed: {e}")
        return None


async def broadcast_to_channel(
    channel_name: str,
    event_type: str,
    payload: Dict[str, Any]
) -> bool:
    """
    Broadcast a message to a Supabase channel.
    
    Args:
        channel_name: The channel to broadcast to
        event_type: The event type (e.g., "transcription", "translation")
        payload: The data to broadcast
        
    Returns:
        bool: True if successful, False otherwise
    """
    try:
        if not config.supabase.service_role_key:
            logger.warning("âš ï¸ SUPABASE_SERVICE_ROLE_KEY not found - skipping broadcast")
            return False
            
        session = await _pool.get_session()
        
        async with get_db_headers() as headers:
            # Use broadcast-specific timeout
            broadcast_timeout = aiohttp.ClientTimeout(total=config.supabase.broadcast_timeout)
            
            try:
                async with session.post(
                    f"{config.supabase.url}/functions/v1/broadcast",
                    json={
                        "channel": channel_name,
                        "event": event_type,
                        "payload": payload
                    },
                    headers=headers,
                    timeout=broadcast_timeout
                ) as response:
                    if response.status == 200:
                        return True
                    else:
                        error_text = await response.text()
                        logger.warning(f"âš ï¸ Broadcast failed: {response.status} - {error_text}")
                        return False
            except asyncio.TimeoutError:
                logger.warning(f"âš ï¸ Broadcast timeout for channel {channel_name}")
                return False
            except Exception as e:
                logger.error(f"Error broadcasting: {e}")
                return False
                    
    except Exception as e:
        logger.error(f"âŒ Broadcast error: {e}")
        return False


async def query_prompt_template_for_room(room_id: int) -> Optional[Dict[str, Any]]:
    """
    Query the prompt template for a specific room.
    
    Args:
        room_id: The room ID
        
    Returns:
        Template data dictionary or None if not found
    """
    try:
        session = await _pool.get_session()
        async with get_db_headers() as headers:
            # Use the database function to get the appropriate template
            url = f"{config.supabase.url}/rest/v1/rpc/get_room_prompt_template"
            data = {"room_id": room_id}
            
            timeout = aiohttp.ClientTimeout(total=config.supabase.http_timeout)
            
            try:
                async with session.post(url, headers=headers, json=data, timeout=timeout) as response:
                    if response.status == 200:
                        result = await response.json()
                        if result and len(result) > 0:
                            template = result[0]
                            # Parse template_variables if it's a string
                            if isinstance(template.get('template_variables'), str):
                                try:
                                    import json
                                    template['template_variables'] = json.loads(template['template_variables'])
                                except:
                                    template['template_variables'] = {}
                            return template
                    else:
                        error_text = await response.text()
                        logger.warning(f"Failed to query prompt template: {response.status} - {error_text}")
            except asyncio.TimeoutError:
                logger.warning("Timeout querying prompt template")
            except Exception as e:
                logger.error(f"Error querying prompt template: {e}")
        
        return None
        
    except Exception as e:
        logger.error(f"âŒ Prompt template query failed: {e}")
        return None


async def close_database_connections():
    """Close all database connections. Call this on shutdown."""
    await _pool.close()
    logger.info("âœ… Database connections closed")


================================================
FILE: .claude/settings.local.json
================================================
{
  "permissions": {
    "allow": [
      "Bash(git clean:*)",
      "Bash(git push:*)",
      "Bash(chmod:*)",
      "Bash(find:*)",
      "Bash(ls:*)",
      "Bash(diff:*)",
      "Bash(cd:*)",
      "Bash(cd:*)",
      "Bash(cd:*)",
      "Bash(dos2unix:*)",
      "Bash(sed:*)",
      "Bash(cd:*)",
      "Bash(cd:*)",
      "Bash(cd:*)",
      "Bash(cd:*)",
      "Bash(cd:*)",
      "Bash(cd:*)",
      "Bash(cd:*)",
      "Bash(md5sum:*)"
    ],
    "deny": []
  }
}

